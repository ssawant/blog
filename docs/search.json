[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Santosh Sawant",
    "section": "",
    "text": "LLM Architect learning to innovate, optimize, and scale the next generation of large language models."
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "",
    "section": "",
    "text": "Show, Don’t Tell: Aligning Language Models with Demonstrated Feedback\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJun 4, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJun 4, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nContextual Position Encoding: Learning to Count What’s Important\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJun 3, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSimilarity is Not All You Need: Endowing Retrieval-Augmented Generation with Multi–layered Thoughts\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 31, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nNearest Neighbor Speculative Decoding for LLM Generation and Attribution\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 30, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nVeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nZamba: A Compact 7B SSM Hybrid Model\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 28, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nLayer-Condensed KV Cache for Efficient Inference of Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 20, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nXmodel-VLM: A Simple Baseline for Multimodal Vision Language Model\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSUTRA: Scalable Multilingual language model architecture\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 15, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nLinearizing Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 14, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Local to Global: A Graph RAG Approach to Query-Focused Summarization\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 13, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nIs Flash Attention Stable?\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 10, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nLayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 9, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nBetter & Faster Large Language Models via Multi-token Prediction\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 8, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nNeMo-Aligner: Scalable Toolkit for Efficient Model Alignment\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 7, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nPROMETHEUS 2: An Open Source Language Model Specialized in Evaluating Other Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 3, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nOctopus v4: Graph of language models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMay 2, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nReplacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 30, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMake Your LLM Fully Utilize the Context\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 29, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nCodecLM: Aligning Language Models with Tailored Synthetic Data\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 25, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nLLM-R2 : A Large Language Model Enhanced Rule-based Rewrite System for Boosting Query Efficiency\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 24, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nTransformerFAM: Feedback attention is working memory\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 19, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nRecurrentGemma: Moving Past Transformers for Efficient Open Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 18, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 17, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nTrust Region Direct Preference Optimization (TR-DPO) : Learn Your Reference Model for Real Good Alignment\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 16, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nRHO-1: Not All Tokens Are What You Need\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 15, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nDirect Nash Optimization: Teaching Language Models to Self-Improve with General Preferences\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 9, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nStream of Search (SoS): Learning to Search in Language\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 8, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nReFT: Representation Finetuning for Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 5, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMixture-of-Depths: Dynamically allocating compute in transformer-based language models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 4, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nsDPO: Don’t Use Your Data All at Once\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 3, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nGecko: Versatile Text Embeddings Distilled from Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 2, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nJamba: A Hybrid Transformer-Mamba Language Model\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nApr 1, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMini-Gemini: Mining the Potential of Multi-modality Vision Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 28, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nRigorLLM: Resilient Guardrails for large language models against undesired content\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 27, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nDRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 26, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSiMBA: Simplified Mamba-based Architecture for Vision and Multivariate Time series\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 25, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nCobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 22, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nEvolutionary Optimization of Model Merging Recipes\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 21, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nmPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 20, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nPERL: Parameter Efficient Reinforcement Learning from Human Feedback\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 19, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nRAFT: Adapting Language Model to Domain Specific RAG\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 18, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nUSER-LLM: Efficient LLM Contextualization with User Embeddings\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 15, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nRAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 14, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMoAI: Mixture of All Intelligence for Large Language and Vision Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 13, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nVideoMamba: State Space Model for Efficient Video Understanding\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 12, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nGaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 8, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nInfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 7, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nDesign2Code: How Far Are We From Automating Front-End Engineering?\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 6, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nDiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized Diffusion Model\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 5, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nVisionLLaMA : A Unified LLaMA Interface for Vision Tasks\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 4, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nBeyond Language Models: Byte Models are Digital World Simulators\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nMar 1, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nThe Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 29, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nChunkLlama : Training-Free Long-Context Scaling of Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 28, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMobiLlama: Towards Accurate and Lightweight Fully Transparent GPT\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 27, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nTinyLLaVA: A Framework of Small-scale Large Multimodal Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nThe FinBen: An Holistic Financial Benchmark for Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nGRIT : Generative Representational Instruction Tuning\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 16, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nAespa: Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nGraph Mamba: Towards Learning on Graphs with State Space Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nFiddler: CPU-GPU Orchestration for Fast Local Inference of MoE Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 13, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nPHATGOOSE: Learning to Route Among Specialized Experts for Zero-Shot Generalization\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nTag-LLM: Repurposing General-Purpose LLMs for Specialized Domains\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nHydragen: High-Throughput LLM Inference with Shared Prefixes\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMambaFormer: Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nBlackMamba: Mixture of Experts for State-Space Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nRepeat After Me: Transformers are Better than State Space Models at Copying\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nRe3val: Reinforced and Reranked Generative Retrieval\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 2, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nFIND: INterface for Foundation models’ embeDDings\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nBLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nEAGLE: Extrapolation Algorithm for Greater Language-model Efficiency\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 29, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMambaByte: Token-free Selective State Space Model\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nInstruction-Tune Llama2 with TRL\n\n\n\n\n\n\nhugging face\n\n\nllm\n\n\nmodel building\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nTowards Conversational Diagnostic AI\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nChatQA: Building GPT-4 Level Conversational QA Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Fine-Tune LLMs with TRL\n\n\n\n\n\n\nhugging face\n\n\nllm\n\n\nmodel building\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMedusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMerge Model using Mergekit\n\n\n\n\n\n\ntools\n\n\nllm\n\n\nmodel building\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nTuning Language Models by Proxy\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nDeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 18, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Evaluation Improves Selective Generation in Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-RAG: Learning to Retrieve, Generate and Critique through Self-Reflections\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nReciprocal Rank Fusion (RRF) with LambdaMART: Context Tuning for Retrieval Augmented Generation (RAG)\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 15, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nChain of Thought (CoT): The Impact of Reasoning Step Length on Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 12, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nInfinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSoaring from 4K to 400K: Extending LLM’s Context with Activation Beacon\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 10, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nImproving Text Embeddings with Large Language Models using fine-tuned Mistral-7B LLM\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 9, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nDOCLLM: A Layout Aware Generative Language Models for Multi model document understanding\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Play Fine-Tuning (SPIN): Converts Weak Language Models to Strong Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 5, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nA Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 3, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nMamba-Chat: A Chat LLM based on State Space Models\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 2, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\n\n\n\n\n\n\nKwaiAgents: Generalized Information-seeking Agent System with LLMs - 2 Open-source models fine tuned for agent systems! Better than GPT-3.5 turbo as an agent!\n\n\n\n\n\n\nllm\n\n\nresearch paper\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/mergekit/Mearge_LLMs_with_mergekit.html",
    "href": "posts/mergekit/Mearge_LLMs_with_mergekit.html",
    "title": "Merge Model using Mergekit",
    "section": "",
    "text": "Model merging is a technique that combines two or more LLMs into a single model. It’s a relatively new and experimental method to create new models for cheap (no GPU required). Model merging works surprisingly well and produced many state-of-the-art models on the Open LLM Leaderboard.\nIn this tutorial, we will implement it using the mergekit library. More specifically, we will review four merge methods and provide examples of configurations. Then, we will use mergekit to create our own model"
  },
  {
    "objectID": "posts/mergekit/Mearge_LLMs_with_mergekit.html#merge-models",
    "href": "posts/mergekit/Mearge_LLMs_with_mergekit.html#merge-models",
    "title": "Merge Model using Mergekit",
    "section": "Merge models",
    "text": "Merge models\n\n!mergekit-yaml config.yaml merge --copy-tokenizer --allow-crimes --out-shard-size 1B --lazy-unpickle\n\nFetching 8 files: 100% 8/8 [00:00&lt;00:00, 18275.83it/s]\nFetching 11 files: 100% 11/11 [00:00&lt;00:00, 21670.90it/s]\n  0% 0/291 [00:00&lt;?, ?it/s]WARNING:root:Using common submatrix of size torch.Size([32000, 4096]) for model.embed_tokens.weight\n 70% 203/291 [02:58&lt;00:44,  1.98it/s]WARNING:root:Using common submatrix of size torch.Size([32000, 4096]) for lm_head.weight\n100% 291/291 [04:22&lt;00:00,  1.11it/s]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n\n!pip install -qU huggingface_hub\n\nfrom huggingface_hub import ModelCard, ModelCardData\nfrom jinja2 import Template\n\nusername = \"santoshsawant\"\n\ntemplate_text = \"\"\"\n---\nlicense: apache-2.0\ntags:\n- merge\n- mergekit\n- lazymergekit\n{%- for model in models %}\n- {{ model }}\n{%- endfor %}\n---\n\n# {{ model_name }}\n\n{{ model_name }} is a merge of the following models using [mergekit](https://github.com/cg123/mergekit):\n\n{%- for model in models %}\n* [{{ model }}](https://huggingface.co/{{ model }})\n{%- endfor %}\n\n## Configuration\n\n```yaml\n{{- yaml_config -}}\n```\n\"\"\"\n\n# Create a Jinja template object\njinja_template = Template(template_text.strip())\n\n# Get list of models from config\ndata = yaml.safe_load(yaml_config)\nif \"models\" in data:\n    models = [data[\"models\"][i][\"model\"] for i in range(len(data[\"models\"])) if \"parameters\" in data[\"models\"][i]]\nelif \"parameters\" in data:\n    models = [data[\"slices\"][0][\"sources\"][i][\"model\"] for i in range(len(data[\"slices\"][0][\"sources\"]))]\nelif \"slices\" in data:\n    models = [data[\"slices\"][i][\"sources\"][0][\"model\"] for i in range(len(data[\"slices\"]))]\nelse:\n    raise Exception(\"No models or slices found in yaml config\")\n\n# Fill the template\ncontent = jinja_template.render(\n    model_name=MODEL_NAME,\n    models=models,\n    yaml_config=yaml_config,\n    username=username,\n)\n\n# Save the model card\ncard = ModelCard(content)\ncard.save('merge/README.md')\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/330.1 kB ? eta -:--:--     ━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.4/330.1 kB 2.2 MB/s eta 0:00:01     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 327.7/330.1 kB 5.3 MB/s eta 0:00:01     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 330.1/330.1 kB 4.6 MB/s eta 0:00:00\n\n\n\nfrom google.colab import userdata\nfrom huggingface_hub import HfApi\n\nusername = \"santoshsawant\"\n\n# Defined in the secrets tab in Google Colab\napi = HfApi(token=userdata.get(\"huggingface\"))\n\napi.create_repo(\n    repo_id=f\"{username}/{MODEL_NAME}\",\n    repo_type=\"model\"\n)\napi.upload_folder(\n    repo_id=f\"{username}/{MODEL_NAME}\",\n    folder_path=\"merge\",\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/santoshsawant/NeuralHermes-7B-slerp/commit/57dae104809557372cabe688eb608448d32fa485', commit_message='Upload folder using huggingface_hub', commit_description='', oid='57dae104809557372cabe688eb608448d32fa485', pr_url=None, pr_revision=None, pr_num=None)"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Curriculum vitæ",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#define-our-use-case",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#define-our-use-case",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "1. Define our use case",
    "text": "1. Define our use case\nWhen fine-tuning LLMs, it is important you know your use case and the task you want to solve. This will help you to choose the right model or help you to create a dataset to fine-tune your model. If you haven’t defined your use case yet. You might want to go back to the drawing board. I want to mention that not all use cases require fine-tuning and it is always recommended to evaluate and try out already fine-tuned models or API-based models before fine-tuning your own model.\nAs an example, we are going to use the following use case:\n\nWe want to fine-tune a model, which can generate SQL queries based on a natural language instruction, which can then be integrated into our BI tool. The goal is to reduce the time it takes to create a SQL query and make it easier for non-technical users to create SQL queries.\n\nText to SQL can be a good use case for fine-tuning LLMs, as it is a complex task that requires a lot of (internal) knowledge about the data and the SQL language."
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#setup-development-environment",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#setup-development-environment",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "2. Setup development environment",
    "text": "2. Setup development environment\nOur first step is to install Hugging Face Libraries and Pyroch, including trl, transformers and datasets. If you haven’t heard of trl yet, don’t worry. It is a new library on top of transformers and datasets, which makes it easier to fine-tune, rlhf, align open LLMs.\n\n# Install Pytorch & other libraries\n!pip install \"torch==2.1.2\" tensorboard\n\n# Install Hugging Face libraries\n!pip install  --upgrade \\\n  \"transformers==4.36.2\" \\\n  \"datasets==2.16.1\" \\\n  \"accelerate==0.26.1\" \\\n  \"evaluate==0.4.1\" \\\n  \"bitsandbytes==0.42.0\" \\\n  # \"trl==0.7.10\" # \\\n  # \"peft==0.7.1\" \\\n\n# install peft & trl from github\n!pip install git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e --upgrade\n!pip install git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f--upgrade\n\nIf you are using a GPU with Ampere architecture (e.g. NVIDIA A10G or RTX 4090/3090) or newer you can use Flash attention. Flash Attention is a an method that reorders the attention computation and leverages classical techniques (tiling, recomputation) to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. The TL;DR; accelerates training up to 3x. Learn more at FlashAttention.\nNote: If your machine has less than 96GB of RAM and lots of CPU cores, reduce the number of MAX_JOBS. On the g5.2xlarge we used 4.\n\nimport torch; assert torch.cuda.get_device_capability()[0] &gt;= 8, 'Hardware not supported for Flash Attention'\n# install flash-attn\n!pip install ninja packaging\n!MAX_JOBS=4 pip install flash-attn --no-build-isolation\n\nInstalling flash attention can take quite a bit of time (10-45 minutes).\nWe will use the Hugging Face Hub as a remote model versioning service. This means we will automatically push our model, logs and information to the Hub during training. You must register on the Hugging Face for this. After you have an account, we will use the login util from the huggingface_hub package to log into our account and store our token (access key) on the disk.\n\nfrom huggingface_hub import login\n\nlogin(\n  token=\"\", # ADD YOUR TOKEN HERE\n  add_to_git_credential=True\n)"
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#create-and-prepare-the-dataset",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#create-and-prepare-the-dataset",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "3. Create and prepare the dataset",
    "text": "3. Create and prepare the dataset\nOnce you have determined that fine-tuning is the right solution we need to create a dataset to fine-tune our model. The dataset should be a diverse set of demonstrations of the task you want to solve. There are several ways to create such a dataset, including: * Using existing open-source datasets, e.g., Spider * Using LLMs to create synthetically datasets, e.g., Alpaca * Using Humans to create datasets, e.g., Dolly. * Using a combination of the above methods, e.g., Orca\nEach of the methods has its own advantages and disadvantages and depends on the budget, time, and quality requirements. For example, using an existing dataset is the easiest but might not be tailored to your specific use case, while using humans might be the most accurate but can be time-consuming and expensive. It is also possible to combine several methods to create an instruction dataset, as shown in Orca: Progressive Learning from Complex Explanation Traces of GPT-4.\nIn our example we will use an already existing dataset called sql-create-context, which contains samples of natural language instructions, schema definitions and the corresponding SQL query.\nWith the latest release of trl we now support popular instruction and conversation dataset formats. This means we only need to convert our dataset to one of the supported formats and trl will take care of the rest. Those formats include: * conversational format\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n\ninstruction format\n\n{\"prompt\": \"&lt;prompt text&gt;\", \"completion\": \"&lt;ideal generated text&gt;\"}\n{\"prompt\": \"&lt;prompt text&gt;\", \"completion\": \"&lt;ideal generated text&gt;\"}\n{\"prompt\": \"&lt;prompt text&gt;\", \"completion\": \"&lt;ideal generated text&gt;\"}\nIn our example we are going to load our open-source dataset using the 🤗 Datasets library and then convert it into the the conversational format, where we include the schema definition in the system message for our assistant. We’ll then save the dataset as jsonl file, which we can then use to fine-tune our model. We are randomly downsampling the dataset to only 10,000 samples.\nNote: This step can be different for your use case. For example, if you have already a dataset from, e.g. working with OpenAI, you can skip this step and go directly to the fine-tuning step.\n\nfrom datasets import load_dataset\n\n# Convert dataset to OAI messages\nsystem_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\nSCHEMA:\n{schema}\"\"\"\n\ndef create_conversation(sample):\n  return {\n    \"messages\": [\n      {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n      {\"role\": \"user\", \"content\": sample[\"question\"]},\n      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n    ]\n  }  \n\n# Load dataset from the hub\ndataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\ndataset = dataset.shuffle().select(range(12500))\n\n# Convert dataset to OAI messages\ndataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n# split dataset into 10,000 training samples and 2,500 test samples\ndataset = dataset.train_test_split(test_size=2500/12500)\n\nprint(dataset[\"train\"][345][\"messages\"])\n\n# save datasets to disk \ndataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\ndataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")"
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#fine-tune-llm-using-trl-and-the-sfttrainer",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#fine-tune-llm-using-trl-and-the-sfttrainer",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "4. Fine-tune LLM using trl and the SFTTrainer",
    "text": "4. Fine-tune LLM using trl and the SFTTrainer\nWe are now ready to fine-tune our model. We will use the SFTTrainer from trl to fine-tune our model. The SFTTrainer makes it straightfoward to supervise fine-tune open LLMs. The SFTTrainer is a subclass of the Trainer from the transformers library and supports all the same features, including logging, evaluation, and checkpointing, but adds additiional quality of life features, including: * Dataset formatting, including conversational and instruction format * Training on completions only, ignoring prompts * Packing datasets for more efficient training * PEFT (parameter-efficient fine-tuning) support including Q-LoRA * Preparing the model and tokenizer for conversational fine-tuning (e.g. adding special tokens)\nWe will use the dataset formatting, packing and PEFT features in our example. As peft method we will use QLoRA a technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance by using quantization. If you want to learn more about QLoRA and how it works, check out Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA blog post.\nNow, lets get started! 🚀\nFirst, we need to load our dataset from disk.\n\nfrom datasets import load_dataset\n\n# Load jsonl data from disk\ndataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")\n\nNext, we will load our LLM. For our use case we are going to use CodeLlama 7B. CodeLlama is a Llama model trained for general code synthesis and understanding. But we can easily swap out the model for another model, e.g. Mistral or Mixtral models, TII Falcon, or any other LLMs by changing our model_id variable. We will use bitsandbytes to quantize our model to 4-bit.\nNote: Be aware the bigger the model the more memory it will require. In our example we will use the 7B version, which can be tuned on 24GB GPUs. If you have a smaller GPU.\nCorrectly, preparing the LLM and Tokenizer for training chat/conversational models is crucial. We need to add new special tokens to the tokenizer and model and teach to understand the different roles in a conversation. In trl we have a convinient method called setup_chat_format, which: * Adds special tokens to the tokenizer, e.g. &lt;|im_start|&gt; and &lt;|im_end|&gt;, to indicate the start and end of a conversation. * Resizes the model’s embedding layer to accommodate the new tokens. * Sets the chat_template of the tokenizer, which is used to format the input data into a chat-like format. The default is chatml from OpenAI.\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom trl import setup_chat_format\n\n# Hugging Face model id\nmodel_id = \"codellama/CodeLlama-7b-hf\" # or `mistralai/Mistral-7B-v0.1`\n\n# BitsAndBytesConfig int-4 config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",\n    torch_dtype=torch.bfloat16,\n    quantization_config=bnb_config\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.padding_side = 'right' # to prevent warnings\n\n# # set chat template to OAI chatML, remove if you start from a fine-tuned model\nmodel, tokenizer = setup_chat_format(model, tokenizer)\n\nThe SFTTrainer  supports a native integration with peft, which makes it super easy to efficiently tune LLMs using, e.g. QLoRA. We only need to create our LoraConfig and provide it to the trainer. Our LoraConfig parameters are defined based on the qlora paper and sebastian’s blog post.\n\nfrom peft import LoraConfig\n\n# LoRA config based on QLoRA paper & Sebastian Raschka experiment\npeft_config = LoraConfig(\n        lora_alpha=128,\n        lora_dropout=0.05,\n        r=256,\n        bias=\"none\",\n        target_modules=\"all-linear\",\n        task_type=\"CAUSAL_LM\", \n)\n\nBefore we can start our training we need to define the hyperparameters (TrainingArguments) we want to use.\n\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"code-llama-7b-text-to-sql\", # directory to save and repository id\n    num_train_epochs=3,                     # number of training epochs\n    per_device_train_batch_size=3,          # batch size per device during training\n    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n    logging_steps=10,                       # log every 10 steps\n    save_strategy=\"epoch\",                  # save checkpoint every epoch\n    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n    bf16=True,                              # use bfloat16 precision\n    tf32=True,                              # use tf32 precision\n    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n    push_to_hub=True,                       # push model to hub\n    report_to=\"tensorboard\",                # report metrics to tensorboard\n)\n\nWe now have every building block we need to create our SFTTrainer to start then training our model.\n\nfrom trl import SFTTrainer\n\nmax_seq_length = 3072 # max sequence length for model and packing of the dataset\n\ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    packing=True,\n    dataset_kwargs={\n        \"add_special_tokens\": False,  # We template with special tokens\n        \"append_concat_token\": False, # No need to add additional separator token\n    }\n)\n\nStart training our model by calling the train() method on our Trainer instance. This will start the training loop and train our model for 3 epochs. Since we are using a PEFT method, we will only save the adapted model weights and not the full model.\n\n# start training, the model will be automatically saved to the hub and the output directory\ntrainer.train()\n\n# save model \ntrainer.save_model()\n\nThe training with Flash Attention for 3 epochs with a dataset of 10k samples took 01:29:58 on a g5.2xlarge. The instance costs 1,212$/h which brings us to a total cost of only 1.8$.\n\n# free the memory again\ndel model\ndel trainer\ntorch.cuda.empty_cache()\n\n\nOptional: Merge LoRA adapter in to the original model\nWhen using QLoRA, we only train adapters and not the full model. This means when saving the model during training we only save the adapter weights and not the full model. If you want to save the full model, which makes it easier to use with Text Generation Inference you can merge the adapter weights into the model weights using the merge_and_unload method and then save the model with the save_pretrained method. This will save a default model, which can be used for inference.\nNote: You might require &gt; 30GB CPU Memory.\n\n\n#### COMMENT IN TO MERGE PEFT AND BASE MODEL ####\n# from peft import PeftModel, PeftConfig\n# from transformers import AutoModelForCausalLM, AutoTokenizer\n# from peft import AutoPeftModelForCausalLM\n\n# # Load PEFT model on CPU\n# config = PeftConfig.from_pretrained(args.output_dir)\n# model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,low_cpu_mem_usage=True)\n# tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n# model.resize_token_embeddings(len(tokenizer))\n# model = PeftModel.from_pretrained(model, args.output_dir)\n# model = AutoPeftModelForCausalLM.from_pretrained(\n#     args.output_dir,\n#     torch_dtype=torch.float16,\n#     low_cpu_mem_usage=True,\n# )  \n# # Merge LoRA and base model and save\n# merged_model = model.merge_and_unload()\n# merged_model.save_pretrained(args.output_dir,safe_serialization=True, max_shard_size=\"2GB\")"
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#test-model-and-run-inference",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#test-model-and-run-inference",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "4. Test Model and run Inference",
    "text": "4. Test Model and run Inference\nAfter the training is done we want to evaluate and test our model. We will load different samples from the original dataset and evaluate the model on those samples, using a simple loop and accuracy as our metric.\nNote: Evaluating Generative AI models is not a trivial task since 1 input can have multiple correct outputs. If you want to learn more about evaluating generative models, check out Evaluate LLMs and RAG a practical example using Langchain and Hugging Face blog post.\n\nimport torch\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer, pipeline \n\npeft_model_id = \"./code-llama-7b-text-to-sql\"\n# peft_model_id = args.output_dir\n\n# Load Model with PEFT adapter\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n  peft_model_id,\n  device_map=\"auto\",\n  torch_dtype=torch.float16\n)\n# load into pipeline\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\nLet’s load our test dataset try to generate an instruction.\n\nfrom datasets import load_dataset \nfrom random import randint\n\n\n# Load our test dataset\neval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\nrand_idx = randint(0, len(eval_dataset))\n\n# Test on sample \nprompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n\nprint(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\nprint(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\nprint(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")\n\nNice! Our model was able to generate a SQL query based on the natural language instruction. Lets evaluate our model on the full 2,500 samples of our test dataset. Note: As mentioned above, evaluating generative models is not a trivial task. In our example we used the accuracy of the generated SQL based on the ground truth SQL query as our metric. An alternative way could be to automatically execute the generated SQL query and compare the results with the ground truth. This would be a more accurate metric but requires more work to setup.\n\nfrom tqdm import tqdm\n\n\ndef evaluate(sample):\n    prompt = pipe.tokenizer.apply_chat_template(sample[\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n    predicted_answer = outputs[0]['generated_text'][len(prompt):].strip()\n    if predicted_answer == sample[\"messages\"][2][\"content\"]:\n        return 1 \n    else:\n        return 0\n\nsuccess_rate = []\nnumber_of_eval_samples = 1000\n# iterate over eval dataset and predict\nfor s in tqdm(eval_dataset.shuffle().select(range(number_of_eval_samples))):\n    success_rate.append(evaluate(s))\n\n# compute accuracy\naccuracy = sum(success_rate)/len(success_rate)\n\nprint(f\"Accuracy: {accuracy*100:.2f}%\")  \n        \n\nWe evaluated our model on 1000 samples from the evaluation dataset and got an accuracy of 79.50%, which took ~25 minutes. This is quite good, but as mentioned you need to take this metric with a grain of salt. It would be better if we could evaluate our model by running the qureies against a real database and compare the results. Since there might be different “correct” SQL queries for the same instruction. There are also several ways on how we could improve the performance by using few-shot learning, using RAG, Self-healing to generate the SQL query."
  },
  {
    "objectID": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#deploy-the-llm-for-production",
    "href": "posts/fine-tune-llms-hugging-face/fine-tune-llms-in-2024-with-trl.html#deploy-the-llm-for-production",
    "title": "How to Fine-Tune LLMs with TRL",
    "section": "6. Deploy the LLM for Production",
    "text": "6. Deploy the LLM for Production\nYou can now deploy your model to production. For deploying open LLMs into production we recommend using Text Generation Inference (TGI). TGI is a purpose-built solution for deploying and serving Large Language Models (LLMs). TGI enables high-performance text generation using Tensor Parallelism and continous batching for the most popular open LLMs, including Llama, Mistral, Mixtral, StarCoder, T5 and more. Text Generation Inference is used by companies as IBM, Grammarly, Uber, Deutsche Telekom, and many more. There are several ways to deploy your model, including:\n\nDeploy LLMs with Hugging Face Inference Endpoints\nHugging Face LLM Inference Container for Amazon SageMaker\nDIY\n\nIf you have docker installed you can use the following command to start the inference server.\nNote: Make sure that you have enough GPU memory to run the container. Restart kernel to remove all allocated GPU memory from the notebook.\n\n%%bash \n# model=$PWD/{args.output_dir} # path to model\nmodel=$(pwd)/code-llama-7b-text-to-sql # path to model\nnum_shard=1             # number of shards\nmax_input_length=1024   # max input length\nmax_total_tokens=2048   # max total tokens\n\ndocker run -d --name tgi --gpus all -ti -p 8080:80 \\\n  -e MODEL_ID=/workspace \\\n  -e NUM_SHARD=$num_shard \\\n  -e MAX_INPUT_LENGTH=$max_input_length \\\n  -e MAX_TOTAL_TOKENS=$max_total_tokens \\\n  -v $model:/workspace \\\n  ghcr.io/huggingface/text-generation-inference:latest\n\nOnce your container is running you can send requests.\n\nimport requests as r \nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nfrom random import randint\n\n# Load our test dataset and Tokenizer again\ntokenizer = AutoTokenizer.from_pretrained(\"code-llama-7b-text-to-sql\")\neval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\nrand_idx = randint(0, len(eval_dataset))\n\n# generate the same prompt as for the first local test\nprompt = tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\nrequest= {\"inputs\":prompt,\"parameters\":{\"temperature\":0.2, \"top_p\": 0.95, \"max_new_tokens\": 256}}\n\n# send request to inference server\nresp = r.post(\"http://127.0.0.1:8080/generate\", json=request)\n\noutput = resp.json()[\"generated_text\"].strip()\ntime_per_token = resp.headers.get(\"x-time-per-token\")\ntime_prompt_tokens = resp.headers.get(\"x-prompt-tokens\")\n\n# Print results\nprint(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\nprint(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\nprint(f\"Generated Answer:\\n{output}\")\nprint(f\"Latency per token: {time_per_token}ms\")\nprint(f\"Latency prompt encoding: {time_prompt_tokens}ms\")\n\nAwesome, Don’t forget to stop your container once you are done.\n\n!docker stop tgi"
  },
  {
    "objectID": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#define-our-use-case-in-detail-and-create-a-template-for-our-instructions",
    "href": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#define-our-use-case-in-detail-and-create-a-template-for-our-instructions",
    "title": "Instruction-Tune Llama2 with TRL",
    "section": "1. Define our use case in detail and create a template for our instructions",
    "text": "1. Define our use case in detail and create a template for our instructions\nBefore we describe our use case, we need to better understand what even is an instruction.\n\nAn instruction is a piece of text or prompt that is provided to an LLM, like Llama, GPT-4, or Claude, to guide it to generate a response. Instructions allow humans to steer the conversation and constrain the language model’s output to be more natural, useful, and aligned with the user’s goals. Crafting clear, well-formulated instructions is key to productive conversations.\n\nExamples of instructions are listed below in the table.\n\n\n\n\n\n\n\nCapability\nExample Instruction\n\n\n\n\nBrainstorming\nProvide a diverse set of creative ideas for new flavors of ice cream.\n\n\nClassification\nCategorize these movies as either comedy, drama, or horror based on the plot summary.\n\n\nClosed QA\nAnswer the question ‘What is the capital of France?’ with a single word.\n\n\nGeneration\nWrite a poem in the style of Robert Frost about nature and the changing seasons.\n\n\nInformation Extraction\nExtract the names of the main characters from this short story.\n\n\nOpen QA\nWhy do leaves change color in autumn? Explain the scientific reasons.\n\n\nSummarization\nSummarize this article on recent advancements in renewable energy in 2-3 sentences.\n\n\n\nAs described in the beginning, we want to fine-tune a model to be able to generate instructions based on input. (output). We want to use this as a way to create synthetic datasets to personalize LLMs and Agents.\nConverting the idea into a basic prompt template following the Alpaca format we get.\n### Instruction:\nUse the Input below to create an instruction, which could have been used to generate the input using an LLM. \n\n### Input:\nDear [boss name],\n\nI'm writing to request next week, August 1st through August 4th,\noff as paid time off.\n\nI have some personal matters to attend to that week that require \nme to be out of the office. I wanted to give you as much advance \nnotice as possible so you can plan accordingly while I am away.\n\nPlease let me know if you need any additional information from me \nor have any concerns with me taking next week off. I appreciate you \nconsidering this request.\n\nThank you, [Your name]\n\n### Response:\nWrite an email to my boss that I need next week 08/01 - 08/04 off."
  },
  {
    "objectID": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#create-an-instruction-dataset",
    "href": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#create-an-instruction-dataset",
    "title": "Instruction-Tune Llama2 with TRL",
    "section": "2. Create an instruction dataset",
    "text": "2. Create an instruction dataset\nAfter we defined our use case and prompt template, we need to create our instruction dataset. Creating a high-quality instruction dataset is key for a good-performing model. Research shows that “Less Is More for Alignment” shows that creating a high-quality, low-quantity (~1000 samples) dataset can achieve the same performance as less-quality and high-quantity datasets.\nThere are several ways to create an instruction dataset, including:\n\nUsing an existing dataset and converting it into an instruction dataset, e.g., FLAN\nUse existing LLMs to create synthetically instruction datasets, e.g., Alpaca\nUse Humans to create instructions datasets, e.g., Dolly.\n\nEach of the methods has its own advantages and disadvantages and depends on the budget, time, and quality requirements. For example, using an existing dataset is the easiest but might not be tailored to your specific use case, while using humans might be the most accurate but can be time-consuming and expensive. It is also possible to combine several methods to create an instruction dataset, as shown in Orca: Progressive Learning from Complex Explanation Traces of GPT-4.\nTo keep it simple, we are going to use Dolly an open-source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\nLet’s start coding, but first, let’s install our dependencies.\n\n!pip install \"transformers==4.34.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.23.0\" \"bitsandbytes==0.41.1\" \"trl==0.4.7\" \"safetensors&gt;=0.3.1\" --upgrade\n\nTo load the databricks/databricks-dolly-15k dataset, we use the load_dataset() method from the 🤗 Datasets library.\n\nfrom datasets import load_dataset\nfrom random import randrange\n\n# Load dataset from the hub\ndataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n\nprint(f\"dataset size: {len(dataset)}\")\nprint(dataset[randrange(len(dataset))])\n# dataset size: 15011\n\nFound cached dataset json (/home/ubuntu/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n\n\ndataset size: 15011\n{'instruction': 'On what month and day was Antwan Deon Odom born?', 'context': 'Antwan Deon Odom (born September 24, 1981) is a former American football defensive end. He was drafted by the Tennessee Titans in the second round of the 2004 NFL Draft. He played college football at Alabama. He has also played for the Cincinnati Bengals.', 'response': 'September 24', 'category': 'closed_qa'}\n\n\nTo instruct tune our model, we need to convert our structured examples into a collection of tasks described via instructions. We define a formatting_function that takes a sample and returns a string with our format instruction.\n\ndef format_instruction(sample):\n    return f\"\"\"### Instruction:\nUse the Input below to create an instruction, which could have been used to generate the input using an LLM. \n\n### Input:\n{sample['response']}\n\n### Response:\n{sample['instruction']}\n\"\"\"\n\nLet’s test our formatting function on a random example.\n\nfrom random import randrange\n\nprint(format_instruction(dataset[randrange(len(dataset))]))\n\n### Instruction:\nUse the Input below to create an instruction, which could have been used to generate the input using an LLM. \n\n### Input:\nSir Dorabji Tata and Allied Trusts and Sir Ratan Tata Trust\n\n### Response:\nWhat are the names of Tata trusts which Ratan Tata heads?"
  },
  {
    "objectID": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#instruction-tune-llama-2-using-trl-and-the-sfttrainer",
    "href": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#instruction-tune-llama-2-using-trl-and-the-sfttrainer",
    "title": "Instruction-Tune Llama2 with TRL",
    "section": "3. Instruction-tune Llama 2 using trl and the SFTTrainer",
    "text": "3. Instruction-tune Llama 2 using trl and the SFTTrainer\nWe will use the recently introduced method in the paper “QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation” by Tim Dettmers et al. QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. The TL;DR; of how QLoRA works is:\n\nQuantize the pre-trained model to 4 bits and freeze it.\nAttach small, trainable adapter layers. (LoRA)\nFinetune only the adapter layers while using the frozen quantized model for context.\n\nIf you want to learn more about QLoRA and how it works, I recommend you to read the Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA blog post.\n\nFlash Attention\nFlash Attention is a an method that reorders the attention computation and leverages classical techniques (tiling, recomputation) to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. It is based on the paper “FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness”. The TL;DR; accelerates training up to 3x. Learn more at FlashAttention. Flash Attention is currently only available for Ampere (A10, A40, A100, …) & Hopper (H100, …) GPUs. You can check if your GPU is supported and install it using the following command:\nNote: If your machine has less than 96GB of RAM and lots of CPU cores, reduce the number of MAX_JOBS. On the g5.2xlarge we used 4.\npython -c \"import torch; assert torch.cuda.get_device_capability()[0] &gt;= 8, 'Hardware not supported for Flash Attention'\"\npip install ninja packaging\nMAX_JOBS=4 pip install flash-attn --no-build-isolation\nInstalling flash attention can take quite a bit of time (10-45 minutes).\nThe example supports the use of Flash Attention for all Llama checkpoints, but is not enabled by default. To use Flash Attention change the value of use_flash_attentin to True\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nuse_flash_attention = False\n\n# Hugging Face model id\nmodel_id = \"NousResearch/Llama-2-7b-hf\"  # non-gated\n# model_id = \"meta-llama/Llama-2-7b-hf\" # gated\n\n\n# BitsAndBytesConfig int-4 config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    use_cache=False,\n    use_flash_attention_2=use_flash_attention,\n    device_map=\"auto\",\n)\nmodel.config.pretraining_tp = 1\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n\n\n\nThe SFTTrainer  supports a native integration with peft, which makes it super easy to efficiently instruction tune LLMs. We only need to create our LoRAConfig and provide it to the trainer.\n\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n\n# LoRA config based on QLoRA paper\npeft_config = LoraConfig(\n        lora_alpha=16,\n        lora_dropout=0.1,\n        r=64,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\", \n)\n\n\n# prepare model for training\nmodel = prepare_model_for_kbit_training(model)\n\nBefore we can start our training we need to define the hyperparameters (TrainingArguments) we want to use.\n\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"llama-7-int4-dolly\",\n    num_train_epochs=3,\n    per_device_train_batch_size=6 if use_flash_attention else 4,\n    gradient_accumulation_steps=2,\n    gradient_checkpointing=True,\n    optim=\"paged_adamw_32bit\",\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    learning_rate=2e-4,\n    bf16=True,\n    fp16=False,\n    tf32=True,\n    max_grad_norm=0.3,\n    warmup_ratio=0.03,\n    lr_scheduler_type=\"constant\",\n    disable_tqdm=False,  # disable tqdm since with packing values are in correct\n)\n\n\n# Upcast layer for flash attnetion\nif use_flash_attention:\n    from utils.llama_patch import upcast_layer_for_flash_attention\n    torch_dtype = torch.bfloat16 if args.bf16 else torch.float16 if args.fp16 else torch.float32\n    model = upcast_layer_for_flash_attention(model, torch_dtype)\n\nmodel = get_peft_model(model, peft_config)\n\nWe now have every building block we need to create our SFTTrainer to start then training our model.\n\nfrom trl import SFTTrainer\n\nmax_seq_length = 2048 # max sequence length for model and packing of the dataset\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    packing=True,\n    formatting_func=format_instruction, \n    args=args,\n)\n\nStart training our model by calling the train() method on our Trainer instance.\n\n# train\ntrainer.train() # there will not be a progress bar since tqdm is disabled\n\n# save model\ntrainer.save_model()\n\nYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\nThe training without Flash Attention enabled took 03:08:00 on a g5.2xlarge. The instance costs 1,212$/h which brings us to a total cost of 3.7$. The training with Flash Attention enabled took 02:08:00 on a g5.2xlarge. The instance costs 1,212$/h which brings us to a total cost of 2.6$.\nThe results using Flash Attention are mind blowing and impressive, 1.5x faster and 30% cheaper."
  },
  {
    "objectID": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#test-model-and-run-inference",
    "href": "posts/instruction-tune-llama-2/instruction-tune-llama-2-int4.html#test-model-and-run-inference",
    "title": "Instruction-Tune Llama2 with TRL",
    "section": "4. Test Model and run Inference",
    "text": "4. Test Model and run Inference\nAfter the training is done we want to run and test our model. We will use peft and transformers to load our LoRA adapter into our model.\n\nif use_flash_attention:\n    # unpatch flash attention\n    from utils.llama_patch import unplace_flash_attn_with_attn\n    unplace_flash_attn_with_attn()\n    \nimport torch\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer\n\n\nargs.output_dir = \"llama-7-int4-dolly\"\n\n# load base LLM model and tokenizer\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    args.output_dir,\n    low_cpu_mem_usage=True,\n    torch_dtype=torch.float16,\n    load_in_4bit=True,\n) \ntokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n\nLet’s load the dataset again with a random sample to try to generate an instruction.\n\nfrom datasets import load_dataset \nfrom random import randrange\n\n\n# Load dataset from the hub and get a sample\ndataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\nsample = dataset[randrange(len(dataset))]\n\nprompt = f\"\"\"### Instruction:\nUse the Input below to create an instruction, which could have been used to generate the input using an LLM. \n\n### Input:\n{sample['response']}\n\n### Response:\n\"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n# with torch.inference_mode():\noutputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)\n\nprint(f\"Prompt:\\n{sample['response']}\\n\")\nprint(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\nprint(f\"Ground truth:\\n{sample['instruction']}\")\n\nNice! our model works! If want to accelerate our model we can deploy it with Text Generation Inference. Therefore we would need to merge our adapter weights into the base model.\n\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    args.output_dir,\n    low_cpu_mem_usage=True,\n) \n\n# Merge LoRA and base model\nmerged_model = model.merge_and_unload()\n\n# Save the merged model\nmerged_model.save_pretrained(\"merged_model\",safe_serialization=True)\ntokenizer.save_pretrained(\"merged_model\")\n\n# push merged model to the hub\n# merged_model.push_to_hub(\"user/repo\")\n# tokenizer.push_to_hub(\"user/repo\")"
  },
  {
    "objectID": "posts/EAGLE/EAGLE.html",
    "href": "posts/EAGLE/EAGLE.html",
    "title": "EAGLE: Extrapolation Algorithm for Greater Language-model Efficiency",
    "section": "",
    "text": "Auto-regressive decoding has become the de facto standard for large language models (LLMs). This process generates output tokens one at a time, which makes the generation by LLMs both costly and slow. Speculative sampling based methods offer a solution to this challenge. They divide the generation process of LLMs into two stages: the draft stage, where potential tokens are conjectured at a low cost, and the verification stage, where these tokens are validated in parallel through a single forward pass of the LLM.\nSpeculative sampling aims to accelerate generation by minimizing time overhead and increasing the acceptance rate of drafts generated by the original Large Language Model (LLM). Popular methods like Lookahead and Medusa achieve this by reducing overhead and enhancing acceptance rates. Nonetheless, their full potential is limited by the lower accuracy of the drafts they generate.\nEAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), is a simple framework for lossless acceleration. Unlike traditional speculative sampling methods, EAGLE operates the drafting process auto-regressively at the more regular (second-top-layer) feature level and addresses the sampling uncertainty issues in the next-feature prediction problems by integrating tokens from one time step ahead. The acceleration provided by EAGLE is lossless: it involves no fine-tuning of the target LLM, and the generated text maintains the same distribution as that of vanilla auto-regressive decoding.\nCompared with existing speculative sampling-based techniques, the advantages of EAGLE include:\n\nSimplicity: EAGLE adds only a lightweight plug-in (a single transformer decoder layer) to the LLM, which can be easily deployed in a production.\nReliability: EAGLE does not involve any fine-tuning of the original LLM, and the preservation of the output distribution by EAGLE is theoretically guaranteed for both the greedy and non-greedy settings. This is in sharp contrast to Lookahead and Medusa which focuses on greedy settings only.\nSpeed: EAGLE stands out as the fastest framework within the family of speculative sampling. On MT-bench, EAGLE is 3x faster than vanilla decoding, 2x faster than Lookahead, and 1.6x faster than Medusa. Using gpt-fast, EAGLE attains on average 160 tokens/s with LLaMA2-Chat 13B on a single RTX 3090 GPU, compared to 24 tokens/s of Huggingface’s implementations.\n\nPaper : https://arxiv.org/pdf/2401.15077.pdf\nCode : https://github.com/SafeAILab/EAGLE"
  },
  {
    "objectID": "posts/MambaByte/MambaByte.html",
    "href": "posts/MambaByte/MambaByte.html",
    "title": "MambaByte: Token-free Selective State Space Model",
    "section": "",
    "text": "In December 2023, “Mamba : Linear-Time Sequence Modeling with Selective State Spaces” paper was release and with it the whole discussion about Mamba (SSM) been a viable replacement for Transformer base model had started as Mamba achieved 4-5x higher throughput than a Transformer of a similar size. To capitalize on this, there is a growing trend of re-implementing various transformer based LLMs on Mamba (SSM) architecture such as MoE-Mamba and VMamba.\nLooks like Token free LLM is also going in this direction. Typically, Token-free language models learn directly from raw bytes and remove the bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences, and standard autoregressive Transformers scale poorly in such settings. Look no further, we have MambaByte to save.\nMambaByte, a token-free adaptation of the Mamba state space model, trained autoregressive on byte sequences. MambaByte eliminates the need for patching and achieves better performance and computational efficiency compared to bite level Transformers. MambaByte being a straightforward adaptation of the Mamba architecture, utilizes a linear time approach for sequence modeling by incorporating a selection mechanism that is more effective for discrete data like text section parallel scans for linear recurrences\nMambaByte outperforms other byte-level models over several datasets and shows competitive results with subword Transformers, thus serving as a promising tokenization alternative. SSMs also enable significantly fast text generation due to their recurrent nature, making byte models practical.\nPaper : https://arxiv.org/pdf/2401.13660.pdf"
  },
  {
    "objectID": "posts/Towards Conversational Diagnostic AI/Towards Conversational Diagnostic AI.html",
    "href": "posts/Towards Conversational Diagnostic AI/Towards Conversational Diagnostic AI.html",
    "title": "Towards Conversational Diagnostic AI",
    "section": "",
    "text": "With the Med-PaLM series of LLMs Google is one of the few companies you can claim expertise in building medical domain specific LLMs. The latest addition has been AMIE (Articulate Medical Intelligence Explorer).\nAMIE is a conversational medical AI optimized for diagnostic dialogue. AMIE is instruction fine-tuned with a combination of real-world and simulated medical dialogues, alongside a diverse set of medical reasoning, question answering, and summarization datasets.\nAMIE has a self-play based simulated dialogue environment with automated feedback mechanisms to scale its capabilities across various medical contexts and specialities. There are two types of self-play loops in place : 1. An “inner” self-play loop, where AMIE leveraged in-context critic feedback to refine its behavior on simulated conversations with an AI patient agent; 2. An “outer” self-play loop where the set of refined simulated dialogues were incorporated into subsequent fine-tuning iterations.\nDuring online inference, AMIE used a chain-of-reasoning (COR) strategy to progressively refine its response conditioned on the current conversation to arrive at an accurate and grounded reply to the patient in each dialogue turn\nAcross multiple axes corresponding to both specialist physician (28 out of 32) and patient actor (24 out of 26) perspective, AMIE was rated as superior to PCPs while being non-inferior on the rest. However, the results should be interpreted with appropriate caution. Translating from this limited scope of experimental, towards real-world tools, requires significant additional research and development.\nPaper : https://arxiv.org/pdf/2401.05654.pdf"
  },
  {
    "objectID": "posts/ChatQA/ChatQA.html",
    "href": "posts/ChatQA/ChatQA.html",
    "title": "ChatQA: Building GPT-4 Level Conversational QA Models",
    "section": "",
    "text": "With all open source LLM models trying to outperform GPT-4 one may wonder, which one has truly been successful in Conversational QA - one of the elementary use cases of LLMs.\nIntroducing ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. It proposes a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, it fine-tunes a dense retriever on a multiturn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models\nIn addition, it demonstrates that fine-tuning a single-turn query retriever using its own curated conversational QA data performs comparable to the state-of-the-art LLM-based query rewriting model, without the need of extra computational time and potential API cost from rewriting.\nPaper : https://arxiv.org/pdf/2401.10225.pdf\n  arxiv:2401.10225"
  },
  {
    "objectID": "posts/Medusa/Medusa.html",
    "href": "posts/Medusa/Medusa.html",
    "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",
    "section": "",
    "text": "@article{cai2024medusa,\n  title   = {Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads},\n  author  = {Tianle Cai and Yuhong Li and Zhengyang Geng and Hongwu Peng and Jason D. Lee and Deming Chen and Tri Dao},\n  year    = {2024},\n  journal = {arXiv preprint arXiv: 2401.10774}\n}\n \nWhy is it hard to run inference for large transformer models? Besides the increasing size of SoTA models, there are two main factors contributing to the inference challenge\n\nLarge memory footprint. Both model parameters and intermediate states are needed in memory at inference time. For example, The KV cache should be stored in memory during decoding time; E.g. For a batch size of 512 and context length of 2048, the KV cache totals 3TB, that is 3x the model size. Inference cost from the attention mechanism scales quadratically with input sequence length.\nLow parallelizability. Inference generation is executed in an autoregressive fashion, making the decoding process hard to parallel.\n\nThis paper introduces MEDUSA, a method for improving inference in Large Language Models (LLMs) by adding extra decoding heads to predict multiple tokens in parallel. MEDUSA achieves significant speedup without compromising generation quality.\nMedusa adds extra “heads” to LLMs to predict multiple future tokens simultaneously. When augmenting a model with Medusa, the original model stays untouched, and only the new heads are fine-tuned during training. During generation, these heads each produce multiple likely words for the corresponding position. These options are then combined and processed using a tree-based attention mechanism. Finally, a typical acceptance scheme is employed to pick the longest plausible prefix from the candidates for further decoding.\nSo how does Medusa solve the challenges associated with speculative decoding ?\n\nInstead of introducing a new model, we train multiple decoding heads on the same model.\nThe training is parameter-efficient so that even the “GPU-Poor” can do it. And since there is no additional model, there is no need to adjust the distributed computing setup.\nRelaxing the requirement of matching the distribution of the original model makes the non-greedy generation even faster than greedy decoding.\n\nDuring experimentation, Medusa delivers approximately a 2x speed (1.94x) increase across a range of Vicuna models. Will be interesting to see Medusa’s performance with other open source foundational models.\nPaper : https://arxiv.org/pdf/2401.10774.pdf"
  },
  {
    "objectID": "posts/Tuning Language Models by Proxy/Tuning Language Models by Proxy.html",
    "href": "posts/Tuning Language Models by Proxy/Tuning Language Models by Proxy.html",
    "title": "Tuning Language Models by Proxy",
    "section": "",
    "text": "These days capabilities of large pretrained LLMs can be significantly enhanced for specific domains of interest or task using additional fine tuning. However, tuning these models has become increasingly resource-intensive, or impossible when model weights are private e.g. OpenAI GPT-4.\nPaper has introduced the Proxy-Tuning method, a lightweight decoding-time algorithm that can be used to customize large pretrained language models without accessing their weights. It achieves similar results to direct tuning and can be applied for domain adaptation and task-specific finetuning.\nIn the experiments, it apply proxy-tuning to steer a large pretrained (base) model (LLAMA2-13B or 70B) using small, cheaper-to-tune (anti-)experts (based on LLAMA2-7B) for instruction-following, domain adaptation, and task fine tuning. When it applies proxy-tuning to LLAMA2-70B using proxies of only 7B size, it can close 88% of the gap between LLAMA2-70B and it’s truly-tuned CHAT version, when evaluated across knowledge, reasoning, and safety benchmarks.\nPaper : https://arxiv.org/pdf/2401.08565.pdf"
  },
  {
    "objectID": "posts/MoE-LLaVA/MoE-LLaVA.html",
    "href": "posts/MoE-LLaVA/MoE-LLaVA.html",
    "title": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models",
    "section": "",
    "text": "For Large Vision-Language Models (LVLMs), scaling the model can effectively improve performance. However, expanding model parameters significantly increases the training and inferring costs, as all model parameters are activated for each token in the calculation.\nIn contrast, sparse Mixtures of Experts (MoE) effectively scale model capacity by using fixed activated parameters to process data, which has thrived in the field of NLP . Recently, Mistral LLM equipped with the MoE layers has gained popularity in LLMs. Mixtral-MoE8×7B achieves performance comparable to LLaMA 2-70B with fewer computational resources.\nHowever, directly applying MoE to train sparse LVLMs is challenging as it leads to significant performance degradation. Proper initialization is crucial for sparsifying the LVLM, and that’s exactly what MoE-tuning does. MoW-tuning - a novel three-stage training strategy for adapting MoE to LVLMs and preventing the model degradation caused by sparsity.\nMoE-LLaVA model operates by using multiple sparse paths, where each token is directed to different experts through a router. These activated experts collaboratively process the tokens, while inactive paths remain dormant. By stacking MoE encoder layers iteratively, the model creates a sparse pathway to a larger and more potent Large Vocabulary Language Model (LVLM). This approach allows for efficient and effective processing of input data by dynamically routing tokens to appropriate experts for processing.\nDuring experimentation MoELLaVA model demonstrates great potential for multi-modal understanding and hallucination inhibition. MoELLaVA achieves comparable performance to state-of-the-art 7B models with only 3B sparse activated parameters on multiple visual understanding datasets, and outperforms LLaVA-1.5-13B by 1.1% on the POPE hallucination benchmark with 2.2B activated parameters.\nPaper : https://arxiv.org/pdf/2401.15947.pdf\n \n@article{lin2023video,\n  title={Video-LLaVA: Learning United Visual Representation by Alignment Before Projection},\n  author={Lin, Bin and Zhu, Bin and Ye, Yang and Ning, Munan and Jin, Peng and Yuan, Li},\n  journal={arXiv preprint arXiv:2311.10122},\n  year={2023}\n}"
  },
  {
    "objectID": "posts/DeepSpeed-FastGen/DeepSpeed-FastGen.html",
    "href": "posts/DeepSpeed-FastGen/DeepSpeed-FastGen.html",
    "title": "DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference",
    "section": "",
    "text": "Recently Microsoft DeepSpeed launched DeepSpeed-FastGen LLM serving framework, which offers up to 2.3x higher effective throughput compared to state-of-the-art systems like vLLM. DeepSpeed-FastGen leverages the combination of DeepSpeed-MII and DeepSpeed-Inference to provide an easy-to-use serving system.\nDeepSpeed-FastGen is built to leverage continuous batching and non-contiguous KV caches to enable increased occupancy and higher responsivity for serving LLMs in the data center, similar to existing frameworks such as TRT-LLM, TGI, and vLLM. In order to achieve a new level of performance, DeepSpeed-FastGen introduces SplitFuse which leverages dynamic prompt and generation decomposition and unification to further improve continuous batching and system throughput.\nDuring experiment, DeepSpeed-FastGen outperforms vLLM in both throughput and latency. On Llama-2 70B with 4 A100x80GB, DeepSpeed-FastGen demonstrates up to 2x higher throughput (1.36 rps vs. 0.67 rps) at identical latency (9 seconds) or up to 50% latency reduction (7 seconds vs. 14 seconds) while achieving the same throughput (1.2 rps).\nSupported models : LLaMA and LLaMA-2, Mistral, OPT, Falcon, Mixtral, Phi-2, Qwen\nPaper : https://arxiv.org/pdf/2401.08671.pdf"
  },
  {
    "objectID": "posts/Self-Evaluation Improves Selective Generation in Large Language Models/Self-Evaluation Improves Selective Generation in Large Language Models.html",
    "href": "posts/Self-Evaluation Improves Selective Generation in Large Language Models/Self-Evaluation Improves Selective Generation in Large Language Models.html",
    "title": "Self-Evaluation Improves Selective Generation in Large Language Models",
    "section": "",
    "text": "Trustworthiness of LLMs output is one of the important considerations for safe deployment of LLMs in production.Once of the straightforward way to do so is by measuring quality of selected outputs of LLMs. Reinforcement Learning from Human Feedback (RLHF) is one of the widely used method for better quality-calibrated models.\nSince human feedback data is expensive to obtain, the paper has explored the use of token-level self-evaluation to improve the accuracy and quality of generated content by large language models. Experimental results show that self-evaluation based scores are effective in selective generation. and thereby improving the self-evaluation ability of LLMs to improve quality-calibration.\nThe paper proposes methods to convert open-ended generation into token-level evaluation tasks that the LLM can self-evaluate, such as multi-choice question answering or true/false evaluation. Two main methods are proposed: Sample & Select (multi-choice) and Sample & Eval (true/false).\nExperiments on TRUTHFULQA and TL;DR datasets show the self-evaluation scores significantly improve calibration for selective generation compared to sequence likelihood scores. The hybrid method with a “none of the above ’’ option performs the best overall on accuracy, calibration AUC, and selective AUC metrics. Self-evaluation provides a way to improve calibration of LLMs for selective text generation, without needing extra training data.\nOne of the pitfalls I can see in this paper is that all experiments are carried out with PALM-2 and GPT-3 rather than GPT-3.5 or GPT-4 models as OpenAI API does not provide output log-probabilities for them. It will be interesting to see how Self-Evaluation holds up against GPT-3.5 + models.\nPaper : https://arxiv.org/pdf/2312.09300.pdf"
  },
  {
    "objectID": "posts/Self-RAG/Self-RAG.html",
    "href": "posts/Self-RAG/Self-RAG.html",
    "title": "Self-RAG: Learning to Retrieve, Generate and Critique through Self-Reflections",
    "section": "",
    "text": "Self-RAG is a new framework to train an arbitrary LM to learn to retrieve, generate, and critique to enhance the factuality and quality of generations, without hurting the versatility of LLMs. It outperformed ChatGPT and retrieval-augmented LLama2 Chat on six tasks.\nUnlike a widely-adopted Retrieval-Augmented Generation (RAG; Figure left) approach, Self-RAG retrieves on demand (e.g., can retrieve multiple times or completely skip retrieval) given diverse queries, and criticize its own generation from multiple fine-grained aspects by predicting reflection tokens as an integral part of generation. It conducts a segment-wise beam search to select the output that maximizes the utility for diverse preferences.\nEagerly waiting for Self-RAG SciPhi-Self-RAG-Mistral-7B-32k on top of Mistral-7B.\nPaper : https://arxiv.org/pdf/2310.11511.pdf\nModel : https://huggingface.co/selfrag/selfrag_llama2_7b"
  },
  {
    "objectID": "posts/Reciprocal Rank Fusion /Reciprocal Rank Fusion.html",
    "href": "posts/Reciprocal Rank Fusion /Reciprocal Rank Fusion.html",
    "title": "Reciprocal Rank Fusion (RRF) with LambdaMART: Context Tuning for Retrieval Augmented Generation (RAG)",
    "section": "",
    "text": "RAG typically consists of three primary components: Tool Retrieval, Plan Generation, and Execution. Existing RAG methodologies rely heavily on semantic search for tool retrieval, but this approach has limitations, especially when queries lack specificity or context. Context Tuning, can be looked at as a viable solution, a component in RAG that precedes tool retrieval, to provide contextual understanding and context seeking abilities to improve tool retrieval and plan generation.\nPaper proposes a new lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART. Results indicate that context tuning significantly enhances semantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for context retrieval and tool retrieval tasks respectively, and resulting in an 11.6% increase in LLM-based planner accuracy. The lightweight model outperforms other methods and helps reduce hallucinations during planning. However, limitations include the absence of conversation history for multi-turn tasks, constraints on planner context window size affecting performance, and the use of synthetic personas instead of real-world data due to privacy concerns.\nIn summary, Context Tuning enhances RAG showcasing improvements in retrieval, planning accuracy, and hallucination reduction compared to baseline methods.\nPaper : https://arxiv.org/pdf/2312.05708.pdf"
  },
  {
    "objectID": "posts/Infinite-LLM/Infinite-LLM.html",
    "href": "posts/Infinite-LLM/Infinite-LLM.html",
    "title": "Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache",
    "section": "",
    "text": "Introducing DistAttention, a distributed attention algorithm, and DistKV-LLM, a distributed LLM serving system, to improve the performance and resource management of cloud-based LLM services. The system achieved significant throughput improvements and supported longer context lengths compared to existing systems\nTraditionally, serving LLMs with long context lengths poses challenges due to the dynamic and growing memory requirements of the attention layer’s key-value (KV) cache. This makes efficient resource management difficult. Introducing DistAttention, a novel distributed attention algorithm that partitions the KV cache into smaller blocks (“rBlocks”) to enable distributed processing and storage. The paper also introduces DistKV-LLM, a distributed LLM serving engine that coordinates memory usage across GPUs and CPUs in a data center. It manages the distributed KV cache through two components the rManager and gManager.\nThe rManager virtualizes memory for each instance and handles local and remote memory requests. The gManager maintains a global view of memory usage and facilitates allocation between instances. Techniques like overlapping communication and computation, a memory optimization algorithm (DGFM), and a coordination protocol are proposed to improve performance.\nEvaluation on a 32 GPU cluster shows the system supports context lengths 2-19x longer than prior work, with 1.03-2.4x higher throughput. It achieves efficient resource utilization for long-context LLM serving in distributed environments. In summary, the key novelty lies in DistAttention’s distributed approach to processing the attention layer, and DistKV-LLM’s coordinated management of the distributed KV cache memory across\nPaper : https://arxiv.org/pdf/2401.02669.pdf"
  },
  {
    "objectID": "posts/CoT/CoT.html",
    "href": "posts/CoT/CoT.html",
    "title": "Chain of Thought (CoT): The Impact of Reasoning Step Length on Large Language Models",
    "section": "",
    "text": "If you are doing prompt engineering for LLMs then you might have come across Chain of Thought (CoT) prompting, which is significant in improving the reasoning abilities of LLMS. However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown.\nLength of reasoning steps/chains in prompting impacts the performance of large language models (LLMs) on tasks requiring reasoning abilities. Experiments show that increasing the number of reasoning steps in prompts, even without adding new information, significantly improves LLM performance across multiple datasets. Shortening steps diminishes performance. Surprisingly, incorrect rationales can still yield good results if they maintain sufficient step length, suggesting step length is more important than factual accuracy. The benefits of longer steps scale with task complexity: simpler tasks require fewer steps while complex tasks gain more from longer chains. Zero-shot prompting can also be improved by lengthening initial prompts to encourage more reasoning (e.g. “Think step-by-step, think more steps”). Compressing step lengths undermines few-shot CoT (chain-of-thought) performance, regressing it to zero-shot levels. Bigger LLMs require fewer steps to reach peak performance compared to smaller models, showing a relationship between model size and optimal step count. Altering questions within prompts has minimal impact, suggesting step length rather than question details primarily drives reasoning.\nPaper : https://arxiv.org/pdf/2401.04925.pdf"
  },
  {
    "objectID": "posts/Improving Text Embeddings with Large Language Models/Improving Text Embeddings with Large Language Models.html",
    "href": "posts/Improving Text Embeddings with Large Language Models/Improving Text Embeddings with Large Language Models.html",
    "title": "Improving Text Embeddings with Large Language Models using fine-tuned Mistral-7B LLM",
    "section": "",
    "text": "Check out a groundbreaking paper on improving text embeddings with large language models (LLMs) like GPT-4! The authors propose generating synthetic training data for text embedding tasks using LLMs, instead of relying on human-labeled datasets.\nTheir two-step prompt method generates diverse synthetic data for hundreds of thousands of embedding tasks across 93 languages, covering semantic textual similarity, bitext retrieval, and more. The Mistral-7B LLM is fine-tuned on the synthetic data and achieves state-of-the-art results on the MTEB benchmark, outperforming previous models by 2.4 points on average across task categories.\nIn summary, this paper presents an effective and efficient method for improving text embeddings by leveraging data generation with LLMs.\nPaper : https://arxiv.org/pdf/2401.00368.pdf"
  },
  {
    "objectID": "posts/DocLLM/DocLLM.html",
    "href": "posts/DocLLM/DocLLM.html",
    "title": "DOCLLM: A Layout Aware Generative Language Models for Multi model document understanding",
    "section": "",
    "text": "Introducing DocLLM, a groundbreaking generative language model that can understand visually rich documents without the need for expensive image encoders. DocLLM uses a disentangled attention mechanism that captures the interdependencies between text and layout, making it possible to handle irregular layouts and heterogeneous content in visual documents.\nDocLLM’s pre-training objective focuses on infilling missing text segments, and the pre-trained model is fine-tuned using instructions from various datasets, including visual question answering, natural language inference, key information extraction, and document classification.\nEvaluated against comparable models, DocLLM outperforms on 14 out of 16 datasets and generalizes to 4 out of 5 unseen datasets. Its awareness of multi-page documents and page breaks enhances its ability to understand long documents.\nDocLLM can enable the use of more types of data for pre-training language models, allowing documents with complex layouts to be used without much preprocessing. Its cohesive text blocks for pre-training enable meaningful infilling.\nPaper : https://arxiv.org/pdf/2401.00908.pdf"
  },
  {
    "objectID": "posts/Soaring from 4K to 400K/Soaring from 4K to 400K.html",
    "href": "posts/Soaring from 4K to 400K/Soaring from 4K to 400K.html",
    "title": "Soaring from 4K to 400K: Extending LLM’s Context with Activation Beacon",
    "section": "",
    "text": "Activation Beacon is a plug-and-play module for large language models that allows them to process longer contexts with a limited context window, while preserving their original capabilities. It achieves competitive memory and time efficiency and can be trained efficiently with short-sequence data. When dealing with long-sequence data, it resorts to sliding windows for streaming processing, which leads to a superior working efficiency at both inference and training time. With the diversely sampled condensing ratios, it can be effectively learned to support the extensions for a wide scope of context lengths based on short-sequence training data. The experimental study verifies Activation Beacon as an effective, efficient, compatible, low-cost (training) method to extend the context length of LLM.\nPaper : https://arxiv.org/pdf/2401.03462.pdf"
  },
  {
    "objectID": "posts/Comprehensive Survey of Hallucination Mitigation Techniques /Comprehensive Survey of Hallucination Mitigation Techniques .html",
    "href": "posts/Comprehensive Survey of Hallucination Mitigation Techniques /Comprehensive Survey of Hallucination Mitigation Techniques .html",
    "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models",
    "section": "",
    "text": "The paper provides a comprehensive taxonomy categorizing over 32 techniques for mitigating hallucinations in large language models (LLMs). It groups the techniques into categories such as prompt engineering, self-refinement through feedback and reasoning, prompt tuning, and model development. Key mitigation techniques highlighted include:\n\nRetrieval Augmented Generation (RAG) which enhances LLM responses by retrieving information from authoritative external knowledge bases. This helps ground the responses in facts.\nMethods leveraging iterative feedback loops and self-contradiction detection to refine LLM outputs. For example, the Self-Reflection Methodology employs knowledge acquisition and answer generation over multiple cycles.\nPrompt tuning techniques like UPRISE which tune lightweight retrievers to automatically provide task-specific prompts that reduce hallucinations.\nNovel model decoding strategies such as Context-Aware Decoding that override an LLM’s biases by amplifying differences between outputs with and without context.\nUtilizing knowledge graphs and adding faithfulness based loss function\nSupervised Fine-tuning\n\nPaper : https://arxiv.org/pdf/2401.01313.pdf"
  },
  {
    "objectID": "posts/KwaiAgents/KwaiAgents.html",
    "href": "posts/KwaiAgents/KwaiAgents.html",
    "title": "KwaiAgents: Generalized Information-seeking Agent System with LLMs - 2 Open-source models fine tuned for agent systems! Better than GPT-3.5 turbo as an agent!",
    "section": "",
    "text": "Driven by curiosity, humans have continually sought to explore and understand the world around them, leading to the invention of various tools to satiate this inquisitiveness. Despite not having the capacity to process and memorize vast amounts of information in their brains, humans excel in critical thinking, planning, reflection, and harnessing available tools to interact with and interpret the world, enabling them to find answers efficiently. The recent advancements in large language models (LLMs) suggest that machines might also possess the aforementioned human-like capabilities, allowing them to exhibit powerful abilities even with a constrained parameter count. In this paper, we introduce KwaiAgents, a generalized information-seeking agent system based on LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its cognitive core, which is capable of understanding a user’s query, behavior guidelines, and referencing external documents. The agent can also update and retrieve information from its internal memory, plan and execute actions using a time-aware search-browse toolkit, and ultimately provide a comprehensive response. We further investigate the system’s performance when powered by LLMs less advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework, designed to ensure even an open-sourced 7B or 13B model performs well among many agent systems. We exploit both benchmark and human evaluations to systematically validate these capabilities. Extensive experiments show the superiority of our agent system compared to other autonomous agents and highlight the enhanced generalized agent-abilities of our fine-tuned LLMs.\nPaper : https://arxiv.org/pdf/2312.04889v1.pdf"
  },
  {
    "objectID": "posts/SPIN/SPIN.html",
    "href": "posts/SPIN/SPIN.html",
    "title": "Self-Play Fine-Tuning (SPIN): Converts Weak Language Models to Strong Language Models",
    "section": "",
    "text": "Self-Play Fine-Tuning (SPIN) is a new fine-tuning method to improve large language models (LLMs) without needing additional human-annotated data.\nThe key idea is to use a self-play mechanism where the LLM plays against itself. Specifically, the LLM from the previous iteration generates synthetic responses. The new LLM being trained tries to discern between the synthetic responses and real human responses. This iterates, with the new LLM becoming the synthetic data generator for the next iteration.\nThe method is shown to significantly enhance LLMs’ performance on a variety of benchmarks: - On the HuggingFace Open LLM Leaderboard, SPIN improves the average score from 58.14 to 63.16 with over 10% gains on some datasets. - On the MT-Bench benchmark, the score improves from 5.94 to 6.78. - The gains match or exceed models trained with additional human preference data.\nTheoretically, it is proven that SPIN converges when the LLM distribution aligns perfectly with the real data distribution.\nOverall, the self-play approach enables iterative improvement of LLMs without needing extra human feedback or data, converting weak models to strong models by unleashing the full potential of existing labeled data.\nPaper : https://arxiv.org/pdf/2401.01335.pdf"
  },
  {
    "objectID": "posts/LLM Maybe LongLM/LLM Maybe LongLM.html",
    "href": "posts/LLM Maybe LongLM/LLM Maybe LongLM.html",
    "title": "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning",
    "section": "",
    "text": "With only four lines of code modification, the proposed method can effortlessly extend existing LLMs’ context window without any fine-tuning. This work elicits LLMs’ inherent ability to handle long contexts without fine-tuning. The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference.\nIn this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts. Based on this argument, we suggest extending LLMs’ context window by themselves to fully utilize the inherent ability.We propose Self-Extend to stimulate LLMs’ long context handling potential. The basic idea is to construct bi-level attention information: the group level and the neighbor level. The two levels are computed by the original model’s self-attention, which means the proposed does not require any training. With only four lines of code modification, the proposed method can effortlessly extend existing LLMs’ context window without any fine-tuning. We conduct comprehensive experiments and the results show that the proposed method can effectively extend existing LLMs’ context window’s length.\nPaper : https://arxiv.org/pdf/2401.01325.pdf"
  },
  {
    "objectID": "posts/Mamba-Chat/Mamba-Chat.html",
    "href": "posts/Mamba-Chat/Mamba-Chat.html",
    "title": "Mamba-Chat: A Chat LLM based on State Space Models",
    "section": "",
    "text": "Mamba-Chat is the first chat language model based on a state-space model architecture, not a transformer.\nThe model is based on Albert Gu’s and Tri Dao’s work Mamba: Linear-Time Sequence Modeling with Selective State Spaces (paper) as well as their model implementation. This repository provides training / fine-tuning code for the model based on some modifications of the Huggingface Trainer class. Mamba-Chat is based on Mamba-2.8B and was fine-tuned on 16,000 samples of the HuggingFaceH4/ultrachat_200k dataset.\nMamba: Linear-Time Sequence Modeling with Selective State Spaces\nPaper : https://arxiv.org/pdf/2312.00752.pdf"
  },
  {
    "objectID": "posts/BLIVA/BLIVA.html",
    "href": "posts/BLIVA/BLIVA.html",
    "title": "BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions",
    "section": "",
    "text": "Vision Language Models (VLMs), such as OpenAI’s GPT-4, Flamingo, BLIP-2 and LLaVA have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios.\nStandard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context.\nTo improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the model to capture intricate details potentially missed during the query decoding process.\nBLIVA uses a Q-Former to draw out instruction-aware visual features from the patch embeddings generated by a frozen image encoder. These learned query embeddings are then fed as soft prompt inputs into the frozen Language-Learning Model (LLM). Additionally, the system repurposes the originally encoded patch embeddings through a fully connected projection layer, serving as a supplementary source of visual information for the frozen LLM.\nDuring experiment, BLIVA significantly enhances performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA benchmark) and in undertaking general (not particularly text-rich) VQA benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), and achieved 17.72% overall improvement in a comprehensive multimodal LLM benchmark (MME), comparing to baseline InstructBLIP. BLIVA demonstrates significant capability in decoding real-world images, irrespective of text presence.\nPaper : https://arxiv.org/pdf/2308.09936.pdf"
  },
  {
    "objectID": "posts/FIND/FIND.html",
    "href": "posts/FIND/FIND.html",
    "title": "FIND: INterface for Foundation models’ embeDDings",
    "section": "",
    "text": "Foundation models across the vision and language domains, such as GPT4, DALLE-3, SAM and LLaMA etc., have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) .\nHowever, the process of training individual foundation models has become remarkably costly. Furthermore, the full potential of these models remains untapped due to limitations in their fixed output modalities (i.e. text output for Q&A and visual output for image generation). Although techniques such as prompt engineering and adaptive tuning have shown promising results, these approaches struggle with integrating different foundation models off the shelf, expanding the output types and task objectives.\nPaper proposes FIND - a generalized interface for aligning foundation models’ embeddings. The interface enables task-adaptive prototyping, which means we only need to change the configure file instead of the model architecture when adapting to the new tasks. Because all the vision-language tasks are trained in a unified way, this creates an interleaved shared embedding space where vision and language references are replaceable and addable. The proposed interface has the following favorable attributes: 1. Generalizable. It applies to various tasks spanning retrieval, segmentation, etc., under the same architecture and weights. 2. Prototypable. Different tasks are able to be implemented through prototyping attention masks and embedding types. 3. Extendable. The proposed interface is adaptive to new tasks, and new models. 4. Interleavable. With the benefit of multi-task multi-modal training, the proposed interface creates an interleaved shared embedding space.\nFurthermore, FIND has achieved SoTA performance on interleaved image retrieval and segmentation and shows better or comparable performance on generic/interactive/grounded segmentation and image-text retrieval.\nPaper : https://arxiv.org/pdf/2312.07532.pdf"
  },
  {
    "objectID": "posts/Re3val/Re3val.html",
    "href": "posts/Re3val/Re3val.html",
    "title": "Re3val: Reinforced and Reranked Generative Retrieval",
    "section": "",
    "text": "The primary objective of retrieval models is to enhance the accuracy of answers by selecting the most relevant documents retrieved for a given query, ensuring models have sufficient information to help the downstream reasoning process. However, there are two major limitations: First, the generative retrieval does not account for contextual information. Secondly, the retrieval can’t be tuned for the downstream readers as decoding the page title is a non-differentiable operation.\nPaper introduces Re3val - Reinforced and Reranked Generative Retrieval, a novel framework specifically designed to address the challenges in neural information retrieval. Re3val uses Dense Passage Retrieval (DPR) contexts for reranking retrieved page titles, leading to improved RPrecision. Re3val enhances performance by integrating generated questions in pre-training and utilizing REINFORCE during distant supervision. Moreover, Re3val achieves more accurate answers by reading reranked contexts retrieved with the reranked page titles. These advancements enable Re3val to achieve state-of-the-art performance while also offering cost savings by reducing training time and minimizing the need for extensive data labeling.\nTypical Re3val Training Pipeline consists of the following. Generated questions after filtering are integrated into pre-training (1), followed by few-shot training (3) with REINFORCE (2, 4). Retrieved DPR contexts (5), perturbed page titles (6), and queries are concatenated for reranker training (7). Gold and negative passages retrieved with BM-25 are employed (8) for context reranker training (9). Contexts are retrieved using the top 5 reranked titles from KILT (10), where missing titles are imputed with BM-25 (11). DPR contexts are imputed (12) if lacking five gold contexts during FiD model pre-training (13). FiD model is fine-tuned using five reranked contexts (14).\nDuring inference Reranker concatenates retrieved DPR contexts (1), page titles (2), and query to rerank page titles (3). Contexts retrieved with the top five reranked page titles (4), including BM-25 imputed titles (5), are reranked (6). The top-5 reranked contexts are used to generate an answer (7).\nExperimental results demonstrate Re3val’s superiority over the CorpusBrain zero-shot baseline, with an average 8% R-Precision improvement across five tasks using reduced pretraining data. Re3val also achieves an average 1.9% R-Precision increase compared to other generative models via page title reranking with limited taskspecific data. Moreover, by employing a context reranker before grounding, Re3val achieves top-1 KILT scores among generative retrieval models, showing an average 2.1% improvement across five datasets.\nPaper : https://arxiv.org/pdf/2401.16979.pdf"
  },
  {
    "objectID": "posts/Repeat After Me/Repeat After Me.html",
    "href": "posts/Repeat After Me/Repeat After Me.html",
    "title": "Repeat After Me: Transformers are Better than State Space Models at Copying",
    "section": "",
    "text": "Transformers are the workhorse of modern sequence modeling, achieving remarkable performance on a variety of tasks, but they have unavoidable inefficiencies. Specifically, when it comes to memory and compute requirements to predict the next token of a sequence of length. Recently, there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as “generalized state space models” (GSSMs).\nGSSMs have demonstrated impressive performance, but it is not yet clear what these models sacrifice for their improved efficiency, if anything. Well that’s what exactly this paper has found out. It seems that GSSMs are promising in terms of inference-time efficiency but are limited compared to transformer models on tasks that require copying from the input context.\nTo understand this gap in capabilities experimentation was carried out to copy strings of length that are exponential in the number of heads of the transformer under the following scenarios.\n\nCopying: training efficiency. Here the train models copy strings of length ≤ 300 and evaluate string-level accuracy on strings of length 300. Transformers train much faster than GSSMs.\nCopying: length generalization. Here train models copy strings of length ≤ 50 until all models are perfect in-distribution and evaluate string-level accuracy. Evaluating on longer inputs, the transformer models dramatically outperform the GSSMs.\nLookup with pretrained models. Here the task requires looking up and retrieving a number from a “phone book” of varying length that is entirely in context. Pythia (a transformer model) substantially outperforms Mamba (a GSSM) across model sizes.\nCopy: natural language strings. It compares pretrained models on their ability to copy natural language strings sampled from C4 of varying lengths and report string-level accuracy. The transformer models substantially outperform the GSSMs.\nCopy: shuffled strings. To test whether it mattered that the strings were in natural language, randomly shuffle the word order of the strings from the previous experiment. it was found that this degrades performance, especially for the Mamba models.\n\nOverall transformers are better than GSSMs at copying from their input context. However, SSMs have many advantages over transformers when it comes to memory and computational complexity as well as generating long consistent text. Future work should focus on building hybrid architectures that endow state space models with an attention-like mechanism, allowing them to retrieve relevant pieces of text from their input. What do you think ?\nSo why size of input context is so much important in LLMs. In order to understand this let’s look at GPU level. Modern GPUs have a “problem”: they’re too fast. GPUs have become very fast at performing calculations, insomuch that the speed of computation (FLOPs) is much higher than the memory bandwidth (GB/s) or speed of data transfer between memory areas. For example, an NVIDIA A100 can perform 19.5 TFLOPs while having a memory bandwidth of 2TB/s, which is 40 times slower considering each operation is 32 bit.\nThis means that sometimes the bottleneck is not how many operations we perform, but how much data transfer our operations need, and that depends on the size and the quantity of the tensors involved in our calculations. For example, computing the same operation on the same tensor N time may be faster than computing the same operation on N different tensors, even if they have the same size, this is because the GPU may need to move the tensors around. That what happens during memory-intensive tasks such as copying long strings, retrieval and few-shot question answering.\nSo the goal should not only be to optimize the number of operations we do, but also minimize the memory access/transfers that we perform."
  },
  {
    "objectID": "posts/BlackMamba/BlackMamba.html",
    "href": "posts/BlackMamba/BlackMamba.html",
    "title": "BlackMamba: Mixture of Experts for State-Space Models",
    "section": "",
    "text": "State-space models (SSMs) have recently demonstrated competitive performance to transformers at large-scale language modeling benchmarks while achieving linear time and memory complexity as a function of sequence length. Mamba, a recently released SSM model, shows impressive performance in both language modeling and long sequence processing tasks. Simultaneously, mixture-of-expert (MoE) models have shown remarkable performance while significantly reducing the compute and latency costs of inference at the expense of a larger memory footprint.\nSo why not combine Mamba with MoE. Well that is what this paper has explored with BlackMamba, a novel architecture that combines the Mamba SSM with MoE to obtain the benefits of both. This Mamba-MoE architecture have the following improvements over a dense transformer:\n\nMamba: Linear computational complexity with respect to input sequence length for both training and inference. Autoregressive generation in constant time and memory.\nMoE: Inference latency and training FLOPs of the equivalent smaller dense base model, while preserving model quality close to an equi-parameter dense model.\n\nBlackMamba architecture simply replaces both the MLP layer in a transformer with an expert layer, and the attention layer with a mamba SSM layer. Further, it used the SwiGLU activation function for the expert MLPs. For the expert router, it used top-1 routing with a Sinkhorn routing function to load-balance between experts. It utilized a novel custom version of the Sinkhorn algorithm which converges substantially faster than vanilla Sinkhorn. Model was trained using Megatron-LM distributed training framework and was trained in bf16 precision on 300B tokens on custom dataset both 340M/1.5B and 630M/2.8B models .\n\n\nBlackMamba performs competitively against both Mamba and transformer baselines, and outperforms in inference and training FLOPs. BlackMamba inherits and combines both of the benefits of SSM and MoE architectures, combining linear-complexity generation from SSM with cheap and fast inference from MoE. Moreover, BlackMamba shows that it is capable of rapid generation with both linear time and memory cost.\nPaper: https://arxiv.org/pdf/2402.01771.pdf"
  },
  {
    "objectID": "posts/MambaFormer/MambaFormer.html",
    "href": "posts/MambaFormer/MambaFormer.html",
    "title": "MambaFormer: Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks",
    "section": "",
    "text": "State-space models (SSMs), such as Mamba, have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention. Although SSMs exhibit competitive performance, against Transformer models in standard regression in-context learning (ICL) tasks, it falls short in more complex ICL tasks like Vector-valued MQAR. To address these limitations, the paper has introduced a hybrid model, MambaFormer, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently.\nBefore jumping to MambaFormer lets us have a deepdive at in-context learning (ICL). In-context learning (ICL) has emerged as one of the most remarkable capabilities of LLMs like GPT-3 and GPT-4. With just a few demonstration examples (few-short learning), these models can rapidly adapt to new tasks and make accurate predictions without any parameter updates. Numerous research studies have been dedicated to understanding the mechanics of Attention in transformer models that enable such meta-in-context-learning capabilities, either through constructive arguments or extensive experimental investigation. Transformer language models are currently the only large models that have been reported to be capable of ICL in practice.\nSo Can attention-free models perform ICL?\nWell that’s what this paper tries to address. Series of experiment against Transformer, Mumba and MumbaFormer LLMs was carried out for various ICL task such as Linear regression, Sparse linear regression, 2NN regression, Decision Tree, Orthogonal-outlier regression, Many-outlier regression , Sparse parity, Chain-of-Thought I/O and Vector-valued MQAR. MumbaFormer has been the winner in all these tasks. So what is this MambaFormer ?\nMambaFormer is a hybrid architecture that replaces MLP blocks within the transformer with Mamba blocks. Importantly, the architecture also starts with a Mamba block and does not use positional encoding. During ICL evaluations, it was found that MambaFormer consistently achieves a best-of-both-worlds performance compared to Transformer and Mamba.\nPaper: https://arxiv.org/pdf/2402.04248.pdf"
  },
  {
    "objectID": "posts/Hydragen/Hydragen.html",
    "href": "posts/Hydragen/Hydragen.html",
    "title": "Hydragen: High-Throughput LLM Inference with Shared Prefixes",
    "section": "",
    "text": "Transformer-based large language models (LLMs) such as OpenAI GPT3.5 and GPT4 are now deployed to hundreds of millions of users. LLM inference in such scenarios commonly performed on batches of sequences that share a prefix (the system prompt), such as few-shot examples or a chatbot system prompt. Decoding in this large-batch setting can be bottlenecked by the attention operation, which reads large key-value (KV) caches from memory and computes inefficient matrix-vector products for every sequence in the batch. Even with FlashAttention and PagedAttention models redundantly read the prefix’s keys and values from GPU memory when computing attention, regardless of whether the prefix is redundantly stored.\nIn order to eliminate these redundant reads paper introduce Hydragen, a hardware-aware exact implementation of attention with shared prefixes. Hydragen computes attention over the shared prefix and unique suffixes separately. This decomposition enables efficient prefix attention by batching queries together across sequences, reducing redundant memory reads and enabling the use of hardware-friendly matrix multiplications.\nLet’s take an example of LLM chat model inference, which processes many sequences that share a large shared prefix (the system prompt). With Hydragen overall attention is decomposed into attention over the shared prefix (batched across all queries in a batch) and attention over the remaining suffixes (independent across sequences, as is normally done). Hydragen’s attention decomposition allows many matrix vector products to be replaced with fewer matrix-matrix products. Using matrix-matrix products is particularly important as GPUs dedicate an increasingly large ratio of their total FLOPs to tensor cores that are specialized in matrix multiplication\nIn end-to-end benchmarks, Hydragen increases the throughput of CodeLlama-13b by up to 32x over vLLM, a high-performance inference framework that avoids redundant prefix storage but not redundant prefix reads. Hydragen also enables the use of very long shared contexts: with a high batch size, increasing the prefix length from 1K to 16K tokens decreases Hydragen throughput by less than 15%, while the throughput of baselines drops by over 90%. Hydragen generalizes beyond simple prefix-suffix decomposition and can be applied to tree-based prompt sharing patterns, allowing us to further reduce inference time on competitive programming problems by 55%.\nPaper: https://arxiv.org/pdf/2402.05099.pdf"
  },
  {
    "objectID": "posts/Tag-LLM/Tag-LLM.html",
    "href": "posts/Tag-LLM/Tag-LLM.html",
    "title": "Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains",
    "section": "",
    "text": "General-purpose LLMs like LLaMA and GPT-4 have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains such as processing amino acid sequences for proteins (e.g., MTVPDRSEIAG) or SMILES strings for chemical compounds, hampering their adoption for a wide range of scientific problems. Further, Training domain specific LLM requires a significant amount of compute and in-domain data. Fine-tune existing LLMs or perform in-context learning might be another way to go around, but the former is prone to catastrophic forgetting and can hurt the model’s reasoning abilities\nSo can we effectively repurpose general-purpose LLMs for specialized tasks without compromising their linguistic and reasoning capabilities?\nTo address this issue, the paper has introduced Tag-LLM. A novel model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM’s embedding layer to condition the LLM. Two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compressing function-solving instructions. Further, a three-stage protocol is also been introduce to learn these tags using auxiliary data and domain knowledge.\nLet’s consider the task of protein-drug binding affinity prediction. In Tag-LLM method domain tags ⟨Protein⟩, ⟨SMILES⟩ and a function tag ⟨Binding Affinity⟩ is been injects to the input, which are mapped to specially trained embeddings. The model’s last hidden state is passed to a task-specific head to generate predictions of the desired type (e.g., a scalar binding affinity value in this case).\nWhile experimenting with the LLaMA-7B model, on a diverse set of ten domains, encompassing eight languages and two specialized scientific domains (protein sequences and SMILES molecule representations). Results demonstrate that the domain tags can act as effective context switchers, and a function tag can be applied to multiple domains to solve different tasks, achieving zero-shot generalization to unseen problems.\nPaper: https://arxiv.org/pdf/2402.05140.pdf\nGithub: https://github.com/sjunhongshen/Tag-LLM"
  },
  {
    "objectID": "posts/PHATGOOSE/PHATGOOSE.html",
    "href": "posts/PHATGOOSE/PHATGOOSE.html",
    "title": "PHATGOOSE: Learning to Route Among Specialized Experts for Zero-Shot Generalization",
    "section": "",
    "text": "The availability of Huggingface PEFT modules has made it cheap and easy to modularly adapt a given pre-trained model to a specific task or domain. In the meantime, extremely large-scale language models (LLMs) are now being treated as “general-purpose” and often exhibit strong zero-shot generalization. Relying on zero-shot generalization stands in stark contrast to the aforementioned approach of training specialized models for each task such as PEFT.\nSo can we leverage a large collection of specialized modules to improve zero-shot generalization of a base language model ?\nThat’s what this paper has tried to address with PHATGOOSE : Post-Hoc Adaptive Tokenwise Gating Over an Ocean of Specialized Experts, a post-hoc method that enables zero-shot generalization among specialized models. PHATGOOSE recycles PEFT modules by introducing an additional computationally inexpensive step after training the PEFT-based model itself. Specifically, the entire model (including the newly introduced PEFT modules) is frozen and a per-module gate is trained. This gate (whose parameters are shared across sequence positions) comprises a linear layer followed by a sigmoid nonlinearity that determines whether the activation at a given sequence position should be fed into the module or not. Training this gate only requires a small amount of additional compute compared to performing PEFT. The gates for every module across specialized models are then combined to determine how to route different tokens to different modules during inference using a standard “top-k” routing strategy.\nTo test the effectiveness of PHATGOOSE, T5 family models were used to improve zero-shot generalization on standard benchmarks. Notably, it was found that PHATGOOSE not only outperforms prior methods involving merging experts or retrieving a single expert but can also outperform explicit multitask training in some cases. In qualitative analysis, it was found that PHATGOOSE uses a diverse set of modules to perform a given task, thereby combining abilities from multiple specialized models and, in some cases, producing better performance than the single best-performing expert model. Overall, this work sets the groundwork for a promising new framework for the decentralized development of generalist AI systems.\nPaper : https://arxiv.org/pdf/2402.05859.pdf\nGithub : https://github.com/r-three/phatgoose"
  },
  {
    "objectID": "posts/Fiddler/Fiddler.html",
    "href": "posts/Fiddler/Fiddler.html",
    "title": "Fiddler: CPU-GPU Orchestration for Fast Local Inference of MoE Models",
    "section": "",
    "text": "Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architectures are showing remarkable performance on various tasks. By activating a subset of experts inside feed-forward layers with a gating mechanism, such models scale up model size and improve model performance with a small computation overhead. However, running them on resource-constrained settings, where GPU memory resources are not abundant, is challenging due to huge model sizes.\nTo address this paper proposes Fiddler, a fast inference system for LLMs based on Mixture-of-Experts (MoE) architecture at local devices. It allows you to run an unquantized Mixtral-8x7B model (&gt;90GB of parameters) with &gt;3 token/s on a single 24GB GPU. The key idea behind Fiddler is to use the CPU’s computation power. Existing offloading systems primarily utilize the memory resources available on the CPU, while the computation mainly occurs on the GPU. The typical process involves: (1) When some expert weights are missing from the GPU memory, (2) they are copied from the CPU memory to the GPU memory, then (3) GPU executes the expert layer. Although GPU execution is faster, the data movement introduces significant overhead.\nOn the other hand, Fiddler uses CPU computation resources in addition to memory resources. The process is as follows: (1) when some expert weights are missing on the GPU memory, (2) it copies the activation values from the GPU memory to the CPU memory, instead of copying the weights. (3) The computation of the expert layer then happens on the CPU, and (4) the output activation after the expert is copied back to the GPU.\nThis approach significantly reduces the latency of CPU-GPU communication, especially since the size of activations is considerably smaller than the weight size (batch_size x 4096 versus 3 x 4096 x 14336 per expert for the Mixtral-8x7B) for a small batch size. Despite slower computation speeds on the CPU compared to the GPU, avoiding the weight copying process makes this approach more efficient.\nCompared with DeepSpeed-MII and Mixtral offloading, Fiddler is on average faster by 19.4 and 8.2 times for Environment 1, and by 22.5 and 10.1 times for Environment 2.\nPaper : https://arxiv.org/pdf/2402.07033.pdf\nGithub : https://github.com/efeslab/fiddler"
  },
  {
    "objectID": "posts/GraphMamba/Graph Mamba.html",
    "href": "posts/GraphMamba/Graph Mamba.html",
    "title": "Graph Mamba: Towards Learning on Graphs with State Space Models",
    "section": "",
    "text": "Graph Transformers (GTs) has shown promising potential in graph representation learning. GTs, however, have quadratic computational cost, lack inductive biases on graph structures, and rely on complex Positional/Structural Encodings (SE/PE). Recently, Mamba’s outstanding performance in language modeling, outperforming Transformers of the same size and matching Transformers twice its size, motivates several recent studies to adapt its architecture for different data modalities. Mamba architecture is specifically designed for sequence data and the complex non-causal nature of graphs makes directly applying Mamba on graphs challenging.\nTo address all the above mentioned limitations, the paper presents Graph Mamba Networks (GMNs), a new class of machine learning on graphs based on state space models. Recipe for Graph Mamba Networks is simple : (1) Tokenization: the graph is mapped into a sequence of tokens (m ≥ 1: subgraph and m = 0: node tokenization) (2) (Optional Step) PE/SE: inductive bias is added to the architecture using information about the position of nodes and the structure of the graph. (3) Local Encoding: local structures around each node are encoded using a subgraph vectorization mechanism. (4) Token Ordering: the sequence of tokens are ordered based on the context. (Subgraph tokenization (m ≥ 1) has implicit order and does not need this step). (5) (Stack of) Bidirectional Mamba: it scans and selects relevant nodes or subgraphs to flow into the hidden states.\nExperimental evaluations demonstrate that GMNs attain an outstanding performance in long-range, small-scale, large-scale, and heterophilic benchmark datasets, while consuming less GPU memory. These results show that while Transformers, complex message-passing, and SE/PE are sufficient for good performance in practice, neither is necessary.\nPaper : https://arxiv.org/pdf/2402.08678.pdf\nCodes and models will be available soon (Feb 20)."
  },
  {
    "objectID": "posts/Aespa/Aespa.html",
    "href": "posts/Aespa/Aespa.html",
    "title": "Aespa: Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers",
    "section": "",
    "text": "With the increasing complexity of generative AI models, post-training quantization (PTQ) has emerged as a promising solution for deploying hyper-scale models on edge devices such as mobile devices and TVs. Existing PTQ schemes, however, consume considerable time and resources, which could be a bottleneck in real situations where frequent model updates and multiple hyper-parameter tunings are required. As a cost effective alternative, one-shot PTQ schemes have been proposed. Still, the performance is somewhat limited because they cannot consider the inter-layer dependency within the attention module, which is a very important feature of Transformers.\nTo address this paper propose a novel post-training quantization algorithm, called attention-centric efficient and scalable post-training quantization algorithm (Aespa), that pursues both accuracy and efficiency. The key idea of Aespa is to perform quantization layer-wise for efficiency while considering cross-layer dependency by refining quantization loss functions to preserve the attention score. To accelerate the quantization process, a refined quantization objectives for the attention module is been propose. Through a complexity analysis, it was demonstrate that it is about 10 times faster quantization than existing block-wise approaches can be achieved by exploiting the proposed objectives.\nExperimentation on various LLMs including OPT, BLOOM, and LLaMA demonstrates that Aespa approach outperforms conventional quantization schemes such as RTN, QPTQ and Z-Fold by a significant margin, particularly for low-bit precision (INT2).\nPaper : https://arxiv.org/pdf/2402.08958.pdf"
  },
  {
    "objectID": "posts/GRIT/GRIT.html",
    "href": "posts/GRIT/GRIT.html",
    "title": "GRIT : Generative Representational Instruction Tuning",
    "section": "",
    "text": "All text-based language problems can be reduced to either generation or embedding. Creating a single general model that performs such a wide range of tasks has been a long-standing goal. Recently, large language models (LLMs) have emerged as a promising direction for a single multi-task model.\nSo can we train an unified LLM which is equally good both in generation and embedding tasks ?\nIntroducing generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. GRIT combines this two previously disjoint training paradigms by: (1) Generative instruction tuning, whereby the model is trained to respond to instructions by generating an answer and (2) Representational instruction tuning, whereby the model is trained to represent a provided input according to an instruction Via the instructions and separate loss functions the model learns to differentiate the two streams.\nCompared to other open models, resulting LLM trained with GRIT - GRITLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GRITLM 8X7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, it was found that GRIT matches training on only generative or embedding data, thus unifying both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by &gt; 60% for long documents, by no longer requiring separate retrieval and generation models.\nPaper: https://arxiv.org/pdf/2402.09906.pdf\nCode : https://github.com/ContextualAI/gritlm"
  },
  {
    "objectID": "posts/FinBen/FinBen.html",
    "href": "posts/FinBen/FinBen.html",
    "title": "The FinBen: An Holistic Financial Benchmark for Large Language Models",
    "section": "",
    "text": "Recent studies have shown the great potential of advanced LLMs such as GPT-4 on financial text analysis and prediction tasks in the financial domain. While their potential is evident, a comprehensive understanding of their capabilities and limitations for finance, remains largely unexplored.\nExisting financial domain evaluation benchmarks including FLUE, BBTCFLEB, and PIXIU, have a limited scope and are solely focused on financial NLP tasks, primarily targeting language understanding abilities where LLMs have already been extensively evaluated. These benchmarks fail to capture other crucial facets of the financial domain, such as comprehending and extracting domain-specific financial knowledge and resolving realistic financial tasks. As such, their efficacy in evaluating and understanding LLM performance is limited.\nTo bridge this gap, the paper proposes FinBen, the first comprehensive open-sourced evaluation benchmark, specifically designed to thoroughly assess the capabilities of LLMs in the financial domain. FinBen encompasses 35 datasets across 23 financial tasks, organized into three spectrums of difficulty inspired by the CattellHorn-Carroll theory, to evaluate LLMs’ cognitive abilities in inductive reasoning, associative memory, quantitative reasoning, crystallized intelligence, and more.\nEvaluation of 15 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals insights into their strengths and limitations within the financial domain. The findings indicate that GPT-4 leads in quantification, extraction, numerical reasoning, and stock trading, while Gemini shines in generation and forecasting; however, both struggle with complex extraction and forecasting, showing a clear need for targeted enhancements. Instruction tuning boosts simple task performance but falls short in improving complex reasoning and forecasting abilities.\nPaper : https://arxiv.org/pdf/2402.12659.pdf\nCode : https://github.com/The-FinAI/PIXIU"
  },
  {
    "objectID": "posts/TinyLLaVA/TinyLLaVA.html",
    "href": "posts/TinyLLaVA/TinyLLaVA.html",
    "title": "TinyLLaVA: A Framework of Small-scale Large Multimodal Models",
    "section": "",
    "text": "Large language models (LLMs) with large model size can greatly improve task performance but demand expensive computational resources for training. To address this, the LLM communities started releasing smaller-scale models like 7-B and sub-3B versions, maintaining performance parity with larger predecessors like OpenFlamingo and LLaVA series.\nNow more efforts on exploring various ways for efficient training, applying sparse MoE, freezing or lora tuning backbones and deploying in terms of using such tiny LLMs have started and TinyLLaVA is one such effort. TinyLLaVA framework that provides a unified perspective in designing and analyzing small-scale Large Multimodal Models (LMMs). Study shows the effects of different vision encoders, connection modules, language models, training data and training recipes.\nExperiments show that with better training recipes and quality of data, smaller LMMs can achieve on-par performance with larger counterparts, setting new baselines for the research field. Finally it presents a family of small scale LMMs, encompassing three language models: Phi2 , StableLM-2, and TinyLlama, and two vision encoders: CLIP, and SigLIP. Best model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B models such as LLaVA-1.5 and Qwen-VL. Hope these findings can serve as baselines for future research in terms of data scaling, training setups and model selections.\nPaper : https://arxiv.org/pdf/2402.14289.pdf\nCode : https://github.com/DLCV-BUAA/TinyLLaVABench"
  },
  {
    "objectID": "posts/ChunkAttention/ChunkAttention.html",
    "href": "posts/ChunkAttention/ChunkAttention.html",
    "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition",
    "section": "",
    "text": "Self-attention, one of the critical components in LLM, has a poor performance during inference since it performs intensive memory operations on key/value tensors of context tokens (KV cache) and is memory-bound. Due to which it becomes a significant source of inference latency for long sequences. To negate this it’s now common to use prompt prefixes caching during multi-tenant LLMs serving scenarios. But there are some limitations to this (1) predefined system prompts are static and inflexible in frequent refreshes for large scale deployments (2) there is memory waste in case of long system prompts and low hit rate.\nTo address this paper has introduced ChunkAttention, a prefix aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the presence of shared system prompts.\nExperiments show that ChunkLlama (ChunkAttention on Llama) can achieve comparable throughput with SOTA PagedAttention kernel without shared system prompts and can outperform it by 3.2-4.8× with a shared system prompt of 1024 to 4096 tokens on A100 (80G) by applying prefix-aware KV cache and two-phase partition. ChunkLlama can achieve 1.6× (2.9 against 1.8) and 2.3× (2.3 against 1.0) higher throughput compared to vLLM when 1024 and 2048 prefix tokens are shared while maintaining a normalized latency of less than 40 ms/token. The KV cache memory usage is reduced by 70%-90% with long shared prefixes. The peak batch size is also reduced by 20%-40% since ChunkLlama can decode faster.\nPaper : https://arxiv.org/pdf/2402.15220.pdf"
  },
  {
    "objectID": "posts/MobiLlama/MobiLlama.html",
    "href": "posts/MobiLlama/MobiLlama.html",
    "title": "MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT",
    "section": "",
    "text": "MobiLlama, another Small Language Models (SLMs) for resource constrained devices. MobileLlama is a SLM design that initiates from a larger model and applies a careful parameter sharing scheme to reduce both the pre-training and the deployment cost. MobileLlama was trained using the Amber data sources.\nMobiLlama, with an aim to develop accurate SLMs by alleviating the redundancy in the transformer blocks. Different from the conventional SLM design where dedicated feed forward layers (FFN) are typically allocated to each transformer block, it employs a shared FFN design for all the transformer blocks within SLM.\nMobiLlama 0.5B and 0.8B models outperform OLMo-1.17B and TinyLlama-1.1B in terms of pre-training tokens, pre-training time and memory, model parameters, overall accuracy across nine benchmarks and on-device efficiency. MobiLlama achieves comparable accuracy while requiring significantly fewer pre-training data (1.2T tokens vs. 3T tokens), lesser pre-training time and GPU memory along with being efficient in terms of deployment on a resource constrained device\nPaper : https://lnkd.in/gR2RzMiF\nCode : https://lnkd.in/gg_-ArZt\nModel : https://lnkd.in/gXegFw-8"
  },
  {
    "objectID": "posts/ChunkLlama/ChunkLlama.html",
    "href": "posts/ChunkLlama/ChunkLlama.html",
    "title": "ChunkLlama : Training-Free Long-Context Scaling of Large Language Models",
    "section": "",
    "text": "The ability to comprehend and process long-context information is essential for large language models (LLMs) to cater to a wide range of applications effectively. Finetuning is one way to improve LLM long-context capability but due to the high cost of continual pretraining on longer sequences, previously released long-context models are typically limited to scales of 7B/13B.\nTo address this, researchers have introduced Dual Chunk Attention (DCA), a new training-free framework to extrapolate the context window of LLMs. Inspired by efficient chunk-based attention patterns, DCA segments self-attention computations for a long sequence into small chunks, each chunk being smaller than the size of the pretraining window. DCA consists of three components: (1) intra-chunk attention, tailored for processing tokens within the same chunk; (2) inter-chunk attention, for processing tokens between distinct chunks; and (3) successive chunk attention, for processing tokens in successive, distinct chunks. These respective treatments help the model effectively capture both long-range and short-range dependencies in a sequence. In addition to that, the chunk-based attention calculation can be seamlessly integrated with Flash Attention 2, a key element for long-context scaling in the open-source community.\nDual chunk attention provides a training-free and effective method for extending the context window of large language models (LLMs) to more than 8x times their original pre-training length. DCA can be seamlessly integrated with (1) popular extrapolation methods such as Positional Interpolation (PI) and NTK-Aware RoPE; and (2) widely-used libraries for memory-efficient inference like FlashAttention and vLLM.\nDCA achieves performance on practical long-context tasks that is comparable to or even better than that of fine tuned models. When compared with proprietary models, ChunkLlama2 training-free 70B model attains 94% of the performance of gpt-3.5-16k, indicating it is a viable open-source alternative\nPaper : https://arxiv.org/pdf/2402.17463.pdf\nCode : https://github.com/HKUNLP/ChunkLlama"
  },
  {
    "objectID": "posts/BitNet/BitNet.html",
    "href": "posts/BitNet/BitNet.html",
    "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
    "section": "",
    "text": "Large Language Models (LLMs) have demonstrated remarkable performance in a wide range of natural language processing tasks, but their increasing size has posed challenges for deployment and overall cost. One approach to address these challenges is to use post-training quantization to create low-bit models for inference moving from 16 bits to lower bits, such as 4-bit variants. However, post-training quantization is sub-optimal, even though it is widely used in industry LLMs.\nRecent work on 1-bit model architectures, such as BitNet, presents a promising direction for reducing the cost of LLMs while maintaining their performance. BitNext advantages include (1) matrix multiplication of BitNet only involves integer addition (2) lower memory footprint thereby reducing the cost and time of loading weights from DRAM, leading to faster and more efficient inference.\nContinuing on this work, researchers developed a new 1-bit LLM variant called BitNet b1.58, where every parameter is ternary, taking on values of {-1, 0, 1}. An additional value of 0 to the original 1-bit BitNet, resulting in 1.58 bits in the binary system. BitNet b1.58 retains all the benefits of the original 1-bit BitNet. Furthermore, BitNet b1.58 offers two additional advantages. (1) its modeling capability is stronger due to its explicit support for feature filtering, made possible by the inclusion of 0 in the model weights, which can significantly improve the performance of 1-bit LLMs.(2) BitNet b1.58 can match full precision (i.e., FP16) baselines in terms of both perplexity and end-task performance, starting from a 3B size, when using the same configuration (e.g., model size, training tokens, etc.).\nDuring experimental comparison between BitNet b1.58 and LLaMA LLM. It shows that BitNet b1.58 starts to match full precision LLaMA LLM at 3B model size in terms of perplexity, while being 2.71 times faster and using 3.55 times less GPU memory. In particular, BitNet b1.58 with a 3.9B model size is 2.4 times faster, consumes 3.32 times less memory, but performs significantly better than LLaMA LLM 3B.\nSo what next from here ? Yes 1-bit Mixture-of-Experts (MoE) LLMs.\nPaper : https://arxiv.org/abs/2402.17764"
  },
  {
    "objectID": "posts/bGPT/bGPT.html",
    "href": "posts/bGPT/bGPT.html",
    "title": "Beyond Language Models: Byte Models are Digital World Simulators",
    "section": "",
    "text": "Bytes are the foundation of all digital data, devices, and software, from computer processors to operating systems in everyday electronics. Therefore, training models for next byte prediction can potentially lead to a paradigm shift in deep learning, allowing them to truly understand and simulate all activities in the digital world. This has practical benefits not only in conventional areas, but also in some underexplored areas such as boosting cybersecurity, improving computer diagnostics, optimizing data compression, and even advancing complex tasks like reverse-engineering the source code of that software from its binary representation.\nPaper has introduced bGPT, a model designed for binary data processing and digital world modeling by next byte prediction. bGPT segments byte sequences into patches, predicts next patch features with a patch-level decoder, and reconstructs bytes within patches using these features with a byte-level decoder. Its advantages are twofold: 1) Interpreting Digital System: By training on byte sequences, bGPT can learn the patterns of digital systems, enabling it to predict, simulate, and diagnose algorithm or hardware behavior. This ability allows for the reconstruction of complex systems from binary data. 2) Unified Modeling: bGPT integrates various data types into a single framework, treating everything as a byte sequence. This simplifies modeling and allows for easy integration of various data sources.\nExperimentation include two main areas: 1) well-studied tasks like generative modeling and classification on digital media data (e.g., text, audio, and images); and 2) relatively underexplored tasks intrinsic to binary-native operations, including data conversion and CPU state modeling, which represent algorithm and hardware simulation, respectively. bGPT models were pre-trained on IrishMAN for data conversion, CPU states for CPU state modeling, Wikipedia for text (achieved a score of 1.0639 BPB and an accuracy of 92.49%), ImageNet for images (achieved a score of 3.12 BPB and an accuracy of 88.69%), and LibriSpeech for audio (achieved a score of 1.48 BPB and an accuracy of 93.63%). All showcased generative samples from bGPT are produced using the same data preprocessing, model architecture, hyperparameters, and training objectives, without any modality-specific customizations.\nPaper : https://arxiv.org/abs/2402.19155\nModel : https://huggingface.co/sander-wood/bgpt/tree/main\nCode : https://github.com/sanderwood/bgpt"
  },
  {
    "objectID": "posts/VisionLLaMA/VisionLLaMA.html",
    "href": "posts/VisionLLaMA/VisionLLaMA.html",
    "title": "VisionLLaMA : A Unified LLaMA Interface for Vision Tasks",
    "section": "",
    "text": "Large language models, especially the LLaMA family of models, aroused great interest in the research community for multimodal models application, where many methods heavily rely on LLaMA for text processing and CLIP-fashioned vision transformers for visual perception.\nCan the same transformer be used to process both text and 2D images?\nThat is what researchers try to address by unveiling a LLaMA-like vision transformer in plain and pyramid forms, termed VisionLLaMA. VisionLLaMA follows the pipeline of ViT and retains the architecture design of LLaMA as closely as possible. For an image of H × W, it’s firstly transformed and flattened into N = H×W P 2 non-overlapped patches X ∈ R N×C. Then a class token is prepended at the beginning of the sequence and the whole sequence is processed by L VisionLLaMA blocks. The basic block differs from the standard ViT block by two components: self-attention with positional encoding (RoPE) and SwiGLU activation. Researchers also introduce AS2DRoPE (i.e. auto-scaled 2D RoPE), which expands rotated positional encoding from 1D to 2D and utilizes interpolation scaling to accommodate arbitrary resolutions. For the Pyramid Transformer, VisionLLaMA is applied to windows based transformers such as Twin that utilize additive relative position encoding Swin.\nDuring experimentation VisionLLaMA was trained either in supervised or self-supervised schemes to validate the power in a myriad of downstream vision tasks like image classification, detection, and segmentation. Particularly VisionLLaMA image generation capacity was explored under the diffusion framework DiT and SiT to confirm its potency. VisionLLaMA converges much faster than ViT across all models. SiT-LLaMA with 300k training iterations even outperforms the baseline with 400k steps. Further, VisionLLaMA converges faster than DeiT3-L. In conclusion, VisionLLaMA has strong potential to serve as a new vision backbone to facilitate a large realm of downstream applications.\nPaper : https://arxiv.org/pdf/2403.00522.pdf"
  },
  {
    "objectID": "posts/DiffuseKronA/DiffuseKronA.html",
    "href": "posts/DiffuseKronA/DiffuseKronA.html",
    "title": "DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized Diffusion Model",
    "section": "",
    "text": "In recent years, text-to-image (T2I) generation models such as DreamBooth and BLIP-Diffusion have rapidly evolved, generating intricate and highly detailed images that often defy discernment from real-world photographs. Yet encounter limitations due to their intensive fine-tuning demands and substantial parameter requirements. While the low-rank adaptation (LoRA) module within DreamBooth offers a reduction in trainable parameters, it introduces a pronounced sensitivity to hyperparameters, leading to a compromise between parameter efficiency and the quality of T2I personalized image synthesis.\nTo address these constraints, researcher has introduce DiffuseKronA, a novel Kronecker product-based adaptation module that not only significantly reduces the parameter count by up to 35% and 99.947% compared to LoRA-DreamBooth and the original DreamBooth, respectively, but also enhances the quality of image synthesis.\nThe main idea of DiffuseKronA is to leverage the Kronecker product to decompose the weight matrices of the attention layers in the UNet model. Kronecker Product is a matrix multiplication method that captures structured relationships and pairwise interactions between elements of two matrices. In contrast to the low-rank decomposition in LoRA, the Kronecker Adapter in DiffuseKronA offers a higher-rank approximation with less parameter count and greater flexibility.\nDuring experimentation performance of DiffuseKronA was compared LoRA-DreamBooth under the following criteria:\nEnhanced Stability: DiffusekronA is more stable compared to LoRA-DreamBooth. Stability refers to variations in images generated across different learning rates and Kronecker factor/ranks, which makes LoRA-DreamBooth harder to fine-tune.\nText Alignment and Fidelity: On average, DiffusekronA captures better subject semantics and large contextual prompts.\nInterpretability: Leverages the advantages of the Kronecker product to capture structured relationships in attention-weight matrices. More controllable decomposition makes DiffusekronA more interpretable.\nAll in all, DiffusekronA outperforms LoRA-DreamBooth in terms of visual quality, text alignment, fidelity, parameter efficiency, and stability.\nPaper : https://arxiv.org/pdf/2402.17412.pdf"
  },
  {
    "objectID": "posts/Design2Code/Design2Code.html",
    "href": "posts/Design2Code/Design2Code.html",
    "title": "Design2Code: How Far Are We From Automating Front-End Engineering?",
    "section": "",
    "text": "Recent releases of advanced multimodal LLMs such as GPT-4V and Gemini version pro have led to breakthroughs in visual and code generation understanding. This has opened up new possibilities in front-end development, where such multimodal large language models (LLMs) have the potential to translate visual designs into code directly, streamlining the front-end engineering process.\nSo can we take a screenshot of the website design and give this image to LLMs to obtain the full code implementation that can render into the desired web page in a fully end-to-end manner ?\nThat’s what researchers tried to answer with Design2Code task, which provides the first systematic study on this visual design to code implementation task. Researcher introduce Design2Code benchmark, a curated list of 484 real-world webpages as benchmark test cases and develop a set of automatic evaluation metrics to assess how well current multimodal LLMs can generate the code implementations that directly render into the given reference webpages, given the screenshots as input. Further, a suite of multimodal prompting methods such as Direct Prompting, Text-Augmented Prompting and Self-Revision Prompting were also developed, which show their effectiveness on GPT-4V and Gemini Vision Pro.\nFinally, researchers finetune an open-source Design2Code-18B model, with CogAgent-18B as base model, that successfully matches the performance of Gemini Pro Vision. Both human evaluation and automatic metrics show that GPT-4V is the clear winner on this task, where annotators think GPT-4V generated webpages can replace the original reference webpages in 49% cases in terms of visual appearance and content; and perhaps surprisingly, in 64% cases GPT-4V generated webpages are considered better than even the original reference webpages. fine-grained break-down metrics indicate that open-source models mostly lag in recalling visual elements from the input webpages and in generating correct layout designs, while aspects like text content and coloring can be drastically improved with proper finetuning.\nPaper : https://arxiv.org/pdf/2403.03163.pdf"
  },
  {
    "objectID": "posts/InfiMM-HD/InfiMM-HD.html",
    "href": "posts/InfiMM-HD/InfiMM-HD.html",
    "title": "InfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding",
    "section": "",
    "text": "Multimodal Large Language Models (MLLMs) have experienced significant advancements recently, largely driven by the integration of pretrained vision encoders with Large Language Models (LLMs). This trend is exemplified by developments in Flamingo, BLIP-2, LLaVA, and MiniGPT-4. However, challenges persist in accurately recognizing and comprehending intricate details within high-resolution images. This is attributed to pretrained Vision Transformer (ViT) encoders used by MLLMs, where low resolution suffices for basic image-level semantic understanding but is inadequate for detailed, region-level analysis.\nTo address these challenges, researchers introduce InfiMM-HD, a novel architecture specifically designed for processing images of high resolutions with low computational overhead. InfiMM-HD consists of three components: a Vision Transformer Encoder, a Gated Cross Attention Module, and a Large Language Model. InfiMM-HD employs a cross attention mechanism to seamlessly integrate visual information with language models in a low-dimensional space. To address the formidable computational demands associated with high-resolution images, it partitioned the input high resolution image into smaller sub-images, each subjected to individual processing using a shared Vision Transformer (ViT) specifically tailored for relatively lower resolutions.\nFurther, researcher also introduced a four-stage training pipeline that effectively achieves a high-resolution Multimodal Large Language Model with reduced training cost, from initial low-resolution pretraining stage, to continue pretraining stage for knowledge injection and alignment, to dynamic resolution adaption stage for high resolution adoption and finally go through visual instruction fine-tuning stage.\nDuring evaluation under Visual Question Answering (VQA) and text oriented VQA tasks InfiMM-HD outperformed its closest competitor by an average margin of 3.88%. InfiMM-HD was also evaluated on recently proposed MLLMs evaluation benchmarks, including MMMU, MMVet, InfiMM-Eval, MMB, MME, and POPE. Overall InfiMM-HD demonstrates commendable overall performance, highlighting its adaptability and competence across diverse disciplines.\nPaper : https://arxiv.org/pdf/2403.01487.pdf\nCode : https://github.com/InfiMM/infimm-hd/\nModel : https://huggingface.co/Infi-MM/infimm-hd"
  },
  {
    "objectID": "posts/GaLore/GaLore.html",
    "href": "posts/GaLore/GaLore.html",
    "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection",
    "section": "",
    "text": "Training Large Language Models (LLMs) is challenging due to memory constraints from weight and optimizer size. Low-rank adaptation (LoRA) addresses this by adding trainable low-rank matrices to frozen pre-trained weights, reducing parameters and memory usage. However for fine-tuning, LoRA is not shown to reach a comparable performance as fullrank fine-tuning. For pre-training from scratch, it is shown to require a full-rank model training as a warmup, before optimizing in the low-rank subspace. There are two possible reasons: (1) the optimal weight matrices may not be low-rank, and (2) the reparameterization changes the gradient training dynamic.\nTo address this researchers have introduced Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory efficient than common low-rank adaptation methods such as LoRA. GaLore takes a novel approach compared to methods like LoRA and ReLoRA. Instead of doing low-rank projection in weight space (W = W0 + BA) and baking this into the weights every T steps, GaLore performs low-rank projection in gradient space: G = P @ G' @ Q^T. i.e, GaLore recomputes the projection matrices P and Q every T steps using the SVD of the full gradient. This allows the low-rank subspace to maximally adapt to the changing gradient distribution during training.\nSince GaLore computes full gradients first and uses them to update the projection matrices every T steps. This means that it has only one matrix for the gradients (G) instead of two like in LoRA. Well, P and Q must be stored, but they aren’t optimized.\nThe memory savings in GaLore come from the reduction in optimizer state and the lack of newly introduced weights. In their implementation, they only use P for further savings. This is different from LoRA where the savings only come from the low-rank approx of the weights.\nOur approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5% and total training memory by 63.3%, compared to a BF16 baseline. Notably, it demonstrates, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies.\nPaper : https://arxiv.org/pdf/2403.03507.pdf\nCode : https://github.com/jiaweizzhao/galore"
  },
  {
    "objectID": "posts/MoA/MoA.html",
    "href": "posts/MoA/MoA.html",
    "title": "Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models",
    "section": "",
    "text": "Adapter-based fine-tuning methods, such as LoRA, are key to making large language models disruptive in various domain specific applications. LoRA introduces a limited number of domain-specific parameters to retain domain related knowledge and does not need to fine-tuning all parameters of the pre-trained model, which can effectively reduce the training cost of LLMs.\nSo is it possible to obtain multiple customized domain capabilities from a single LLM model with LoRA?\nSimple approach would be to directly mix data from multiple domains together and only add one LoRA module for instruction fine-tuning. However, achieving the right balance of data is crucial in such a case in order to prevent catastrophic forgetting and interference between tasks.\nTo address these limitations and enhance training flexibility, researchers propose the Mixture-of-LoRAs (MoA) architecture – a novel and parameter-efficient tuning method designed for multi-task learning with LLMs. MoA introduces a routing mechanism within a decoder-only model architecture to automatically select LoRA experts. This mechanism is applicable to mainstream Large Language Models (LLMs) and can deploy LoRA modules for multiple tasks using the same LLM while managing limited computing resources. Additionally, To enhance training and inference efficiency, MoA employs parallel processing of different domain samples within a batch during training and a LoRA expert selection approach during inference. This approach harnesses the strengths of different expert models and the base LLM while leveraging the complementary nature of knowledge across different domains.\nExperiments on diverse domain specific tasks such in Finance, Medicine, Stackoverflow and leetcode have demonstrate that MoA outperform both single LoRA and single-LoRA mix in teams of PPL, BLUE and ROUGE-L (Perplexity: 4.0128→3.9450, BLUE: 28.5348→30.5912, ROUGE-L: 37.7877→39.0163), which will also further promote the application of domain specific LLMs.\nPaper : https://arxiv.org/pdf/2403.03432.pdf"
  },
  {
    "objectID": "posts/VideoMamba/VideoMamba.html",
    "href": "posts/VideoMamba/VideoMamba.html",
    "title": "VideoMamba: State Space Model for Efficient Video Understanding",
    "section": "",
    "text": "Mastering spatiotemporal representation is one of the key areas in any video understanding task. However there usually are two challenges associated with it: (1) the large spatiotemporal redundancy within short video clips, and (2) the complex spatiotemporal dependencies among long contexts. Models such as 3D-CNN + Video transformer, S4, RMKV and RetNet tried to resolve above challenges associated with spatio-temporal but none has been successful so far.\nSo can Mamba work well for video understanding?\nThat’s what researchers have tried to address with VideoMamba, a purely SSM-based model tailored for video understanding. VideoMamba harmoniously merges the strengths of convolution and attention in vanilla ViT style. It offers a linear-complexity method for dynamic spatiotemporal context modeling, ideal for high-resolution long videos.\nFramework of VideoMamba strictly follow the architecture of vanilla ViT and adapt the bidirectional mamba block (B-Mamba) for 3D video sequences. Bidirectional Mamba (B-Mamba) block, adapts bidirectional sequence modeling for vision-specific applications. This block processes flattened visual sequences through simultaneous forward and backward SSMs, enhancing its capacity for spatially-aware processing. To apply the B-Mamba layer for spatiotemporal input, VideoMamba extends the original 2D scan into different Spatial-First bidirectional 3D scan, organizing spatial tokens by location then stacking them frame by frame.\nExtensive evaluations reveal VideoMamba’s four core abilities: (1) Scalability in the visual domain without extensive dataset pretraining, thanks to a novel self-distillation technique; (2) Sensitivity for recognizing short-term actions even with fine-grained motion differences; (3) Superiority in long-term video understanding, showcasing significant advancements over traditional feature-based models; and (4) Compatibility with other modalities, demonstrating robustness in multi-modal contexts. Through these distinct advantages, VideoMamba sets a new benchmark for video understanding, offering a scalable and efficient solution for comprehensive video understanding.\nPaper : https://lnkd.in/g8quHTqR\nCode : https://lnkd.in/gFN3sbZ5\nModel : https://lnkd.in/gH85xRkz"
  },
  {
    "objectID": "posts/MoAI/MoAI.html",
    "href": "posts/MoAI/MoAI.html",
    "title": "MoAI: Mixture of All Intelligence for Large Language and Vision Models",
    "section": "",
    "text": "Following the success of the instruction-tuned LLMs, several visual instruction tuning datasets have been meticulously curated to enhance zero-shot vision language (VL) performances in large language and vision models (LLVMs). Due to this several open-source LLVMs such as InstructBLIP, Owen-VL and LLaVA1.5 have been closing the gap in zero-shot VL performances compared to closed-source LLVMs such as GPT-4V, Gemini-Pro, and Qwen-VL-Plus.\nHowever, current LLVMs have disregarded the detailed and comprehensive real-world scene understanding available from specialized computer vision (CV) models in visual perception tasks such as segmentation, detection, scene graph generation (SGG), and optical character recognition (OCR). Instead, the existing LLVMs rely mainly on the large capacity and emergent capabilities of their LLM backbones.\nIn light of this, researcher propose a new LLVM, Mixture of All Intelligence ( MoAI), which leverages auxiliary visual information obtained from various sources: (1) panoptic segmentation , (2) open-world object detection, (3) SGG, and (4) OCR models. To effectively leverage this information, two new modules are introduce: MoAI-Compressor and MoAI-Mixer. The MoAI-Compressor aligns and condenses the verbalized outputs of the external CV models into auxiliary visual information, enabling the efficient use of relevant information for VL tasks. Subsequently, MoAI-Mixer blends three types of intelligence—(1) visual features, (2) auxiliary features from external CV models, and (3) language features—into a cohesive whole.\nDuring experimentation, MoAI-7B surpasses the zero-shot performances, despite being relatively small compared to the considerably larger open-source (InstructBLIP, Owen-VL and LLaVA1.5) and closed source models (GPT-4V, Gemini-Pro, and Qwen-VL-Plus). Notably, those related to real-world scene understanding such as object existence, positions, relations, and OCR without enlarging the model size or curating extra visual instruction tuning datasets. MoAI performs well even on hallucination zero-shot datasets: POPE and HallusionBench, which suggests that accurately recognizing objects and their relationships can help prevent LLVMs from making mistakes.\nPaper : https://arxiv.org/pdf/2403.07508.pdf\nCode : https://github.com/ByungKwanLee/MoAI\nModel : https://huggingface.co/BK-Lee/MoAI-7B"
  },
  {
    "objectID": "posts/RAT/RAT.html",
    "href": "posts/RAT/RAT.html",
    "title": "RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation",
    "section": "",
    "text": "Factual correctness has been one of the growing concerns around LLMs reasoning capabilities. This issue becomes more significant when it comes to zero-shot CoT (Chain of Thought) prompting, aka. “let’s think step-by-step” and long-horizon generation tasks that require multi-step and context-aware reasoning, including code generation, task planning, mathematical reasoning, etc.\nSeveral prompting techniques have been proposed to mitigate this issue, one promising direction, Retrieval Augmented Generation (RAG), which utilizes retrieved information to facilitate more factually grounded reasoning.\nSo can we synergize RAG with prompting techniques such as CoT for sophisticated long-horizon reasoning ?\nWell that is what researchers have done with retrieval-augmented thoughts (RAT) prompting strategy. RAT comprises two key steps: (1) First initial a zero-shot CoT produced by LLMs along with the original task prompt and used it as queries to retrieve the information that could help revise the possibly flawed CoT. (2) Now instead of retrieving and revising with the full CoT and producing the final response at once, used the LLMs to produce the response step-by-step following the CoT (a series of subtasks), and revised only the current thought step based on the information retrieved with task prompt, the current and the past CoTs.\nDuring experimentation several LLMs of varied scales: GPT-3.5 , GPT-4 and CodeLLaMA7b were used with RAT. The results indicate that combining RAT with these LLMs elicits strong advantages over vanilla CoT prompting and RAG approaches. In particular, performances across following selection of tasks was measured : (1) code generation: HumanEval (+20.94%), HumanEval+ (+18.89%), MBPP (+14.83%), MBPP+ (+1.86%); (2) mathematical reasoning problems: GSM8K (+8.36%), and GSMHard (+31.37%); (3) Minecraft task planning (2.96 times on executability and +51.94% on plausibility);( 4) creative writing (+19.19% on human score). Nevertheless, RAT reveals how LLMs revise their reasoning process in a zero-shot fashion with the help of outside knowledge, just as what humans do.\nPaper : https://arxiv.org/pdf/2403.05313.pdf"
  },
  {
    "objectID": "posts/USER-LLM/USER-LLM.html",
    "href": "posts/USER-LLM/USER-LLM.html",
    "title": "USER-LLM: Efficient LLM Contextualization with User Embeddings",
    "section": "",
    "text": "Large language models (LLMs) have revolutionized the field of user modeling and personalization due to its ability to learn and adapt from massive amounts of textual data. By analyzing user interactions and understanding user preferences, LLMs can be leveraged to power recommendations, language generation, summarization, and question answering in ways that are highly relevant and engaging to users.\nHowever, user interaction data is often complex, spanning multiple journeys with sparse data points, various interaction types (multimodal), and potential noise or inconsistencies. This complexity can hinder an LLM’s ability to identify and focus on the most relevant patterns.\nTo address these inherent complexities and limitations of leveraging raw user interaction data with LLMs, researchers have proposed USER-LLM, a novel approach centered around user embeddings. USER-LLM dynamically incorporates user preferences and behaviors from various interaction modalities (e.g., video watch history, ratings, location visits), enhancing LLM understanding and personalization capabilities while supporting various encoder architectures and multimodal fusion mechanisms.\nThe USER-LLM approach consists of two key phases: generating high-quality user embeddings and contextualizing LLMs with these user embeddings. In phase one, a Transformer-based encoder is pretrain on user interaction data, utilizing self-supervised learning to capture behavioral patterns across multiple interaction modalities. Then a multifeature autoregressive Transformer is used to generate embeddings that capture long-range dependencies and contextual relationships within sequential data while handling multimodal user data effectively. In phase two, user embeddings is integrated with an LLM during fine tuning using cross attention, where the LLM’s intermediate text representations attend to the output embeddings from the pretrained user encoder, enabling dynamic context injection (similar to Flamingo).\nDuring experimentation, USER-LLM v.s. DualEnc & Bert4Rec baselines for next item prediction. USER-LLM outperforms the two nonLLM baselines on MovieLens and Google Local review datasets. Overall, USER-LLM showed competitive performance compared with non-LLM baselines and text-prompt-based LLM personalization techniques, particularly in handling long sequences and understanding users deeply.\nPaper : https://arxiv.org/pdf/2402.13598.pdf"
  },
  {
    "objectID": "posts/RAFT/RAFT.html",
    "href": "posts/RAFT/RAFT.html",
    "title": "RAFT: Adapting Language Model to Domain Specific RAG",
    "section": "",
    "text": "Adapting LLMs to the specialized domains, which is essential to many emerging applications, usually takes two paths: in-context learning through Retrieval-Augmented Generation (RAG) and supervised fine-tuning. RAG-based methods allow the Language Model (LLM) to use documents for answering questions but miss out on learning opportunities in fixed domain settings. On other hand, Supervised fine-tuning offers better learning of general document patterns but current approaches don’t effectively use documents during testing or consider retrieval imperfections during training.\nSo can we adapt pre-trained LLMs for Retrieval Augmented Generation (RAG) in specialized domains?\nThat’s what researchers have tried to address with Retrieval Augmented Fine Tuning (RAFT). RAFT is a training recipe that improves the model’s ability to answer questions in an “openbook” in-domain setting. In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don’t help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAFT’s chain-of-thought (COT) style response helps improve the model’s ability to reason.\nCompared with the base Llama2 instruction-tuned model, RAFT train model with RAG does much better in terms of extracting information as well as being robust towards distractors. The gain can be as big as 35.25% on Hotpot QA and 76.35% on Torch Hub evaluation. Compared with DSF (domain specific finetuning) on the specific dataset, RAFT does much better on tasks like HotpotQA and HuggingFace datasets (30.87% on HotpotQA and 31.41% on HuggingFace). Overall, RAFT presents a novel, yet simple technique to improve pretrained LLMs for in-domain RAG.\nPaper : https://arxiv.org/pdf/2403.03432.pdf"
  },
  {
    "objectID": "posts/PERL/PERL.html",
    "href": "posts/PERL/PERL.html",
    "title": "PERL: Parameter Efficient Reinforcement Learning from Human Feedback",
    "section": "",
    "text": "Reinforcement Learning from Human Feedback (RLHF) is one of the most popular methods to align Pretrained Large Language Models (LLMs) with human preferences. It involves fitting a reward model (RM) on human preference data, and then uses this RM to tune the parameters of the LLM using Reinforcement Learning (RL). Typically, there are 2 model training processes as part of the RLHF process: reward model training, and reinforcement learning. However, the complexity and computational cost of the RLHF training process has hindered its adoption.\nSo is there any way to make RLHF more efficient and accessible ?\nThat is what researchers from Google have solved by using Parameter Efficient Reinforcement Learning (PERL), in which LoRA is used to perform reward model training and reinforcement learning.\nDuring reward model training in PERL, LoRA adapters are attached to each attention projection matrix and trained while keeping the language model backbone frozen. These trained adapters are saved and combined with the projection matrices during inference, resulting in a reward model equivalent to a non-LoRA one.\nIn the PERL reinforcement learning loop, language models with LoRA adapters serve as policy models. LoRA adapters are attached to each attention projection matrix, with only these adapters being trained while the language model backbone remains frozen. Training involves policy gradient computation on reward scores, along with KL regularization with the anchor policy. The memory required for training, primarily due to modern optimizers like Adam or Adafactor, is reduced significantly by PERL’s reduction in trainable parameters.\nDuring extensive experiments on various datasets (Reddit TL;DR, BOLT English SMS/Chat dataset, Anthropic’s Helpfulness and Harmlessness dataset and Stanford Human Preferences Dataset), PERL achieves comparable results to conventional RLHF, for which all the model parameters are tuned, while reducing memory usage by approx 50%, and speeding up the training by up to 90% for the Reward Model training, and more modest memory savings of 20%, and speed-up of 10% in the RL loop.\nPaper : https://arxiv.org/pdf/2403.10704.pdf"
  },
  {
    "objectID": "posts/EvoLLM/EvoLLM.html",
    "href": "posts/EvoLLM/EvoLLM.html",
    "title": "Evolutionary Optimization of Model Merging Recipes",
    "section": "",
    "text": "Model merging offers a novel approach to leverage the strengths of multiple pre-trained models. It allows us to combine task-specific models, each potentially fine-tuned for a particular downstream task, into a single unified model. Model merging works surprisingly well and produced many state-of-the-art models on the Open LLM Leaderboard (https://lnkd.in/g3b7eZpm).\nThere are multiple widely used model merge algorithm such as Spherical Linear Interpolation (SLERP), TIES-Merging and DARE each one has its own advantage over one another. Recently, SakanaAI has released a new model merge method called Evolutionary Model Merge, a general method that uses evolutionary techniques to efficiently discover the best ways to combine different models from the vast ocean of different open-source models with diverse capabilities. Evolutionary Model Merge approach encompasses (1) evolving the weights for mixing parameters at each layer in the parameter space (PS); (2) evolving layer permutations in the data flow space (DFS); and (3) an integrated strategy that combines both methods for merging in both PS and DFS. Notice that merging in the PS is not simply copying and stitching of the layers parameters, but also mixes the weights.\nDuring experimentation, the Evolutionary Model merge method was used to automatically evolve for a Japanese Large Language Model (LLM) capable of Math reasoning, and a Japanese Vision-Language Model (VLM). Surprisingly, both models achieve state-of-the-art results on several LLM and Vision benchmarks, while not being explicitly optimized to be good at these benchmarks! EvoLLM-JP-A-v1-7B achieved 52.0%, outperforming individual models Shisa Gamma 7B v1 (9.6%) Abel 7B 002 (30.0%) where as EvoVLM-JP-v1-7B achieved 51.25, outperforming LLaVA-1.6-Mistral-7B (41.10).\nPaper : https://arxiv.org/pdf/2403.13187.pdf\nModel : https://huggingface.co/SakanaAI\nDemo : https://huggingface.co/spaces/SakanaAI/EvoVLM-JP"
  },
  {
    "objectID": "posts/mPLUG/mPLUG.html",
    "href": "posts/mPLUG/mPLUG.html",
    "title": "mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding",
    "section": "",
    "text": "Structure information is critical for understanding the semantics of text-rich images, such as documents, tables, and charts. Most of Existing Multimodal Large Language Models (MLLMs) for Visual Document Understanding consist of a pre-trained visual encoder (e.g. the ViT from CLIP ) and the LLM with a Vision-toText (V2T) module presenting a promising performance on text recognition ability but lack general structure understanding abilities for text-rich document images.\nFor better Visual Document Understanding with MLLMs researchers have proposed Unified Structure Learning on text-rich images for MLLMs and design both structure-aware parsing tasks and multi-grained text localization tasks across 5 domains: document, webpage, table, chart, and natural image.\nTo better encode structure information, a simple and effective SOTA model DocOwl 1.5 was built. DocOwl 1.5 follows the typical architecture of MLLMs, which consists of a visual encoder, a vision-to-text module, and a large language model as the decoder. To better keep the textual and layout information in text-rich images of high resolution, vision-to-text module H-Reducer was design, which can not only maintain the layout information but also reduce the length of visual features by merging horizontal adjacent patches through convolution, enabling the LLM to understand high-resolution images more efficiently.\nFinally, to enhance the text recognition and structure understanding abilities, Unified Structure Learning is performed with structure-aware parsing and multi-grained text localization tasks for all types of images. Then, the model is jointly tuned on multiple downstream tasks of Visual Document understanding.\nDocOwl 1.5 achieves state-of-the-art performance on 10 visual document understanding benchmarks, covering documents (DocVQA, InfoVQA, DeepForm, KLC ), tables (WTQ, TabFact), charts (ChartQA), natural images (TextVQA, TextCaps), and webpage screenshots (VisualMRC). improving the SOTA performance of MLLMs with a 7B LLM by more than 10 points in 5/10 benchmarks.\nPaper : https://arxiv.org/pdf/2403.12895.pdf"
  },
  {
    "objectID": "posts/Cobra/Cobra.html",
    "href": "posts/Cobra/Cobra.html",
    "title": "Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference",
    "section": "",
    "text": "Vision language models (VLMs) like GPT-4, LLaMAadapter, and LLaVA have been instrumental in augmenting LLMs with visual understanding capabilities. VLMs serve as foundational models in tackling a wide array of tasks including Visual Question Answering (VQA), captioning, and visual content generation. However, there has not been much progress to improve VLMs performance mainly due to LLMs transformer architecture, which has a less efficient quadratic computation complexity.\nSo can VLMs based on non transformer architecture like Mamba perform better than transformer one ?\nWell that’s what researchers from Westlake University have addressed with Cobar, an efficient Mamba language model with integrated visual modality. Cobar consists of three components: a vision encoder, a projector, and a Mamba backbone. For vision encoder DINOv2 and SigLIP are fused and used as vision backbone. The intuition is that combining the visual representations, which capture low-level spatial properties from DINOv2 and the semantic properties provided by SigLIP further improves the performance on downstream tasks. The projector is a simple learnable MLP that aligns the features of vision and text. Finally, the LLM backbone is a Mamba language model with 2.8B parameters. During training, the parameters of vision encoders are frozen and the parameters of the projector and Mamba LLM backbone are fine-tuned.\nDuring experimentation several ablation studies on projectors (MLP or Lightweight Downsample Projector), vision backbones (DINOv2 + SigLIP or SigLIP only), and LLM backbones (base model or instruction-tuned chat model) was carried out. Compared with Baselines, Cobra achieves comparable performance to LLaVA v1.5 7B with about 43% of the number of parameters. Inference Speed, Cobra performs 3× ∼ 4× faster than MobileVLM v2 3B and TinyLLaVA 3B on a single NVIDIA A100 80G GPU. Overall, Cobra is competitive in the field of Visual Large Language Models (VLLM), especially in processing visual information and generating natural language descriptions.\nPaper : https://arxiv.org/pdf/2403.14520.pdf"
  },
  {
    "objectID": "posts/SiMBA/SiMBA.html",
    "href": "posts/SiMBA/SiMBA.html",
    "title": "SiMBA: Simplified Mamba-based Architecture for Vision and Multivariate Time series",
    "section": "",
    "text": "Recently, Structured State Space models (SSM) such as Mumba have been pitched as an for Transformer based models especially when it comes to increase efficiency and performance for processing longer input sequences. Mamba, while being the state-of-the-art SSM, is good for longer input sequences but has a stability issue when scaled to large networks for computer vision datasets. This is evident in Mamba when all eigenvalues of matrix A are negative real numbers, which leads to the problem of vanishing/exploding gradients.\nTo address this, researchers from Microsoft have proposed SiMBA (Simplified Mamba-based Architecture). SiMBA introduces Einstein FFT (EinFFT) for channel modeling by specific eigenvalue computations and uses the Mamba block for sequence modeling. EinFFT is specifically designed for complex number representations of frequency components, enabling the effective capture of key patterns in Image patch data with a global view and energy compaction.\nSiMBA illustrates an important trade-off between performance and scalability. Mamba by itself may have stability issues for large networks. Mamba combined with MLP for channel mixing bridges the performance gap for small-scale networks, but may have the same stability issues for large networks. Mamba combined with EinFFT solves stability issues for both small-scale and large networks.\nExtensive performance studies across image and time-series benchmarks demonstrate that SiMBA outperforms existing SSMs, bridging the performance gap with state-of-the-art transformers. Notably, SiMBA establishes itself as the new state-of-the-art SSM on ImageNet and transfer learning benchmarks such as Stanford Car and Flower as well as task learning benchmarks as well as seven time series benchmark datasets.\nPaper : https://arxiv.org/pdf/2403.15360.pdf"
  },
  {
    "objectID": "posts/DRAGIN/DRAGIN.html",
    "href": "posts/DRAGIN/DRAGIN.html",
    "title": "DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models",
    "section": "",
    "text": "Traditional methods of RAG typically rely on single-round retrieval, using the LLM’s initial input to retrieve relevant information from external corpora. While this method is effective for straightforward tasks, it tends to fall short for complex multi-step tasks and long-form generation tasks. In contrast, Dynamic RAG performs multiple times of retrieval during the generation process of LLMs. It includes two steps: identifying the optimal moment to activate the retrieval module (deciding when to retrieve), and crafting the appropriate query once retrieval is triggered (determining what to retrieve). However, current dynamic RAG methods such as IR CoT, RETRO or IC-RALM fall short in one or both aspects.\nTo overcome these limitations, a new framework, DRAGIN, i.e., Dynamic Retrieval Augmented Generation has been introduced based on the real-time Information Needs of LLMs. For the timing of retrieval (deciding when to retrieve), RIND: Real-time Information Needs Detection is used. This method refines the retrieval activation process by evaluating not only the uncertainty of each token, but also its semantic contribution and the impact on the following context. RIND begins by quantifying the uncertainty of each token generated during the LLM’s inference process. This is accomplished by recording the entropy of the token’s probability distribution across the vocabulary. For the formulation of retrieval queries, QFS: Query Formulation based on Self-attention is used, which innovates query formulation by leveraging the LLM’s self-attention across the entire context.\nDuring evaluating the performance of DRAGIN against various baselines across four benchmark datasets: 2WikiMultihopQA, HotpotQA, StrategyQA, and IIRC. DRAGIN achieves superior performance on all tasks, demonstrating the effectiveness of underline methods. In conclusion, DRAGIN provide a lightweight RAG framework that can be incorporated into any Transformer-based LLMs without further training, fine-tuning, or prompt engineering.\nPaper : https://arxiv.org/pdf/2403.10081.pdf"
  },
  {
    "objectID": "posts/RigorLLM/RigorLLM.html",
    "href": "posts/RigorLLM/RigorLLM.html",
    "title": "RigorLLM: Resilient Guardrails for large language models against undesired content",
    "section": "",
    "text": "Large language models (LLMs) have demonstrated impressive capabilities in NLG and different downstream tasks. However, the potential of LLMs to produce biased or harmful outputs, especially when exposed to malicious prompts, remains a significant concern.\nExisting mitigation strategies such as OpenAI content moderation API, Perspective API, Nemo Guardrails and LlamaGuard, which directly moderates both the inputs and outputs of LLMs, presents an effective and efficient solution. However, these solutions primarily rely on LLMs for detecting harmful contents, leaving them susceptible to jailbreaking attacks.\nTo overcome this challenges researchers have proposed RigorLLM (Resilient Guardrails for large language models), a novel and multi-faceted framework for input/output content moderation for LLMs based on different levels of constrained optimizations on corresponding components, such as data generation and safe suffix optimization.\nRigorLLM consists of a training stage and a testing stage. During the training stage, real-world harmful and benign data are embedded with a pre-trained text encoder. Next, embedding space is augmented by generating instances belonging to harmful categories leveraging Langevin dynamics. During the testing stage, first a safe suffix is optimized for the input to alleviate the vulnerability against jailbreak attacks. Then the input is augmented by generating text-level transformations such as paraphrases or summaries using LLMs. Finally, predictions for all augmented texts and the original text is obtained by (1) performing probabilistic KNN in the embedding space and (2) querying a pre-trained LLM. Lastly, we aggregate the predictions from KNN and LLM to derive the final prediction, yielding a comprehensive and reliable harmful content detection mechanism.\nDuring benchmark RigorLLM against state-of-the-art solutions such as OpenAI content moderation API, Perspective API, NeMo Guardrails, and LlamaGuard. RigorLLM not only surpasses these baselines in harmful content detection on various datasets but also exhibits superior resilience to jailbreaking attacks. For example, on the ToxicChat dataset, RigorLLM achieves an improvement of 23% in F1 score compared to the best baseline model. Under jailbreaking attacks, RigorLLM maintains a 100% detection rate on harmful content with different adversarial strings, while other baselines exhibit significantly lower performance.\nPaper : https://arxiv.org/pdf/2403.13031.pdf"
  },
  {
    "objectID": "posts/Mini-Gemini/Mini-Gemini.html",
    "href": "posts/Mini-Gemini/Mini-Gemini.html",
    "title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models",
    "section": "",
    "text": "LLM empowered multi-modality inputs are becoming an essential part of Vision Language Models (VLMs) such as LLaVA and Otter. However, despite these advancements, a significant gap remains between academic initiatives and the prowess of well-established models like GPT-4 and Gemini, which are trained with huge amounts of data and resources.\nSo how to push forward the VLMs approaching well-developed models with acceptable cost in an academic setting?\nIntroducing Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Mini-Gemini employs an any-to-any paradigm, which is adept at handling both image and text as input and output. In particular, Mini-Gemini introduced an efficient visual token enhancement pipeline for input images, featuring a dual-encoder system. It comprises twin encoders, one for high-resolution images and the other for low-resolution visual embedding, mirroring the cooperative functionality of the Gemini constellation. During inference, they work in an attention mechanism, where the low-resolution one generates visual queries, and the high-resolution counterpart provides candidate keys and values for reference.\nTo augment the data quality, data based on public resources, including high-quality responses, task-oriented instructions, and generation-related data are being used. The increased amount and quality improve the overall performance and extend the capability of the model.\nIn general, Mini-Gemini further mines the potential of VLMs and empowers current frameworks with image understanding, reasoning, and generation simultaneously. Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs) from 2B to 34B. It is demonstrated to achieve leading performance in several zero-shot benchmarks and even surpasses the developed private models. For instance Mini-Gemini is on par with Gwen-VL-Plus on the MathVista and MMMU benchmark and even surpasses Gemini Pro and GPT-4V on the widely-adopted MMB benchmark\nPaper: https://arxiv.org/pdf/2403.18814\nCode : https://github.com/dvlab-research/MiniGemini"
  },
  {
    "objectID": "posts/Jamba/Jamba.html",
    "href": "posts/Jamba/Jamba.html",
    "title": "Jamba: A Hybrid Transformer-Mamba Language Model",
    "section": "",
    "text": "Finally, the first production-grade commercially available Mamba-based model delivering best-in-class quality and performance is here. Introducing Jamba, a novel architecture which combines Attention and Mamba layers, with MoE modules. Here are some key features of Jamba\n\n3X throughput on long contexts compared to Mixtral 8x7B\nDemocratizes access to a massive 256K context window\nThe only model in its size class that fits up to 140K context on a single GPU\n\nSo how does Jamba provide flexibility for balancing performance and memory requirements, while the previous Mamba-Transformer based models were not able to do so ?\nIt all comes down to Jamba architecture choice, which aims to provide not only a small number of active parameters but also an 8x smaller KV cache compared to a vanilla Transformer. The basic component is a Jamba block, which may be repeated in sequence. Each Jamba block is a combination of Mamba or Attention layers. Each such layer contains either an attention or a Mamba module, followed by a multi-layer perceptron (MLP). Further, In Jamba some of the MLPs may be replaced by MoE layers, which helps increase the model capacity while keeping the active number of parameters, and thus the compute, small\nIn summary, the different degrees of freedom in the Jamba architecture are:\n* l: The number of layers. * a : m : ratio of attention-to-Mamba layers.\n* e: how often to use MoE instead of a single MLP. * n: total number of experts per layer. * K: number of top experts used at each token.\nGiven this design space, Jamba provides flexibility in preferring certain properties over others. For example, increasing m and decreasing a, i.e, increasing the ratio of Mamba layers at the expense of attention layers, reduces the required memory for storing the key-value cache thus reducing overall memory space. Increasing the ratio of Mamba layers also improves throughput, especially at long sequences. However, decreasing a might lower the model’s capabilities. Other architecture details are standard, including grouped-query attention (GQA), SwiGLU activation function, and load balancing for the MoE. The vocabulary size is 64K. The tokenizer is trained with BPE and each digit is a separate token.\nPaper : https://arxiv.org/pdf/2403.19887.pdf"
  },
  {
    "objectID": "posts/Gecko/Gecko.html",
    "href": "posts/Gecko/Gecko.html",
    "title": "Gecko: Versatile Text Embeddings Distilled from Large Language Models",
    "section": "",
    "text": "Recent advancement in the Text Embedding model has been instrumental for various downstream tasks including document retrieval, sentence similarity, classification, and clustering. However, there present challenges in developing general-purpose text embedding models as such models require large amounts of training data to comprehensively cover desired domains and skills. Large language models (LLMs) offer a powerful alternative in such scenarios.\nSo to what extent can we leverage LLMs directly to improve text embedding models?\nIntroduced Gecko, a versatile text embedding model distilled from large language models. Gecko utilizes a two-step distillation process that begins with generating diverse, synthetic paired data using an LLM. Next, data quality is further refined by retrieving a set of candidate passages for each query, and relabeling the positive and hard negative passages using the same LLM.\nTo train Gecko, a 1.2B parameter pre-trained transformer base language model is being used that undergoes two additional training stages: pre-finetuning and fine-tuning. During pre-finetuning stage model is been trained on large self-supervised text corpus such as question-answer pairs from online forums and QA websites.\nFor fine-tuning, Gecko uses a novel fine-tuning dataset FRet, the Few-shot Prompted Retrieval dataset. Given a sampled passage from the web, FRet first utilizes LLMs to generate a relevant task and a query for the passage. Then, each query and task is fed into a pre-trained embedding model to obtain nearest neighbor passages, which are then scored by the LLM to mine positive and negative passages.\nBy combining this LLM-generated and LLM-ranked data with human-annotated data, Gecko-1B with 768-dimensional embeddings achieves the best performance on the popular MTEB benchmark, Gecko with 256 embedding dimensions outperforms all existing entries with 768 embedding size. Gecko with 768 embedding dimensions achieves an average score of 66.31, competing with 7x larger models and 5x higher dimensional embeddings. Moreover, Gecko often outperforms other systems that use either larger base models (7B) or higher dimensional embeddings (1k to 4k).\nPaper : https://arxiv.org/pdf/2403.20327.pdf"
  },
  {
    "objectID": "posts/sDPO/sDPO.html",
    "href": "posts/sDPO/sDPO.html",
    "title": "sDPO: Don’t Use Your Data All at Once",
    "section": "",
    "text": "As development of large language models (LLM) progresses, aligning them with human preferences has become increasingly important to ensure safety and usefulness of the model. Thus, reinforcement learning techniques such as proximal policy optimization (PPO) are key in this alignment phase, despite their complexity.\nTo resolve this complexity during LLM training direct preference optimization (DPO) has been widely used mainly for its simplicity and effectiveness. DPO involves curating preference datasets using human or strong AI (e.g., GPT4 ) judge to select chosen and rejected responses to questions. These datasets are then used to train LLMs by comparing log probabilities of chosen versus rejected answers. However, due to unavailability of log probabilities for proprietary models like GPT-4 the reference model is simply set as the base SFT model, which is a much weaker alternative with potentially misaligned preferences.\nTo address this researchers have proposed ‘stepwise DPO’, named sDPO, where preference datasets are divided to be used in multiple steps. The aligned model from the previous step is used as the reference and target models for the current step. The reference model is used to calculate the log probabilities and the target model is trained using the preference loss of DPO at each step.\nEvaluating results of applying sDPO to the SFT base model such as pretrained-only ‘SOLAR 10.7B’ to the instruction-tuned ‘SOLAR 10.7B + SFT’, seen an increase of +5.24 in terms of H4. Applying sDPO on SOLAR 10.7B + SFT further increases the H4 score upto 74.31, an improvement of +4.80. Notably, ‘SOLAR 10.7B + SFT + sDPO’ outperforms other larger models such as Mixtral 8x7BInstruct-v0.1, despite the smaller number of parameters. This highlights that effective alignment tuning could be the key to unlocking next level performance for smaller LLMs.\nPaper : https://arxiv.org/pdf/2403.19270.pdf"
  },
  {
    "objectID": "posts/Mixture-of-Depths/Mixture-of-Depths.html",
    "href": "posts/Mixture-of-Depths/Mixture-of-Depths.html",
    "title": "Mixture-of-Depths: Dynamically allocating compute in transformer-based language models",
    "section": "",
    "text": "Transformer FLOPs Equation or FLOPs-per-token is one of the key attributes in determining computation budget for any transformer base LLM models. Usually in language models not all tokens and sequences require the same time or effort to accurately make a prediction. And yet, Transformer-based language models spread FLOPs uniformly across input sequences.\nSo can transformers learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimizing the allocation along the sequence for different layers across the model depth ?\nThis is what researchers from Google are trying to address with the Mixture-of-Depths (MoD) Transformer. MoD is similar to mixture-of-experts (MoE) transformers where in a router is used to choose among potential computational paths. But unlike in MoE transformers the possible choices are a standard block’s computation (i.e., self-attention and MLP) or a residual connection.\nIn general, MoD method enforces a total compute budget by capping the number of tokens (𝑘) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-𝑘 routing mechanism. Since 𝑘 is defined a priori, this simple procedure uses a static computation graph with known tensor sizes. Nevertheless, since the identities of the 𝑘 tokens are fluid, MoD method can expend FLOPs non-uniformly across the time and model depth dimensions. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently.\nDuring evaluation Mixture-of-Depths (MoD) Transformer models not only match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50% faster to step during post-training sampling.\nPaper : https://arxiv.org/pdf/2404.02258.pdf"
  },
  {
    "objectID": "posts/ReFT/ReFT.html",
    "href": "posts/ReFT/ReFT.html",
    "title": "ReFT: Representation Finetuning for Language Models",
    "section": "",
    "text": "Parameter-efficient finetuning (PEFT) methods have been instrumental in rapid adoption of fine tuned domain specific LLMs. PEFTs not only reduced memory usage and time during LLMs training but also shown to achieve similar performance to full finetuning LLMs in many practical settings. Recent PEFT adapters such as LoRA and QLoRA have further fuelled this growth.\nA hallmark of current state-of-the-art PEFTs is that they modify weights rather than representations. It has been shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative.\nSo instead of weight updates can we learn interventions to modify a small fraction of model representations ?\nIntroducing Representation Finetuning (ReFT), a family of intervention-based representation finetuning methods. Typically, an intervention I is a tuple ⟨Φ, P, L⟩ that encapsulates a single inference-time modification of the representations computed by a Transformer-based LM. The three components of an intervention are * The intervention function Φ ∶ R d → R d with learned parameters ϕ. * A set of input positions P ⊆ {1, . . . , n} that the intervention is applied to. * The layer L ∈ {1, . . . , m} at which the intervention is applied. A ReFT method is a constrained set of f non-overlapping interventions I = {I1, . . . , If }\nFurther, researchers have also introduced a strong instance of the ReFT family called Low-rank Linear Subspace ReFT (LoReFT). LoReFT is a parameterization of ReFT that intervenes on hidden representations in the linear subspace spanned by a low-rank projection matrix, building directly on the distributed alignment search (DAS) method.\nDuring evaluation of LoReFT on LLaMA-family models against existing PEFTs on standard benchmarks from four domains: commonsense reasoning, arithmetic reasoning, instruction following, and NLU it was found that LoReFT uses 10×–50× times fewer parameters while achieving state-of-the-art performance. These findings indicate that ReFT methods may emerge as more efficient and effective alternatives to weight-based PEFTs.\nPaper : https://arxiv.org/pdf/2404.03592.pdf"
  },
  {
    "objectID": "posts/SoS/SoS.html",
    "href": "posts/SoS/SoS.html",
    "title": "Stream of Search (SoS): Learning to Search in Language",
    "section": "",
    "text": "Transformer-based auto-regressive models such as GPT have shown remarkable performance in generative tasks but struggle when it comes to complex decision-making and reasoning tasks requiring search.\nThere are two main factor attributes for this performance: (1) the snowballing of errors, where a single mistake can compound and lead to increasingly poor performance in subsequent steps, and (2) a difficulty in ‘lookahead tasks’, where the model must predict the consequences of its actions several steps ahead. Both of these issues can be attributed to limited ability to search and backtrack. Even supplementing language models with search ability during inference has not shown much improvement.\nWhat if language models can learn to search during training itself ? thereby improving its ability to discover more flexible search strategies through self-improvement and better equipped itself to handle error compounding and lookahead tasks.\nIntroducing Stream of Search (SoS) framework, a unified language for search that captures an array of different symbolic search strategies such as exploration, backtracking, and pruning. To understand SoS let’s instantiate a simple countdown problem with input numbers and a target number. The goal here is to combine input numbers with simple arithmetic operations to get to the given target number. To start with, the Stream of Search (SoS) dataset is created containing search trajectories generated by diverse search strategies, including exploration and backtracking. Then a transformer-based language model is pretrain on this SoS dataset. Finally, the model is finetuned with two policy improvement methods: Advantage-Induced Policy Alignment (APA) optimize for correctness and Self-Taught Reasoner (STaR) for expert iteration.\nDuring evaluation, SoS models were able to solve approximately 36% of the previously unsolved problems and about 4% of the difficult problems including problems that cannot be solved by any of the heuristic solvers. Finally, SoS + APA and SoS + STaR models also have better models of the environment, making fewer errors while searching, and finding the solution more quickly.\nPaper : https://arxiv.org/pdf/2404.03683.pdf"
  },
  {
    "objectID": "posts/DNO/DNO.html",
    "href": "posts/DNO/DNO.html",
    "title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences",
    "section": "",
    "text": "Fine-tuning LLMs using Reinforcement Learning from Human Feedback (RLHF) has alway been a preferred way for making LLMs more useful by aligning them with human values or preferences. RLHF optimization toward the preference is usually done using a two-step procedure: reward learning, and policy optimization (through RL) to maximize the learned reward. RLHF fundamentally relies on the reward maximization framework, wherein reward-based preferences are governed by, e.g., the BT model.\nHowever, this reward maximization framing poses following major limitations: (1) Reward functions, used to score responses to inputs, can’t capture complex preferences like intransitive or cyclic preferences between multiple outputs. (2) Reward functions in practice can quickly become “stale” as the distribution of the policy shifts under training leaving them vulnerable to “reward hacking (3) Even when preferences can be perfectly expressed in reward-based Behavioral Transfer (BT) models, optimizing towards rewards can lead to problematic behaviors.\nTo address these weaknesses, researchers from Microsoft have introduced Direct Nash Optimization (DNO), a provable and scalable RLHF algorithm that marries the simplicity and stability of contrastive learning with theoretical generality from optimizing general preferences.\nDirect Nash Optimization, addresses these challenges by approximating soft policy iteration updates with a regression-based contrastive objective in a batched manner, which is a much more stable and forgiving learning objective, and establish a concentration bound of Oe( 1/N) on the squared total variation error between the learned policy and its target of the soft policy iteration update at any given iteration t. Theoretically, DNO converges to the Nash equilibrium on-average, but in practice enjoys monotonic improvement across iterations. Because DNO is a batched on-policy algorithm using a regression-based objective, its implementation is straightforward and efficient.\nDuring experimentation, a 7B parameter Orca-2.5 model is aligned by DNO and achieves the state-of-the-art win-rate against GPT-4-Turbo of 33% on AlpacaEval 2.0 (even after controlling for response length), an absolute gain of 26% (7% → 33%) over the initializing model. It outperforms models with far more parameters, including Mistral Large, Self-Rewarding LM (70B parameters), and older versions of GPT-4. Moreover, DNO enjoys monotonic improvement across iterations which helps it improve even over a strong teacher (such as GPT-4).\nPaper : https://arxiv.org/pdf/2404.03715.pdf"
  },
  {
    "objectID": "posts/RHO-1/RHO-1.html",
    "href": "posts/RHO-1/RHO-1.html",
    "title": "RHO-1: Not All Tokens Are What You Need",
    "section": "",
    "text": "High quality training data sets are crucial to boost LLMs performance. Various data filtering techniques such as heuristics and classifiers are being utilized to select such training dataset. However, despite thorough document-level filtering, high-quality datasets still contain many noisy tokens that can negatively affect training.\nFurther, common corpus at the token level may include undesirable content like hallucinations or highly ambiguous tokens that are hard to predict. Applying the same loss to all tokens can result in wasted computation on non-beneficial tokens, possibly limiting LLM’s potential to merely mediocre intelligence.\nSo are all tokens in a corpus equally important for language model training ?\nApparently not, researchers have found two groups of tokens exist during training : “easy tokens” that are already learned, and “hard tokens” that exhibit variable losses and resist convergence. These tokens can lead to numerous ineffective gradient updates. To explain this researchers have introduced a new language model called RHO-1. Unlike traditional LMs that learn to predict every next token in a corpus, RHO-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that are aligned with the desired distribution. The approach involved\n\nTrain a reference language model on high-quality corpora.\nUse above reference model to score each token in a corpus using its loss\nFinally, train a language model only on those tokens that exhibit a high excess loss between the reference and the training model.\n\nDuring evaluation, RHO-1-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively — matching DeepSeekMath with only 3% of the pretraining tokens. Furthermore, when pretraining on 80B general tokens, RHO-1 achieves 6.8% average enhancement across 15 diverse tasks, increasing both efficiency and performance of the language model pre-training.\nPaper : https://arxiv.org/pdf/2404.07965.pdf"
  },
  {
    "objectID": "posts/TR-DPO/TR-DPO.html",
    "href": "posts/TR-DPO/TR-DPO.html",
    "title": "Trust Region Direct Preference Optimization (TR-DPO) : Learn Your Reference Model for Real Good Alignment",
    "section": "",
    "text": "Aligning large language models with human preferences (RLHF) has become increasingly important to ensure safety and overall usefulness of the model. Traditionally, the alignment of language models hinges upon the training objective defined as\n\nwhere D is the collection of training data, πθ is the policy being optimized, πref is the reference model that is usually a supervised fine-tuned LM (SFT policy), and rϕ(x, y) denotes the Reward Model (RM) that is trained in line with human preferences.\nRecent methods such as The Direct Preference Optimization (DPO) reformulates the optimization task of RLHF and eliminates the Reward Model while tacitly maintaining the requirement for the policy to be close to the SFT policy.\n\nwith the dataset D consisting of tuples (x, yw, yl) in which x represents a text prompt, while yw and yl stand for the human annotator’s preferred and less preferred continuations, respectively\nThis practice prompts us to question: Why does the reference model remain static during training ?\nTo address this researchers have introduced a novel conceptualization of the training process for alignment algorithms, dubbed Trust Region Direct Preference Optimization (TR-DPO). TR-DPO features updating the reference policy during training—either by softly integrating πθ into πref using a weighted approach or by outright replacing the reference policy with πθ after a predetermined number of steps.\nTraditionally, vanilla DPO uses a fixed reference policy during the training, where as TR-DPO update it either with soft-update for which parameters of πθ are merged into parameters of πref with some weight α, or with hard-update (right) for which we copy parameters of πref into reference policy once in a predetermined number of training steps.\nDuring evaluation of TR-DPO against DPO on the Anthropic HH and TLDR datasets. TR-DPO outperforms DPO by up to 19%, measured by automatic evaluation with GPT-4. The new alignment approach that we propose allows us to improve the quality of models across several parameters at once, such as coherence, correctness, level of detail, helpfulness, and harmlessness.\nPaper : https://arxiv.org/pdf/2404.09656.pdf"
  },
  {
    "objectID": "posts/MEGALODON/MEGALODON.html",
    "href": "posts/MEGALODON/MEGALODON.html",
    "title": "MEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length",
    "section": "",
    "text": "The Transformer architecture is backbone of any production LLMs, but despite its remarkable capabilities, it faces challenges with quadratic computational complexity and limited inductive bias for length generalization, making it inefficient for long sequence modeling.\nTechniques like efficient attention mechanisms and structured state space models (SSM) have been introduced to overcome these limitations, aiming to enhance scalability and performance. However, the practical application of these methods empirically underperform Transformers in pretraining efficiency and downstream task accuracy.\nTo address this researchers have introduced MEGALODON, an improved MEGA architecture (exponential moving average with gated attention), which harnesses the gated attention mechanism with the classical exponential moving average (EMA). To further improve the capability and efficiency of MEGALODON on large-scale long context pre-training, multiple novel technical components have been introduced.\nFirst, MEGALODON introduces the complex exponential moving average (CEMA) component, which extends the multi-dimensional damped EMA in MEGA to the complex domain. Then, MEGALODON proposes the timestep normalization layer, which generalizes the group normalization layer to autoregressive sequence modeling tasks to allow normalization along the sequential dimension. To improve large-scale pretraining stability, MEGALODON further proposes normalized attention, together with pre-norm with two-hop residual configuration by modifying the widely-adopted pre and post-normalization methods. By simply chunking input sequences into fixed blocks, as is done in MEGA-chunk, MEGALODON achieves linear computational and memory complexity in both model training and inference.\nIn a controlled head-to-head comparison with LLAMA2, MEGALODON achieves better efficiency than Transformer in the scale of 7 billion parameters and 2 trillion training tokens. MEGALODON reaches a training loss of 1.70, landing mid-way between LLAMA2- 7B (1.75) and 13B (1.67). The improvements of MEGALODON over Transformers are robust throughout a range of benchmarks across different tasks and modalities\nPaper : https://arxiv.org/pdf/2404.08801.pdf"
  },
  {
    "objectID": "posts/RecurrentGemma/RecurrentGemma.html",
    "href": "posts/RecurrentGemma/RecurrentGemma.html",
    "title": "RecurrentGemma: Moving Past Transformers for Efficient Open Language Models",
    "section": "",
    "text": "Recently Google has released RecurrentGemma, an open language model which uses Google’s novel Griffin architecture. Griffin combines linear RNN with local attention to achieve excellent performance on language. It has a fixed-sized state, which reduces memory use and enables efficient inference on long sequences.\nTypically, transformer architecture KV cache grows linearly with sequence length. Although there are various techniques such as local attention to reduce the cache size but it comes at the expense of reduced performance. In contrast, RecurrentGemma-2B compresses input sequences into a fixed-size state without sacrificing performance. This reduces memory use and enables efficient inference on long sequences.\nDuring evaluating RecurrentGemma-2B across a broad range of domains, using a combination of automated benchmarks and human evaluation. RecurrentGemma-2B achieves comparable performance to Gemma-2B, even though Gemma-2B was trained on 50% more tokens. In creative writing and coding tasks, RecurrentGemma-2B-IT achieves a 43.7% win rate against the larger Mistral 7B model. A key advantage of RecurrentGemma is its inference speed, which is roughly 40k tokens per second, considerably higher than your average transformer architecture based models In conclusion, RecurrentGemma-2B offers the performance of Gemma, while achieving higher throughput during inference, especially on long sequences.\nPaper : https://lnkd.in/g3YS_su9"
  },
  {
    "objectID": "posts/TransformerFAM/TransformerFAM.html",
    "href": "posts/TransformerFAM/TransformerFAM.html",
    "title": "TransformerFAM: Feedback attention is working memory",
    "section": "",
    "text": "While Transformers have revolutionized deep learning, their quadratic attention complexity hinders their ability to process infinitely long inputs. One of the widely used approaches to handle long context inputs is Sliding Window Attention or Block Sliding Window Attention (BSWA).\nDuring inference, LLMs allocates KV cache twice the window length at the beginning, and then using a ring buffer to update the necessary components at each step, in order to avoid memory allocation and copying operations every step, which are computationally expensive. With SWA or BSWA only a fixed ring buffer (block size + memory segment) needs to cache, which keeps memory usage constant regardless of token length enabling LLMs to generate tokens of infinite length. However, TransformerBSWA has a limited receptive field, approximately equal to the model depth × window size. This means later generated tokens may not be related to tokens outside this receptive field, such as the prompt.\nTo address this limitation, researchers have proposed a novel Transformer architecture called Feedback Attention Memory (FAM) or TransformerFAM in short that enables attention to both homogeneous sequence data and latent representations via a feedback loop. This architecture change fosters the natural emergence of working memory within Transformers allowing it to process indefinitely long sequences.\nTo understand it better we require to see attention patterns in the Transformer layer. In TransformerBSWA, input query attends to the current block and two memory segments, providing past context whereas in TransformerFAM input query attends to the current block, memory segments, and past FAM. FAM query (copied from previous FAM) compresses the current block to update FAM. This feedback loop enables information compression and propagation over indefinite horizon, which is working memory.\nDuring evaluation TransformerFAM outperformed TransformerBSWA on all the long context tasks (LCT), and on various model sizes (1B, 8B, and 24B) regardless of the number of memory segments in BSWA. It shows a significant performance improvement on Scrolls Qasper and NarrativeQA, where it has to understand 5k to 500k tokens of context before answering a question. The LCT results demonstrate that TransformerFAM can effectively compress and retain important contextual information within extremely long contexts. Further, TransformerFAM requires no additional weights, enabling seamless integration with pre-trained models.\nPaper: https://arxiv.org/pdf/2404.09173.pdf"
  },
  {
    "objectID": "posts/LLM-R2/LLM-R2.html",
    "href": "posts/LLM-R2/LLM-R2.html",
    "title": "LLM-R2 : A Large Language Model Enhanced Rule-based Rewrite System for Boosting Query Efficiency",
    "section": "",
    "text": "Recently, DB query rewrite using LLMs has been one of the sort out use cases. The aim of query rewrite is to output a new query equivalent to the original SQL query, while having a shorter execution time. However, most of these methods utilize the sequence-to-sequence generation ability of a language model to directly output a new rewritten query given an input query, without considering any rewrite rules or DBMS information. Relying solely on LLM’s output query may lead to hallucination problems or a syntax or reference error during generation, especially for long and complicated queries.\nTo address this researchers have proposed a novel method of query rewrite named LLM-enhanced rule-based rewrite system (LLM-R2), adopting a large language model (LLM) to propose possible rewrite rules for a database rewrite system. To overcome hallucination, LLM-R2 collects a pool of demonstrations consisting of effective query rewrites using existing methods and designed baselines. Then a contrastive query is learned using a representation model to select the most useful in-context demonstration for the given query to prompt the system, optimizing the LLM’s rewrite rule selection. In addition, to address the challenge of limited training data, the learning curriculum technique utilizes to schedule the training data from easy to hard.\nDuring experimentation LLM-R2 method was applied on three different datasets, namely TPC-H, IMDB, and DSB. A significant decrease in query execution time was observed, taking only 52.5%, 56.0%, 39.8% of the querying time of the original query and 94.5%, 63.1%, 40.7% of the time of the state-of-the-art baseline method on average on the three datasets.\nPaper : https://lnkd.in/gd3SVd6H"
  },
  {
    "objectID": "posts/CodecLM/CodecLM.html",
    "href": "posts/CodecLM/CodecLM.html",
    "title": "CodecLM: Aligning Language Models with Tailored Synthetic Data",
    "section": "",
    "text": "Recent progress in instruction tuned LLM highlights the critical role of high-quality data in enhancing LLMs’ instruction-following capabilities. However, acquiring such data through human annotation remains cost-prohibitive and difficult to scale, hindering further progress. Using LLMs generated instruction aligned synthetic data have often shown to neglect downstream use cases.\nSo how can we tailor synthetic data to align LLMs for different instruction-following tasks?\nTo address this researchers have introduced CodecLM, a general framework for adaptively generating high quality synthetic data for LLM alignment with different downstream instruction distributions and LLMs. Drawing on the Encode-Decode principles CodecLM works as follows: First, the strong LLM (fs) encodes the seed instruction into instruction metadata, specifying its use case and skills required for responses. Next, fs decodes metadata into basic instructions. Meanwhile, Self-Rubrics leverages fs to generate rubrics and actions to improve the basic instruction, tailoring them for the downstream task. Finally, Contrastive Filtering uses a scoring function S to compare fs and ft’s responses. The most effective pairs are selected for aligning the LLM, while less effective instructions are sent for further improvement.\nCodecLM was evaluated on various benchmarks including EvolInstruct, Vicuna, Self-Instruct and Koala and against various methods Self-Instruct, Alpagasus, Tree-Instruct, WizardLM and WizardLM+. All methods were trained on LLaMA-7B or 13B as the target LLM and compared against Gemini-Pro, the strong LLM that generates the data. CodecLM outperforms against all methods consistently on all benchmarks, with both the target LLMs.\nPaper: https://arxiv.org/pdf/2404.05875"
  },
  {
    "objectID": "posts/IN2/IN2.html",
    "href": "posts/IN2/IN2.html",
    "title": "Make Your LLM Fully Utilize the Context",
    "section": "",
    "text": "These days the training context windows of many contemporary LLMs have been expanded to tens of thousands of tokens, thereby enabling these models to process extensive context as input. However, recent studies have revealed that these long-context LLMs struggle to effectively and robustly utilize all the information provided in the context, known as the lost-in-the-middle challenge. This mainly stems from insufficient explicit supervision during the long-context training, which fails to emphasize that any position in a long context can hold crucial information.\nSo how can we make long-context LLMs fully utilize the information in the long context?\nTo address this researchers have introduced INformation-INtensive (IN2) training, a purely data-driven solution to overcome lost-in-the-middle. The IN2 training aims to explicitly teach the model that any position in a long context can contain crucial information. To achieve this goal, a long-context question-answer training dataset D = {Li , qi , ai}, is constructed based on a general natural language corpus C where the answer ai to the question qi requires the information contained in some short segments (∼128 tokens) that are randomly placed in the whole long context Li (ranging from 4K to 32K tokens). Two types of question-answer pairs are generated which require (1) the awareness of fine-grained information in the long context, and (2) the integration and reasoning of information appearing at different positions in the long context.\nDuring experimentation IN2 training method was applied on Mistral-7B model with realnewslike subset from the C4 corpus as training corpus, and GPT-4-Turbo as the stronger LLM to generate QA pairs and a new model was introduce called FILM-7B (FILlin-the-Middle). Further, FILM-7B was evaluated on three probing tasks that encompass various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval). The probing results demonstrate that FILM-7B can robustly retrieve information from different positions in its 32K context window. Beyond these probing tasks, FILM-7B significantly improves the performance on real-world long-context tasks (e.g., 23.5→26.9 F1 score on NarrativeQA), while maintaining a comparable performance on short-context tasks (e.g., 59.3→59.2 accuracy on MMLU).\nPaper : https://arxiv.org/pdf/2404.16811"
  },
  {
    "objectID": "posts/PoLL/PoLL.html",
    "href": "posts/PoLL/PoLL.html",
    "title": "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models",
    "section": "",
    "text": "Evaluating language models is a challenging task: not only is it difficult to find meaningful data to test the models, but evaluating the correctness of a generated response is itself a challenge. You will find that LLM evaluation benchmark tests such as MMLU or automatic evaluation metrics such as BLEU score for machine translation and ROUGE for summarization commonly fail to analyze the intended property of interest.\nRecently, model-based scoring or LLM “as Judge” found to be performing better than above heuristic metrics. However, using LLMs like GPT4 for evaluation tend to have their own biases; often recognizing and preferring their own outputs over those of other models and also much more expensive.\nSo is there any better way to automatically evaluate LLMs?\nCohere has come up with a new approach called Panel of LLm evaluators (PoLL) that replaces a single LLM “as Judge” with multiple LLMs “Juries” and uses vote to get the best result. PoLL consists of a larger number of smaller models called Juries and a voting function to aggregate the score across these Juries and finally Cohen’s Kappa correlation is used to compare results with human preferences.\nDuring experimentation, a PoLL was constructed from three models drawn from three disparate model families (Command R, Haiku, and GP 3.5). Further, two different voting functions for aggregating scores across the judges were used. For QA datasets, max voting was used as all judgements are binary [correct, incorrect]. For Chatbot Arena average pooling methods were used because judgements are scores ranging from 1-5 and a three judge panel often does not produce a clear majority decision. Finally, Cohen’s Kappa correlations were used to compare results with human preferences. Cohen’s kappa Correlation measures inter-rater reliability, which quantifies the level of agreement between two or more raters or judges.\nPoLL outperforms single judges (GPT-4) across multiple datasets, exhibits less intra-model bias due to its composition of disjoint model families, and does so while being over seven times less expensive (7-8x cheaper than GPT-4).\nPaper : https://arxiv.org/pdf/2404.18796"
  },
  {
    "objectID": "posts/Octopusv4/Octopusv4.html",
    "href": "posts/Octopusv4/Octopusv4.html",
    "title": "Octopus v4: Graph of language models",
    "section": "",
    "text": "LLMs have been effective in a wide range of applications, yet the most sophisticated models are often proprietary (GPT 4, Gemini) and considerably costly than open source ones. However, recently niche-specific smaller language models, such as those tailored for legal, medical or financial tasks, have outperformed their proprietary counterparts considerably.\nIs there any way to integrate all these LLMs in order to manage multiple downstream tasks without compromising performance and staying within budget?\nThat is what researcher from nexa4ai has done with Octopus v4, a novel approach that employs functional tokens to integrate multiple open-source models, each optimized for particular tasks. Octopus v4 model leverages functional tokens to intelligently direct user queries to the most appropriate vertical model and reformat the query to achieve the best performance.\nThe architecture consists of two abstraction layers. The first layer utilizes functional tokens to represent the actions executable by the Octopus v2 model. This layer encompasses three distinct Octopus v2 models, each identified by different functional tokens, effectively differentiating them as separate AI agents. The second layer of abstraction pertains to the Octopus v4 model, where internal functional tokens are mapped to various v2 models.\nFramework features a graph of language models with a master node deployed on a central device and worker nodes distributed across various devices. Kubernetes (k8s) is employed for serverless deployment of each individual worker language model. For efficient data sharing, a distributed cache mechanism supported by Redis is used. Note that for each worker node, a small Octopus v4 Lora is attached to guide it to the next neighbor node for the case of multi-Agent use cases. During evaluation it was found that the model achieved a SOTA MMLU score of 74.8 among the same 10B level models.\nPaper : https://arxiv.org/pdf/2404.19296"
  },
  {
    "objectID": "posts/PROMETHEUS2/PROMETHEUS2.html",
    "href": "posts/PROMETHEUS2/PROMETHEUS2.html",
    "title": "PROMETHEUS 2: An Open Source Language Model Specialized in Evaluating Other Language Models",
    "section": "",
    "text": "Proprietary LMs such as GPT-4 model-based evaluation have emerged as a scalable solution for assessing LM-generated text. However, concerns related to transparency, controllability, and affordability these proprietary LMs have led to the development of open source LMs specialized in evaluations. But, these open source evaluator language models fall short in two key areas: they often diverge from human scores and lack flexibility to perform common evaluation methods like direct assessment and pairwise ranking. They also lack the ability to evaluate based on custom criteria, focusing on general attributes instead.\nTo address these issues, researchers have introduced Prometheus 2, a more powerful evaluator LM that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. To achieve this, researchers have merged the weights of two evaluator LMs trained separately on direct assessment and pairwise ranking formats and this weight merging have yield an evaluator LM that not only works in both formats, but also outperforms evaluator LMs that are jointly trained or only trained on a single format.\nFor Prometheus 2 researchers use Mistral-7B and Mixtral8x7B as base models, and merge the weights of evaluator LMs separately trained on the FEEDBACK COLLECTION, a direct assessment feedback dataset, and the PREFERENCE COLLECTION dataset, a new fine-grained pairwise ranking feedback dataset that builds on the FEEDBACK COLLECTION, to obtain resulting models, PROMETHEUS 2 (7B & 8x7B).\nOn four direct assessment benchmarks (Vicuna Bench, MT Bench, FLASK, Feedback Bench), the PROMETHEUS 2 models demonstrate the highest correlation with both human evaluators and proprietary LM-based judges compared to existing open evaluator LMs, with the Pearson correlation surpassing other baselines by 0.2 units across all datasets. Similarly, on four pairwise ranking benchmarks (HHH Alignment, MT Bench Human Judgment, Auto-J Eval, Preference Bench), the PROMETHEUS 2 models show the highest agreement with human evaluators among all the open evaluator LMs we tested, reducing the performance gap with GPT-4 in half.\nPaper : https://arxiv.org/pdf/2405.01535"
  },
  {
    "objectID": "posts/NeMo-Aligner/NeMo-Aligner.html",
    "href": "posts/NeMo-Aligner/NeMo-Aligner.html",
    "title": "NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment",
    "section": "",
    "text": "Aligning Large Language Models (LLMs) with human values and preferences is essential for making them helpful and safe. However, building efficient tools to perform alignment can be challenging, especially for the largest and most competent LLMs which often contain tens or hundreds of billions of parameters.\nIn order to simplify LLM alignment issues, Nvidia has launched NeMo-Aligner, a toolkit for model alignment that can efficiently scale to using hundreds of GPUs for training. NeMo-Aligner comes with highly optimized and scalable implementations for major paradigms of model alignment such as: Reinforcement Learning from Human Feedback (RLHF), Direct Preference Optimization (DPO), SteerLM, and Self-Play Fine-Tuning (SPIN). Additionally, our toolkit supports running most of the alignment techniques in a Parameter Efficient Fine-Tuning (PEFT) setting. NeMo-Aligner is designed for extensibility, allowing support for other alignment techniques with minimal effort.\nNeMo-Aligner addresses scalability challenges by (1) building upon Megatron-LM with 3D (data, tensor, and pipeline)-parallelism training, (2) having a distributed approach to Proximal Policy Optimization (PPO) training in RLHF and (3) integrating PPO inference optimizations based on TensorRT-LLM during rollout stage. Combined, these optimizations allow users to efficiently train the largest models over hundreds of GPUs reducing research iteration time.\nPaper : https://lnkd.in/gMh2vqqc\nGithub : https://lnkd.in/gvqsWZQq"
  },
  {
    "objectID": "posts/Multi-token Prediction/Multi-token Prediction.html",
    "href": "posts/Multi-token Prediction/Multi-token Prediction.html",
    "title": "Better & Faster Large Language Models via Multi-token Prediction",
    "section": "",
    "text": "All Large language models such as GPT and Llama are trained with a next-token prediction loss. However, despite the recent wave of impressive achievements in LLMs, next-token prediction remains an inefficient way of acquiring language, world knowledge and reasoning capabilities. More precisely, LLM training are more focus on next-token prediction based on local patterns and overlooks “hard” decisions.\nSo is there any way we can improve next-token prediction capabilities of LLMs without any additional training ?\nIt looks like AI @ Meta has a solution for it by moving away from traditional next-token prediction to a multi-token prediction strategy, demonstrating that LLMs can achieve better outcomes without additional training time. In order to do so Meta has introduced multi-token prediction architecture with no train time or memory overhead. During training, the model predicts 4 future tokens at once, by means of a shared trunk and 4 dedicated output heads. During inference, it employs only the next-token output head. Optionally, the other three heads may be used to speed-up inference time.\nOne big challenge in training multi-token predictors is reducing their GPU memory utilization. To overcome this Meta multi-tone prediction architecture carefully adapt the sequence of forward and backward operations. By performing the forward/backward on an n-token prediction model with n = 2 heads in sequential order, it avoids materializing all unembedding layer gradients in memory simultaneously and reduces peak GPU memory usage. During inference multiple independent output heads of the architecture, each predicting a future token, allowing parallel token prediction there by speed up decoding from the next-token prediction head with self-speculative decoding methods such as blockwise parallel decoding.\nDuring evaluation, 13B models trained on multi-token prediction architecture solved 12 % more problems on HumanEval and 17 % more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to 3× faster at inference, even with large batch sizes.\nPaper : https://arxiv.org/pdf/2404.19737"
  },
  {
    "objectID": "posts/LayerSkip/LayerSkip.html",
    "href": "posts/LayerSkip/LayerSkip.html",
    "title": "LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding",
    "section": "",
    "text": "Optimizing LLMs operational cost and computation requirement is one of the sortout topics for researchers. Accelerated solutions deploy on mobile, edge devices or commodity GPUs such as laptops do exist but they suffer from significant drop in accuracy since a large portion of these LLM acceleration approaches focus on reducing the number of non-zero weights, number of bits per weight or number of heads per layer.\nSo is there any way we can deploy accelerated LLMs solutions economically without sacrificing accuracy ?\nTo address this AI @ Meta has introduced the LayerSkip method, an end-to-end solution to speed-up inference of large language models (LLMs), which reduces the number of layers required for each token by exiting early during inference. Unlike quantization or sparsity, acceleration by reducing the number of layers does not require specialized hardware or software kernels. LayerSkip solution work as follow\nFirst, during training layer dropout is applied, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit.\nSecond, during inference, the training recipe increases the accuracy of early exit at earlier layers, without adding any auxiliary layers or modules to the model.\nThird, a self-speculative decoding novel solution is utilize where it exists at early layers and verifies and corrects with remaining layers of the model.This self-speculative decoding approach has less memory footprint than other speculative decoding approaches and benefits from shared compute and activations of the draft and verification stages.\nDuring evaluation experiments were run on different Llama model sizes on different types of training: pretraining from scratch, continual pretraining, finetuning on specific data domain, and finetuning on specific task. LayerSkip inference solution have show speedups of up to 2.16× on summarization for CNN/DM documents, 1.82× on coding, and 2.0× on TOPv2 semantic parsing tasks.\nPaper : https://lnkd.in/dWNjW52i"
  },
  {
    "objectID": "posts/Flash Attention Stable/Flash Attention Stable.html",
    "href": "posts/Flash Attention Stable/Flash Attention Stable.html",
    "title": "Is Flash Attention Stable?",
    "section": "",
    "text": "Given the size and complexity of workloads, training Large Language Models (LLMs) often takes months together, across hundreds or thousands of GPUs. For example, LLaMA2’s 70-B parameter model, took 1,720,320 GPU hours to train. With such long training jobs, training instability has become increasingly problematic. As reported in works such as Google’s PaLM model, training instability often manifests itself in the form of loss spikes occurring up to 20 times throughout training. These loss spikes are costly, as they often cause interrupts in the training process, requiring training to stop and restart\nOne under-explored potential cause of training instability is numeric deviation. Numeric deviation between an optimization and its corresponding baseline can lead to the gradual accumulation of errors, which over the course of training have the potential to culminate in loss spikes that require a resetting of the model state.\nTo understand this numeric deviation in training optimizations researchers from Meta developed a principled quantitative approach consists of two phases (1) a numerical microbenchmark of the Flash Attention operation was developed, which allows for the experimentation of different numerical precisions, as well as the testing of various optimizations throughout the algorithm. this framework allows for the direct comparison of the Attention Matrix output between Baseline Attention, Flash Attention, and numeric re-implementation. (2) a data-driven analysis based on Wasserstein distance were used to contextualize this numeric difference via examining model weight changes over the course of training.\nAfter applying the above framework on widely-adopted Flash Attention optimization it was found that flash Attention sees roughly an order of magnitude more numeric deviation as compared to Baseline Attention at BF16 when measured during an isolated forward pass. Further, data-driven analysis based on the Wasserstein Distance have provided upper bounds on how this numeric deviation impacts model weights during training, finding that the numerical deviation present in Flash Attention is 2-5 times less significant than low-precision training.\nIn conclusion, investigations underscore the importance of developing a principled approach to not only quantify, but contextualize, the impact of training optimizations on numeric deviation. By constructing proxies to put this numeric deviation in context, this paper aims to reason about the likelihood of downstream model effects (i.e training instability) that are traditionally difficult to measure.\nPaper: https://lnkd.in/gpSdZu99"
  },
  {
    "objectID": "posts/GraphRAG /GraphRAG.html",
    "href": "posts/GraphRAG /GraphRAG.html",
    "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization",
    "section": "",
    "text": "The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. RAG works great for explicit retrieval tasks but fails during query focused summarization (QFS) tasks such as global questions directed at an entire text corpus.\nTo address this, Microsoft Research has come up with a new approach, GraphRAG, which uses the LLM to create a knowledge graph based on the private dataset. GraphRAG approach uses an LLM to build a graph-based text index in two stages: (1) a knowledge graph from source documents is created, (2) then generating summaries for groups of closely-related entities. When a question is posed, these summaries are used to create partial responses, which are then combined into a final answer for the user.\nA typical Graph RAG pipeline uses an LLM-derived graph index of source document text. This index spans nodes (e.g., entities), edges (e.g., relationships), and covariates (e.g., claims) that have been detected, extracted, and summarized by LLM prompts tailored to the domain of the dataset. Community detection is used to partition the graph index into groups of elements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time. The “global answer” to a given query is produced using a final round of query-focused summarization over all community summaries reporting relevance to that query.\nUnlike previous methods that utilize the structured retrieval and traversal capabilities of graph indexes, Graph RAG prioritizes an untapped aspect of graphs: their built-in modularity. It emphasizes the capacity of community detection algorithms to divide graphs into cohesive clusters of closely-connected nodes. LLM-generated summaries of these community descriptions provide complete coverage of the underlying graph index and the input documents it represents. Query-focused summarization of an entire corpus is then made possible using a map-reduce approach: first using each community summary to answer the query independently and in parallel, then summarizing all relevant partial answers into a final global answer.\nPaper : https://arxiv.org/pdf/2404.16130v1"
  },
  {
    "objectID": "posts/SUPRA/SUPRA.html",
    "href": "posts/SUPRA/SUPRA.html",
    "title": "Linearizing Large Language Models",
    "section": "",
    "text": "Over the last few years, Transformers have displaced Recurrent Neural Networks (RNNs) in sequence modeling tasks, owing to their highly parallel training efficiency and unmatched scaling performance. However, this training efficiency comes at the cost of inference cost that scales linearly with the number of tokens, compared to the fixed-cost inference of RNNs.\nTo overcome this limitation researchers have proper various novel methods such as Linear Transformer which combine good of both worlds i.e, train models with sequence parallelism (i.e. as transformers), but operate as RNNs at inference time. State-space models (SSMs) such as Mumba show impressive performance at smaller scales, matching or exceeding the performance of softmax transformers. However, a gap remains for long-context NLU tasks, showing a persistent advantage of softmax attention.\nRather than pre-training linear models, can we convert an existing transformer into an RNN ?\nWell that is what researchers have done with Scalable UPtraining for Recurrent Attention (SUPRA), a linearization strategy to up train state-of-the-art LLMs into performant RNNs, enabling the study of the strengths and limitations of recurrent models at scale with minimal compute cost. SUPRA replaced the softmax normalization with GroupNorm (GN) and introduced a small MLP to project the queries and keys, converting a pre-trained attention block (left) to a linear attention (right). The model can be trained in parallel as a transformer and used recurrently at inference time with a mathematically equivalent reformulation. This allows it to leverage the strong pre-training data and performance of existing transformer LLMs, while requiring 5% of the training cost.\nCompared to pre-training linear models from scratch, the SUPRA strategy produces competitive models comparable to the best available recurrent LLMs (RWKV and Mamba) at the 7B scale. Not only the strengths of linear models on standard NLU benchmarks was identify but also the enduring limitations on in-context (i.e. MMLU) and long-context (NarrativeQA, Qasper) tasks, showing that linearized models do not inherit these capabilities from the base softmax transformers.\nPaper : https://arxiv.org/pdf/2405.06640\nCode : https://github.com/TRI-ML/linear_open_lm"
  },
  {
    "objectID": "posts/SUTRA/SUTRA.html",
    "href": "posts/SUTRA/SUTRA.html",
    "title": "SUTRA: Scalable Multilingual language model architecture",
    "section": "",
    "text": "Recent advancements in Large Language Models (LLMs) have predominantly focused on a limited set of data-rich languages, with training datasets being notably skewed towards English. Most of the existing multilingual LLMs models often suffer from significant trade-offs between performance, efficiency, and scalability, particularly when extending support across a broader spectrum of languages. Models such as BLOOM and Llama2, typically underperform in languages that are less represented in the training data due to the difficulty of balancing language-specific nuances whereas language-specific LLMs like HyperClova in Korean or OpenHaathi in Hindi are bit challenging due to the exponential data and training requirements.\nTo overcome this challenge researchers have introduced SUTRA (Sanskrit for “thread”), a transformative approach in the architecture of multilingual LLMs. SUTRA is a novel multilingual large language model architecture that is trained by decoupling concept learning from language learning. The input is processed through a multilingual concept encoder, followed by the concept model and finally through a multilingual concept decoder to generate the output response.This architecture enables the core model to focus on universal language agnostic concepts while leveraging specialized neural machine translation (NMT) mechanisms for language-specific processing, thus preserving linguistic nuances without compromising the model’s scalability or performance.\nFurther, SUTRA employs a Mixture of Experts (MoE) strategy, enhancing the model’s efficiency by engaging only the relevant experts based on the linguistic task at hand. MoE Layer is configured in such a way that the Input vectors are routed to a subset of the available experts, specifically 2 out of 8, by a specialized router. The aggregate output of this layer is the sum of the individual outputs, each weighted accordingly. Each expert comprises a feedforward module similar to those found in conventional transformer models.\nIn conclusion, a combination of multilingual skills, online connectivity, and efficiency in language generation incorporated by SUTRA models promises to redefine the landscape of multilingual language modeling.\nPaper : https://arxiv.org/pdf/2405.06694"
  },
  {
    "objectID": "posts/Layer-Condensed KV Cache/Layer-Condensed KV Cache.html",
    "href": "posts/Layer-Condensed KV Cache/Layer-Condensed KV Cache.html",
    "title": "Layer-Condensed KV Cache for Efficient Inference of Large Language Models",
    "section": "",
    "text": "Key-value (KV) cache is one of the most significant parts of any transformer based LLM model and takes over 30% of the GPU memory during deployment. Hence KV cache plays a critical role in deciding overall LLMs throughput and latency. Recently, various work has been focused on improving KV cache performance either by prompt compression or caching strategies such as cache evacuation and sequence caching.\nNow researchers have proposed a novel perspective that is orthogonal to previous efforts: Layer-Condensed KV Cache, a novel method that only computes and caches the KVs of a small number of layers, thus significantly saving memory consumption and improving inference throughput.\nLayer-Condensed KV Cache, a new variant of transformer decoders in which queries of all layers are paired with keys and values of just the top layer so that model do not have to cache or even compute KVs for layers other than the top layer, saving both memory consumption and computation. Furthermore, since models no longer need to compute KVs for these layers, nor do they need to keep the weights WK, WV that map hidden representations to KVs for these layers, thus also saving model parameters.\nDuring experiments on Llama show that this model achieves up to 32× larger batch sizes and up to 26× higher throughput than standard transformers for LLMs of 1B–30B parameters; at the same time, the model has competitive performance to standard transformers in language modeling and downstream tasks. In addition, this method is orthogonal to existing transformer memory-saving techniques, so it is straightforward to integrate them with our model, achieving further improvement in inference efficiency.\nPaper : https://lnkd.in/d3XcDA4Z"
  },
  {
    "objectID": "posts/Xmodel-VLM/Xmodel-VLM.html",
    "href": "posts/Xmodel-VLM/Xmodel-VLM.html",
    "title": "Xmodel-VLM: A Simple Baseline for Multimodal Vision Language Model",
    "section": "",
    "text": "Recently small-scale visual language models performance have come in par with its larger-scale counterparts. Models such as LLaVAPhi [47], which combines the open source multi-modal model LLaVA-1.5 and the open source small language model Phi-2(2.7B) have shown great promise to improve multi-modal model resource efficiency. However, despite the encouraging advancements made in the realm of visual language models, the pursuit of a genuinely optimal harmony between performance and efficiency remains an active and ongoing challenge.\nTo address this researchers have introduced Xmodel-VLM, a cutting-edge multimodal vision language model designed for efficient deployment on consumer GPU servers. The architecture of Xmodel-VLM, closely mirrors that of LLaVA-1.5. It consists of three key components: (1) a vision encoder : a pre-trained CLIP ViT-L/14 with a resolution of 336×336 (2) a lightweight language model (LLM) : an lightweight language model Xmodel-LM 1.1B from scratch based on LLaVA architecture. (3) a projector responsible for aligning the visual and textual spaces. It utilizes a two-layer MLP to strengthen the link between the vision encoder and LLM, employing the Mish activation function. This innovative projector, known as XDP, acts as both a connection enhancer and a downsampling mechanism, reducing visual tokens by 75%. XDP’s design emphasizes simplicity and effectiveness.\nThe training of Xmodel_VLM occurs in two main stages: Stage I - Pre-training: In this stage, the vision encoder is kept frozen The XDP projector and the LLM (Language Model) are both learnable. Stage II - Multi-task Training: Similar to Stage I, the vision encoder remains frozen, while both the XDP projector and the LLM are learnable. This stage involves training the model on multiple tasks simultaneously.\nDuring evaluation across a variety of datasets: VizWiz, SQAI, VQA, POPE, GQA, MMB, MMBCN, MMVet and MME Xmodel-VLM 1.1B demonstrates competitive performance against Qwen-7B, LLaMA-7B and Vicuna-13B despite having small model parameter.\nPaper : https://lnkd.in/gBK9W-2H"
  },
  {
    "objectID": "posts/Zamba/zamba.html",
    "href": "posts/Zamba/zamba.html",
    "title": "Zamba: A Compact 7B SSM Hybrid Model",
    "section": "",
    "text": "Recently, State-of-the-art Transformer-SSM hybrid Architecture has been a driving force in Open source LLMs. Inline with such trends researchers from Zyphra have launched Zamba, a novel 7B SSM-transformer hybrid model which achieves competitive performance against leading open-weight models at a comparable scale. Zamba is trained on 1T tokens from openly available datasets and is the best non-transformer model at this scale.\nThe Zamba architecture consists of a backbone of standard Mamba blocks connected to a shared attention and MLP block. This block is repeated every 6 Mamba blocks but has shared parameters, which enables Mamba to utilize more FLOPs for increased performance at the same memory cost. The input embeddings are always concatenated with the residual stream going into the shared attention block as this provides an additional path for the model to remember the inputs. After the block, a learnt linear projection maps the output back to the residual stream.\nDue to its architecture, Zamba is significantly faster at inference than comparable transformer models and requires substantially less memory for generation of long sequences. Zamba is pretrained in two phases: the first phase is based on existing web datasets, while the second one consists of annealing the model over high quality instruct and synthetic datasets, and is characterized by a rapid learning rate decay\nZamba is also the highest-performing SSM in the small 7B model range and the highest-performing dense SSM model available. Zamba matches state-of-the-art 7B models on many linguistic evals, while lagging slightly behind on tests of reasoning and in context learning, which may be due to the significant data disparity between Zamba and other leading ∼7B models.\nPaper : https://arxiv.org/pdf/2405.16712"
  },
  {
    "objectID": "posts/VeLoRA/VeLoRA.html",
    "href": "posts/VeLoRA/VeLoRA.html",
    "title": "VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections",
    "section": "",
    "text": "LLM Training and finetuning are still far too computationally and memory intensive tasks. Several techniques have been proposed to reduce these memory requirements, such as GaLore, gradient checkpointing, reversible backpropagation, parameter-efficient finetuning, quantization and activation offloading. While these methods are promising and lower the memory cost, they also might introduce a substantial computational overhead, are limited in their memory savings, or require specialized hardware.\nTo address these challenges researchers have introduced a novel approach for efficient training and finetuning, called Vector projected LoRA (VeLoRA). VeLoRA simply divides the tokens up into smaller sub-tokens before projecting them onto a fixed 1-dimensional subspace during the forward pass. These features are then coarsely reconstructed during the backward pass to implement the update rules. By compressing and then reconstructing the activations on the fly, VeLoRA reduces the peak activation memory footprint to a tiny fraction of what is required to store the original activations. This enables fitting much larger models into limited GPU memory compared to approaches like GaLore or gradient checkpointing.\nVeLoRA memory-efficient algorithm consists of two components: (i) The grouping strategy to divide the original high-dimensional tokens into much smaller sub-tokens; and (ii) Fixed rank-1 projections of these sub-tokens using cheap heuristically initialized principal components. Given a large pre-trained model, above steps are applied to compress the intermediate activations saved during training while preserving most of the original model’s training dynamics.\nVeLoRA was evaluated on both moderately-sized vision transformers as well as in large language models. VeLoRA was found to significantly reduce memory requirements while improving the performance effectiveness on VTAB-1K, MMLU, GLUE, and C4 benchmarks outperforming state-of-the-art methods such as LoRA, QLoRA or GaLore.\nPaper : https://arxiv.org/pdf/2405.17991"
  },
  {
    "objectID": "posts/Nest/Nest.html",
    "href": "posts/Nest/Nest.html",
    "title": "Nearest Neighbor Speculative Decoding for LLM Generation and Attribution",
    "section": "",
    "text": "Large language models (LLMs) often hallucinate and lack the ability to provide attribution for their generations. Semi-parametric LMs, such as kNN-LM, approach these limitations by refining the output of an LM for a given prompt using its nearest neighbor matches in a non-parametric data store. However, these models often exhibit slow inference speeds and produce non-fluent texts.\nTo address these challenges researchers from Meta have introduced Nearest Neighbor Speculative Decoding (Nest), a novel semi-parametric language modeling approach that is capable of incorporating real-world text spans of arbitrary length into the LM generations and providing attribution to their sources.\nThe Nest approach first locates the tokens in the corpus using the LM hidden states. The retrieval distribution pk-NN is dynamically interpolated with pLM based on the retriever’s uncertainty λt. The token and its n-gram continuation are then selected from the mixture distribution pM, while the final span length is determined by speculative decoding to remove undesired tokens. The spans incorporated in the final generation provide direct attribution and amortize the generation latency.\nAt each inference step, Nest performs content generation with three sub-steps: * Confidence-based interpolation: Adjusts output probabilities using a Relative Retrieval Confidence score, allowing dynamic adaptation to different tasks. * Dynamic span selection: Extends token selection to include a span of text when confidence in retrieval exceeds a threshold. * Relaxed speculative decoding: Evaluates selected spans based on mixture probability, accepting only highly probable prefixes.\nNest significantly enhances the generation quality and attribution rate of the base LM across a variety of knowledge-intensive tasks, surpassing the conventional kNN-LM method and performing competitively with in-context retrieval augmentation. In addition, Nest substantially improves the generation speed, achieving a 1.8× speedup in inference time when applied to Llama-2-Chat 70B.\nPaper : https://arxiv.org/pdf/2405.17976"
  },
  {
    "objectID": "posts/METRAG/METRAG.html",
    "href": "posts/METRAG/METRAG.html",
    "title": "Similarity is Not All You Need: Endowing Retrieval-Augmented Generation with Multi–layered Thoughts",
    "section": "",
    "text": "Retrieval-augmented generation (RAG) has been pencil in pushing LLM use cases in the Knowledge management system. Nevertheless, existing retrieval-augmented generation approaches are typically similarity-based i.e., they retrieve documents from external corpus based on similarity. Simply aggregating the Top-k document without considering the relationships between them makes it difficult to capture the commonalities and characteristics among them and even confuse LLMs due to excessive text length thus incurring information loss and probably performance degradation\nHence similarity is not always the “panacea” and totally relying on similarity would sometimes degrade the performance of retrieval-augmented generation.\nTo address this researchers have come up with a novel approach called METRAG, a Multi–layEred Thoughts enhanced RetrievalAugmented Generation framework. METRAG endows retrieval-augmented generation with multi-layered thoughts by firstly embracing LLM’s supervision for utility-oriented thoughts and combining similarity and utility of documents for performance boosting and further pursuing compactness oriented thoughts via a task-adaptive summarizer, finally incorporating the derived multi-layered thoughts for answer generation.\nIn general, METRAG combines similarity- and utility-oriented approaches. It involves using a similarity model and a utility model to generate summaries, then distilling summary skills from a powerful teacher model (like GPT4). Finally, multiple generated summaries are evaluated using a reward model to improve alignment with the desired task. Extensive experiments on knowledge intensive tasks have demonstrated the superiority of the proposed METRAG.\nPaper : https://arxiv.org/pdf/2405.19893"
  },
  {
    "objectID": "posts/CoPE/CoPE.html",
    "href": "posts/CoPE/CoPE.html",
    "title": "Contextual Position Encoding: Learning to Count What’s Important",
    "section": "",
    "text": "The attention mechanism is a critical component of Large Language Models (LLMs) that allows tokens in a sequence to interact with each other, but the attention mechanism inherently lacks ordered information and treats sequences as sets. Thus, it is necessary to have an additional mechanism for encoding position information. Position encoding (PE) achieves this by assigning an embedding vector to each position and adding that to the corresponding token representations.\nIn order to tie position measurement to more semantically meaningful units such as words, or sentences, one needs to take context into account. However, this is impossible with current PE methods as position addressing is computed independently of the context, and thus cannot generalize to higher levels of abstraction, such as attending to the i-th sentence.\nTo address these challenges researchers have proposed a new position encoding method, Contextual Position Encoding (CoPE), that allows positions to be conditioned on context by incrementing position only on certain tokens determined by the model. This allows more general position addressing such as attending to the i-th particular word, noun, or sentence.\nCoPE first determines which tokens to count using their context vectors. Specifically, given the current token as a query vector, CoPE computes a gate value for each previous token using their key vectors. Then it aggregates those gate values to determine the relative position of each token with respect to the current token. Unlike token positions, this contextual position can take fractional values, thus cannot have an assigned embedding. Instead, it interpolate embeddings that are assigned to integer values to compute position embeddings. Like the other PE methods, these position embeddings are then added to the key vectors, so a query vector can use them in the attention operation. Since contextual position can vary from query-to-query and layer-to-layer, the model can simultaneously measure distances in multiple units.\nDuring evaluation CoPE were applied to several toy tasks: counting, selective copying and the Flip-Flop task, where it outperforms token-based PE methods, especially in the case of out-of-domain generalization. To test real-world applicability, a language modeling task on Wikipedia text where applied with CoPE leading to better performance. The same performance gain is also observed when trained on code.\nPaper : https://arxiv.org/pdf/2405.18719"
  },
  {
    "objectID": "posts/MMLU-Pro/MMLU-Pro.html",
    "href": "posts/MMLU-Pro/MMLU-Pro.html",
    "title": "MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark",
    "section": "",
    "text": "In the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse domains. MMLU includes a broad range of exam questions from 57 subjects across STEM, the humanities, the social sciences, etc. However, the rapid progress of current LLMs has quickly led to performance saturation on MMLU. Since GPT-4 achieved 86.4% in March 2023, there has not been any significant progress on the benchmark. Even GPT-4o achieved 1% improvement on MMLU to obtain 87.4%.\nThis is due to the structure of the Multiple-Choice Machine Reading Comprehension (MMLU) dataset which provides only three options, potentially allowing LLMs to guess answers without understanding. Further, MMLU questions are often more about knowledge recall than reasoning, especially in STEM subjects, making them easier. Finally, some questions are unanswerable or wrongly annotated, limiting the dataset’s usefulness and affecting model performance.\nTo address these challenges researchers have introduced MMLU-Pro: a comprehensive benchmark designed for proficient-level multi-discipline language understanding and reasoning. MMLU-Pro spans 14 diverse domains including mathematics, physics, chemistry, law, engineering, psychology, and health, encompassing over 12,000 questions and thus meeting the breadth requirement. MMLUPro is distinctive from MMLU in the following aspects: * 1. MMLU-Pro has ten options, which contain 3x more distractors than MMLU. By increasing the distractor numbers, it significantly reduces the probability of correct guess by chance to boost the benchmark’s difficulty and robustness. * 2. MMLU-Pro increases the portion of challenging college-level exam problems. These questions require LLM to perform deliberate reasoning in different domains to derive the final answer. * 3. MLU-Pro integrates two rounds of expert reviews to reduce the noise of the dataset. The first round is based on expert verification. In the second round, it utilizes the SoTA LLMs to identify potential errors and employ annotators to perform more targeted verification.\nExperimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16% to 33% compared to MMLU but also demonstrates greater stability under varying prompts. With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5% in MMLU to just 2% in MMLU-Pro. In conclusion, MMLU-Pro is a more discriminative benchmark to better track progress of both present and upcoming LLMs.\nPaper : https://arxiv.org/pdf/2406.01574"
  },
  {
    "objectID": "posts/DITTO/DITTO.html",
    "href": "posts/DITTO/DITTO.html",
    "title": "Show, Don’t Tell: Aligning Language Models with Demonstrated Feedback",
    "section": "",
    "text": "Aligning Large Language Models (LLMs) with human values and preferences is essential for making them helpful and safe. However, alignment can be challenging, especially for the largest and most competent LLMs which often require a large corpus of (un)acceptable behavior (on the order of ≈ 1K samples.\nIs it possible to align an LLM to a specific setting by leveraging a very small number (&lt; 10) of demonstrations as feedback ?\nWell that is what researchers from Stanford have addressed with Demonstration ITerated Task Optimization (DITTO), a framework for aligning LLMs to specific settings by providing a small number of demonstrations be drawn from a user’s existing interaction logs, or from direct edits made to LLM outputs. DITTO, scaffolds a handful of these demonstrations (&lt; 10) into a substantial dataset of preference comparisons, by treating users’ demonstrations as preferred over model output from both the original LLM and models’ earlier training iterations. This augmented dataset of demonstration-grounded comparisons can then be used to update the language model using an alignment algorithm like DPO.\nIn general, DITTO iteratively aligns LLMs to demonstrated behavior. When a user supplies demonstrations (through edits to a model’s output, past preferred interaction history, or writing examples from scratch), DITTO treats these demonstrations as preferred to all model behavior, including earlier iterations of the trained model. Using demonstrations as feedback allows for cheap generation of online comparison data and enables few-shot alignment with just a handful of samples.\nDITTO’s ability was evaluated to learn fine-grained style and task alignment across domains such as news articles, emails, and blog posts. Additionally, a user study was conducted soliciting a range of demonstrations from participants (N = 16). Across these benchmarks and user study, it was found that win-rates for DITTO outperform few-shot prompting, supervised fine-tuning, and other self-play methods by an average of 19% points. By using demonstrations as feedback directly, DITTO offers a novel method for effective customization of LLMs.\nPaper : https://arxiv.org/pdf/2406.00888"
  }
]