[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Santosh Sawant",
    "section": "",
    "text": "I am a Senior Machine Learning Architect at Tredence, I lead a cross-functional team to deliver a cutting-edge Generative AI platform and products in the Data Analytics, Healthcare and ESG domain."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Fine-tune Mistral-7b with Direct Preference Optimization Boosting the performance of your supervised fine-tuned models",
    "section": "",
    "text": "Fine-tune Mistral-7b with Direct Preference Optimization Boosting the performance of your supervised fine-tuned models\n\nPre-trained Large Language Models (LLMs) can only perform next-token prediction, making them unable to answer questions. This is why these base models are then fine-tuned on pairs of instructions and answers to act as helpful assistants. However, this process can still be flawed: fine-tuned LLMs can be biased, toxic, harmful, etc. This is where Reinforcement Learning from Human Feedback (RLHF) comes into play."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "Blog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 13, 2024\n\n\nTristan Oâ€™Malley\n\n\n\n\n\n\nNo matching items"
  }
]