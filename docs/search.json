[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Santosh Sawant",
    "section": "",
    "text": "I am a Senior Machine Learning Architect at Tredence, I lead a cross-functional team to deliver a cutting-edge Generative AI platform and products in the Data Analytics, Healthcare and ESG domain."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Fine-tune Mistral-7b with Direct Preference Optimization Boosting the performance of your supervised fine-tuned models",
    "section": "",
    "text": "Fine-tune Mistral-7b with Direct Preference Optimization Boosting the performance of your supervised fine-tuned models\n\nPre-trained Large Language Models (LLMs) can only perform next-token prediction, making them unable to answer questions. This is why these base models are then fine-tuned on pairs of instructions and answers to act as helpful assistants. However, this process can still be flawed: fine-tuned LLMs can be biased, toxic, harmful, etc. This is where Reinforcement Learning from Human Feedback (RLHF) comes into play."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "Blog",
    "section": "",
    "text": "Merge Model using Mergekit\n\n\n\n\n\n\ntools\n\n\nllm\n\n\nmodel building\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nSantosh Sawant\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/dpo-mistral-7b/index.html",
    "href": "posts/dpo-mistral-7b/index.html",
    "title": "Merge Model using Mergekit",
    "section": "",
    "text": "In this section, we will focus on four methods currently implemented in mergekit. Note that there are other methods, such as linear and Task Arithmetic. If you’re interested in papers on model merging, I recommend this excellent collection on Hugging Face.\n\n\nSpherical Linear Interpolation (SLERP) is a method used to smoothly interpolate between two vectors. It maintains a constant rate of change and preserves the geometric properties of the spherical space in which the vectors reside.\nThere are several reasons to prefer SLERP over a traditional linear interpolation. For example, in high-dimensional spaces, linear interpolation can lead to a decrease in the magnitude of the interpolated vector (i.e., it reduces the scale of weights). Moreover, the change in direction of the weights often represents more meaningful information (like feature learning and representation) than the magnitude of change.\nSLERP is implemented using the following steps:\n\nNormalize the input vectors to unit length, ensuring they represent directions rather than magnitudes\nCalculate the angle between these vectors using their dot product.\nIf the vectors are nearly collinear, it defaults to linear interpolation for efficiency. Otherwise, SLERP computing scale factors based on the interpolation factor t (t=0 = 100% of the first vector, t=1 = 100% of model 2) and the angle between the vectors.\nThese factors are used to weigh the original vectors, which are then summed to obtain the interpolated vector.\n\nSLERP is currently the most popular merging method, but it is limited to combining only two models at a time.\nExample of configuration:\nslices:\n  - sources:\n      - model: OpenPipe/mistral-ft-optimized-1218\n        layer_range: [0, 32]\n      - model: santoshsawant/NeuralHermes-2.5-Mistral-7B\n        layer_range: [0, 32]\nmerge_method: slerp\nbase_model: OpenPipe/mistral-ft-optimized-1218\nparameters:\n  t:\n    - filter: self_attn\n      value: [0, 0.5, 0.3, 0.7, 1]\n    - filter: mlp\n      value: [1, 0.5, 0.7, 0.3, 0]\n    - value: 0.5\ndtype: bfloat16\nThis is a classic SLERP configuration, applied to every layer of both models. Note that we input a gradient of values for the interpolation factor t. The parameters for the self-attention and MLP layers will use different combinations of OpenPipe/mistral-ft-optimized-1218 and mlabonne/NeuralHermes-2.5-Mistral-7B. The other layers are a 50/50 mixture of the two models.\nYou can find the final model on the Hugging Face Hub at mlabonne/NeuralPipe-7B-slerp.\n\n\n\nIntroduced in this paper by Yadav et al., TIES-Merging is designed to efficiently merge multiple task-specific models into a single multitask model. It addresses two main challenges in model merging:\n\nRedundancy in model parameters: It identifies and eliminates redundant parameters within task-specific models. This is achieved by focusing on the changes made during fine-tuning, identifying the top-k% most significant changes, and discarding the rest.\nDisagreement between parameter signs: Conflicts arise when different models suggest opposing adjustments to the same parameter. TIES-Merging resolves these conflicts by creating a unified sign vector that represents the most dominant direction of change across all models.\n\nTIES-Merging is divided into the following three steps:\n\nTrim: Reduces redundancy in task-specific models by retaining only a fraction the most significant parameters (density parameter) and resetting the rest to zero.\nElect Sign: Resolves sign conflicts across different models by creating a unified sign vector based on the most dominant direction (positive or negative) in terms of cumulative magnitude.\nDisjoint Merge: Averages parameter values that align with the unified sign vector, excluding zero values.\n\nUnlike SLERP, TIES can merge multiple models at a time.\nExample of configuration:\nmodels:\n  - model: mistralai/Mistral-7B-v0.1\n    # no parameters necessary for base model\n  - model: OpenPipe/mistral-ft-optimized-1218\n    parameters:\n      density: 0.5\n      weight: 0.5\n  - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n    parameters:\n      density: 0.5\n      weight: 0.3\nmerge_method: ties\nbase_model: mistralai/Mistral-7B-v0.1\nparameters:\n  normalize: true\ndtype: float16\nWith this config, we use Mistral-7B as a base model to calculate the delta weights. We merge the same two models: mistral-ft-optimized-1218 (50%) and NeuralHermes-2.5-Mistral-7B (30%) with normalization. Here, the density means that we’re only retaining 50% of the parameters of each model (the other half comes from the base model).\nNote that the sum of the weights is not equal to 1 in the config, but the normalize: true parameter will automatically normalize them internally. This config is inspired by the parameters provided by the author of OpenHermes-2.5-neural-chat-7b-v3-1-7B.\nYou can find the final model on the Hugging Face Hub at mlabonne/NeuralPipe-7B-ties.\n\n\n\nIntroduced by Yu et al. (2023), DARE uses an approach similar to TIES with two main differences:\n\nPruning: DARE randomly reset fine-tuned weights to their original values (those of the base model).\nRescaling: DARE rescales the weights to keep the expectations of model outputs approximately unchanged. It adds the rescaled weights of both (or more) models to the weights of the base model with a scale factor. Mergekit’s implementation of this method has two flavours: with the sign election step of TIES (dare_ties) or without (dare_linear).\n\nExample of configuration:\nmodels:\n  - model: mistralai/Mistral-7B-v0.1\n    # No parameters necessary for base model\n  - model: samir-fama/SamirGPT-v1\n    parameters:\n      density: 0.53\n      weight: 0.4\n  - model: abacusai/Slerp-CM-mist-dpo\n    parameters:\n      density: 0.53\n      weight: 0.3\n  - model: EmbeddedLLM/Mistral-7B-Merge-14-v0.2\n    parameters:\n      density: 0.53\n      weight: 0.3\nmerge_method: dare_ties\nbase_model: mistralai/Mistral-7B-v0.1\nparameters:\n  int8_mask: true\ndtype: bfloat16\nIn this configuration, we merge three different models based on Mistral-7B using dare_ties. This time, I chose weights that sum to 1 (the sum should be between 0.9 and 1.1). The density parameter is a little higher than what’s recommended in the paper (&lt;0.5), but it looks like it gives consistently better results (see this discussion).\nYou can find it on the Hugging Face Hub at mlabonne/Daredevil-7B. It’s also the best merge model in this article, outperforming even Marcoro14-7B-slerp.\n\n\n\nThe passthrough method differs significantly from the previous ones. By concatenating layers from different LLMs, it can produce models with an exotic number of parameters (e.g., 9B with two 7B parameter models). These models are often referred to as “frankenmerges” or “Frankenstein models” by the community.\nThis technique is very experimental, but it managed to create impressive models, like goliath-120b using two Llama 2 70B models. The recently released SOLAR-10.7B-v1.0 also uses the same idea, called depth-up scaling in their paper.\nExample of configuration:\nslices:\n  - sources:\n    - model: OpenPipe/mistral-ft-optimized-1218\n      layer_range: [0, 32]\n  - sources:\n    - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n      layer_range: [24, 32]\nmerge_method: passthrough\ndtype: bfloat16\nThe resulting frankenmerge will have all the 32 layers from the first model and 8 additional layers from the second model. This creates a frankenmerge with a total of 40 layers and 8.99B parameters."
  },
  {
    "objectID": "posts/dpo-mistral-7b/index.html#merge-algorithms",
    "href": "posts/dpo-mistral-7b/index.html#merge-algorithms",
    "title": "Merge Model using Mergekit",
    "section": "Merge algorithms",
    "text": "Merge algorithms\nIn this section, we will focus on four methods currently implemented in mergekit. Note that there are other methods, such as linear and Task Arithmetic. If you’re interested in papers on model merging, I recommend this excellent collection on Hugging Face.\n\nSLERP\nSpherical Linear Interpolation (SLERP) is a method used to smoothly interpolate between two vectors. It maintains a constant rate of change and preserves the geometric properties of the spherical space in which the vectors reside.\nThere are several reasons to prefer SLERP over a traditional linear interpolation. For example, in high-dimensional spaces, linear interpolation can lead to a decrease in the magnitude of the interpolated vector (i.e., it reduces the scale of weights). Moreover, the change in direction of the weights often represents more meaningful information (like feature learning and representation) than the magnitude of change.\nSLERP is implemented using the following steps:\n\nNormalize the input vectors to unit length, ensuring they represent directions rather than magnitudes\nCalculate the angle between these vectors using their dot product.\nIf the vectors are nearly collinear, it defaults to linear interpolation for efficiency. Otherwise, SLERP computing scale factors based on the interpolation factor t (t=0 = 100% of the first vector, t=1 = 100% of model 2) and the angle between the vectors.\nThese factors are used to weigh the original vectors, which are then summed to obtain the interpolated vector.\n\nSLERP is currently the most popular merging method, but it is limited to combining only two models at a time.\nExample of configuration:\nslices:\n  - sources:\n      - model: OpenPipe/mistral-ft-optimized-1218\n        layer_range: [0, 32]\n      - model: santoshsawant/NeuralHermes-2.5-Mistral-7B\n        layer_range: [0, 32]\nmerge_method: slerp\nbase_model: OpenPipe/mistral-ft-optimized-1218\nparameters:\n  t:\n    - filter: self_attn\n      value: [0, 0.5, 0.3, 0.7, 1]\n    - filter: mlp\n      value: [1, 0.5, 0.7, 0.3, 0]\n    - value: 0.5\ndtype: bfloat16\nThis is a classic SLERP configuration, applied to every layer of both models. Note that we input a gradient of values for the interpolation factor t. The parameters for the self-attention and MLP layers will use different combinations of OpenPipe/mistral-ft-optimized-1218 and mlabonne/NeuralHermes-2.5-Mistral-7B. The other layers are a 50/50 mixture of the two models.\nYou can find the final model on the Hugging Face Hub at mlabonne/NeuralPipe-7B-slerp.\n\n\nTIES\nIntroduced in this paper by Yadav et al., TIES-Merging is designed to efficiently merge multiple task-specific models into a single multitask model. It addresses two main challenges in model merging:\n\nRedundancy in model parameters: It identifies and eliminates redundant parameters within task-specific models. This is achieved by focusing on the changes made during fine-tuning, identifying the top-k% most significant changes, and discarding the rest.\nDisagreement between parameter signs: Conflicts arise when different models suggest opposing adjustments to the same parameter. TIES-Merging resolves these conflicts by creating a unified sign vector that represents the most dominant direction of change across all models.\n\nTIES-Merging is divided into the following three steps:\n\nTrim: Reduces redundancy in task-specific models by retaining only a fraction the most significant parameters (density parameter) and resetting the rest to zero.\nElect Sign: Resolves sign conflicts across different models by creating a unified sign vector based on the most dominant direction (positive or negative) in terms of cumulative magnitude.\nDisjoint Merge: Averages parameter values that align with the unified sign vector, excluding zero values.\n\nUnlike SLERP, TIES can merge multiple models at a time.\nExample of configuration:\nmodels:\n  - model: mistralai/Mistral-7B-v0.1\n    # no parameters necessary for base model\n  - model: OpenPipe/mistral-ft-optimized-1218\n    parameters:\n      density: 0.5\n      weight: 0.5\n  - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n    parameters:\n      density: 0.5\n      weight: 0.3\nmerge_method: ties\nbase_model: mistralai/Mistral-7B-v0.1\nparameters:\n  normalize: true\ndtype: float16\nWith this config, we use Mistral-7B as a base model to calculate the delta weights. We merge the same two models: mistral-ft-optimized-1218 (50%) and NeuralHermes-2.5-Mistral-7B (30%) with normalization. Here, the density means that we’re only retaining 50% of the parameters of each model (the other half comes from the base model).\nNote that the sum of the weights is not equal to 1 in the config, but the normalize: true parameter will automatically normalize them internally. This config is inspired by the parameters provided by the author of OpenHermes-2.5-neural-chat-7b-v3-1-7B.\nYou can find the final model on the Hugging Face Hub at mlabonne/NeuralPipe-7B-ties.\n\n\nDARE\nIntroduced by Yu et al. (2023), DARE uses an approach similar to TIES with two main differences:\n\nPruning: DARE randomly reset fine-tuned weights to their original values (those of the base model).\nRescaling: DARE rescales the weights to keep the expectations of model outputs approximately unchanged. It adds the rescaled weights of both (or more) models to the weights of the base model with a scale factor. Mergekit’s implementation of this method has two flavours: with the sign election step of TIES (dare_ties) or without (dare_linear).\n\nExample of configuration:\nmodels:\n  - model: mistralai/Mistral-7B-v0.1\n    # No parameters necessary for base model\n  - model: samir-fama/SamirGPT-v1\n    parameters:\n      density: 0.53\n      weight: 0.4\n  - model: abacusai/Slerp-CM-mist-dpo\n    parameters:\n      density: 0.53\n      weight: 0.3\n  - model: EmbeddedLLM/Mistral-7B-Merge-14-v0.2\n    parameters:\n      density: 0.53\n      weight: 0.3\nmerge_method: dare_ties\nbase_model: mistralai/Mistral-7B-v0.1\nparameters:\n  int8_mask: true\ndtype: bfloat16\nIn this configuration, we merge three different models based on Mistral-7B using dare_ties. This time, I chose weights that sum to 1 (the sum should be between 0.9 and 1.1). The density parameter is a little higher than what’s recommended in the paper (&lt;0.5), but it looks like it gives consistently better results (see this discussion).\nYou can find it on the Hugging Face Hub at mlabonne/Daredevil-7B. It’s also the best merge model in this article, outperforming even Marcoro14-7B-slerp.\n\n\nPassthrough\nThe passthrough method differs significantly from the previous ones. By concatenating layers from different LLMs, it can produce models with an exotic number of parameters (e.g., 9B with two 7B parameter models). These models are often referred to as “frankenmerges” or “Frankenstein models” by the community.\nThis technique is very experimental, but it managed to create impressive models, like goliath-120b using two Llama 2 70B models. The recently released SOLAR-10.7B-v1.0 also uses the same idea, called depth-up scaling in their paper.\nExample of configuration:\nslices:\n  - sources:\n    - model: OpenPipe/mistral-ft-optimized-1218\n      layer_range: [0, 32]\n  - sources:\n    - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n      layer_range: [24, 32]\nmerge_method: passthrough\ndtype: bfloat16\nThe resulting frankenmerge will have all the 32 layers from the first model and 8 additional layers from the second model. This creates a frankenmerge with a total of 40 layers and 8.99B parameters."
  }
]