{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Improving Text Embeddings with Large Language Models using fine-tuned Mistral-7B LLM \"\n",
    "author: \"Santosh Sawant\"\n",
    "date: \"2024-01-09\"\n",
    "categories: [llm, research paper]\n",
    "image: \"01.jpeg\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](01.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out a groundbreaking paper on improving text embeddings with large language models (LLMs) like GPT-4! The authors propose generating synthetic training data for text embedding tasks using LLMs, instead of relying on human-labeled datasets. \n",
    "\n",
    "Their two-step prompt method generates diverse synthetic data for hundreds of thousands of embedding tasks across 93 languages, covering semantic textual similarity, bitext retrieval, and more. The Mistral-7B LLM is fine-tuned on the synthetic data and achieves state-of-the-art results on the MTEB benchmark, outperforming previous models by 2.4 points on average across task categories. \n",
    "\n",
    "In summary, this paper presents an effective and efficient method for improving text embeddings by leveraging data generation with LLMs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper : [https://arxiv.org/pdf/2401.00368.pdf](https://arxiv.org/pdf/2401.00368.pdf)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
