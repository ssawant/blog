{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models \"\n",
    "author: \"Santosh Sawant\"\n",
    "date: \"2024-01-04\"\n",
    "categories: [llm, research paper]\n",
    "image: \"01.jpeg\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](01.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper provides a comprehensive taxonomy categorizing over 32 techniques for mitigating hallucinations in large language models (LLMs). It groups the techniques into categories such as prompt engineering, self-refinement through feedback and reasoning, prompt tuning, and model development. Key mitigation techniques highlighted include:\n",
    "\n",
    "* Retrieval Augmented Generation (RAG) which enhances LLM responses by retrieving information from authoritative external knowledge bases. This helps ground the responses in facts.\n",
    "\n",
    "* Methods leveraging iterative feedback loops and self-contradiction detection to refine LLM outputs. For example, the Self-Reflection Methodology employs knowledge acquisition and answer generation over multiple cycles. \n",
    "\n",
    "* Prompt tuning techniques like UPRISE which tune lightweight retrievers to automatically provide task-specific prompts that reduce hallucinations. \n",
    "\n",
    "* Novel model decoding strategies such as Context-Aware Decoding that override an LLM's biases by amplifying differences between outputs with and without context. \n",
    "\n",
    "* Utilizing knowledge graphs and adding faithfulness based loss function\n",
    "\n",
    "* Supervised Fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper : [https://arxiv.org/pdf/2401.01313.pdf](https://arxiv.org/pdf/2401.01313.pdf)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
